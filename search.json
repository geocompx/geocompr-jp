[{"path":"index.html","id":"index","chapter":"ようこそ","heading":"ようこそ","text":"ここは、地理データの解析、可視化、モデリングに関する書籍 Geocomputation R のオンラインホーム (日本語版) です。本書 (原著) は、CRC Pressまたは Amazon から購入することができます。初版のアーカイブは、bookdown.org にてホストされています。本書は、Free Open Source Software Geospatial (FOSS4G) の活動に触発されているおり、本書の基盤となるコードと解説はオープンであり、内容の再現性、透明性、アクセス性を保証しています。\nGitHub でソースコードを公開しているため、誰でも課題 (Issue) を開いたり、新しいコンテンツや誤植の修正に貢献することでプロジェクトに関わり、みんなのために役立てることができます。\n本書のオンライン版は、r.geocompx.org でホストされ、GitHub Actions によって最新版が維持されています。(訳注: 日本語版は、https://babayoshihiko.ddns.net/geo とhttp://124.219.182.167/geo/でホストされている。)\n現在の「ビルド状況」は以下の通りです。version book built GH Actions 2025-06-08.本書は、Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License ライセンスで提供。本書のサンプルコードは、Creative Commons CC0 1.0 Universal (CC0 1.0) ライセンスで提供されています。","code":""},{"path":"index.html","id":"how-to-contribute","chapter":"ようこそ","heading":"貢献するには","text":"bookdown では、GitHub のアカウント (sign-github.com) があれば、Wiki を編集するように簡単に本を編集することができます。\nGitHub にログイン後、書籍サイトの右パネルにある「Edit page」アイコンをクリックしてください。\nこれで、今読んでいるこのページを生成したソース R Markdown ファイルの編集可能なバージョンに移動します。この本の内容に関する問題提起 (コードが実行されないなど) や機能リクエストは、Issue Tracker 課題追跡システムで確認できます。管理者と貢献者は、このリポジトリの行動規範 に従ってください。","code":""},{"path":"index.html","id":"reproducibility","chapter":"ようこそ","heading":"本書のコードを再現","text":"初めて R で地理データを扱うのであれば、手軽に本書の内容を再現する方法は、ウェブブラウザで Binder というサイトにアクセスすることでしょう。\n以下のリンクをクリックすると、RStudio Server を含む新しいウィンドウがウェブブラウザで開き、各章のファイルを開いたり、コードチャンクを実行して、コードが再現可能かどうかテストすることができます (訳注: 英語版が開く)。下の画像のようなものが表示されたら成功です。\nmybinder.org ユーザガイドラインに従い、クラウドベースの環境で R によるジオコンピュテーションの探求を始めてください。\nFIGURE 0.1: Geocomputation R に含まれる再現可能なコードを、Binder が提供するブラウザ上の RStudio Server で実行した画面\nこの本のコードを自分のコンピュータで再現するには、最近のバージョンの R と最新のパッケージが必要です。\nパッケージは remotes パッケージを使用してインストールすることができます。この本で使用するパッケージやデータをインストールした後、テストや教育目的のためにこの本を再構築することができます。\n再構築するには、この本のソースコードを ダウンロードして解凍するか、 clone してください。\nRStudio (または VS Code などの他の IDE) で geocompr.Rproj プロジェクトを開き、以下のコマンドで内容を再現することができるようになります。再現の詳細は、同プロジェクトの GitHub repo を参照。","code":"\ninstall.packages(\"remotes\")\nremotes::install_github(\"geocompr/geocompkg\")\nbookdown::serve_book(\".\")"},{"path":"index.html","id":"getting-involved","chapter":"ようこそ","heading":"プロジェクトに参加","text":"もし、この本が役に立つと思ったら、以下の方法で応援してください。この本について、人々に話すgeocompr GitHub repository にスターをつける引用またはリンクを貼るオンラインで本書について伝える。例えば、Twitterの #geocompr ハッシュタグ を使う (geocompr.github.ioのゲストブックを参照)、本書を使ったコースについて知らせる、等一冊購入するAmazon や Goodreads でレビューを書くGitHub または X (旧 Twitter) あるいは Discord で、質問や提案を行う (日本語の場合、r-wakalang Slack や OSGeo 財団メーリングリスト などがある)質問に答えたり、質問がわかりにくい場合に説明や再現例を求めるなど何らかの応答をする再現可能な研究に向けたオープンソースを皆に広める。とくに、R における地理データを扱う (これは、自分のスキルアップに最適な方法でもある)コミュニティによる翻訳\nスペイン語版: https://geocompr.github.io/es/\nフランス語版: https://geocompr.github.io/fr/\n日本語版: http://babayoshihiko.ddns.net/geo/\nスペイン語版: https://geocompr.github.io/es/フランス語版: https://geocompr.github.io/fr/日本語版: http://babayoshihiko.ddns.net/geo/詳しくは、github.com/geocompx/geocompr 参照。本書で使用している地球儀のアイコンは、Jean-Marc Viglino によって作成され、CC-4.0 International のライセンスで提供されています。\n本書のウェブサイト (英語版) は、Netlify でホストされています。","code":""},{"path":"foreword-1st-edition.html","id":"foreword-1st-edition","chapter":"序文 (第 1 版)","heading":"序文 (第 1 版)","text":"R で「空間的なこと」をすることは、常に幅が広い。興味がある人なら誰でも参加できるよう、地理学、ジオインフォマティクス、ジオコンピュテーション、空間統計学のツールを提供し統合することに努めてきた。\nすなわち、R で「空間的なこと」をするということは、常にオープンソースコード、オープンデータ、そして再現性を含んでいるのである。また、R で「空間的なこと」を行うことは、応用空間データ解析の多くの部門との相互作用に対してオープンであることを目指し、さらに、データ表現と解析方法の新しい進歩を実装し、それらを分野横断的な精査にさらすことを目的としている。\n本書が示すように、同じようなデータから同じような結果が得られる別のワークフローが存在することも多く、他の人がどのようにワークフローを作り、理解しているかを比較することで学ぶこともある。\nこれには、オープンソース GIS や Python、Java などの補完言語に関する類似のコミュニティから学ぶことも含まれる。R の幅広い空間機能は、自分で作っているものや応用しているものを共有しようとする人たちがいなければ、決して進化しなかっただろう。\nこれには、教材、ソフトウェア、研究手法 (再現可能な研究、オープンデータ)、およびこれらの組み合わせが含まれる。\nまた、GDAL、GEOS、PROJ といった「アップストリーム」のオープンソース地理ライブラリからも、R ユーザーは大きな恩恵を受けている。本書は、好奇心と参加意欲があれば、自分の適性に合ったやるべきことが見つかるというわかりやすい例である。\nデータ表現とワークフローが進歩し、定量的なコマンドラインを使用しない新しいユーザーが増え続ける中、この種の本が本当に必要とされている。\n多大な苦労があったにもかかわらず、著者たちは互いに支え合って出版に漕ぎ着けたのである。この本は、著者がチュートリアルやワークショップで試行錯誤を重ね、読者や講師が自分のような人に試し続けてきた内容であることがわかるので、すぐにでも使える。\n著者や R-spatial コミュニティと関わり、ワークフローを構築する上でより多くの選択肢を持つことに価値を見出し、そして最も重要なことは、ここで学んだことを自分の関心のあることに適用することを楽しむことである。Roger Bivandベルゲン、2018年9月","code":""},{"path":"forward-2nd-edition.html","id":"forward-2nd-edition","chapter":"序文 (第 2 版)","heading":"序文 (第 2 版)","text":"Writing books open source data science software constantly changes uncontrolled ways brave undertaking: feels like running race someone else constantly moves finish line. second edition Geocomputation R timely: catches many recent changes, also embraces new R packages, new topical developments computing landscape. now includes chapter raster-vector interactions, discussing package terra replacing package raster raster (vector) data processing. also keeps tmap package creating high quality maps, completing full rewrite cycle.Besides updating contents book, authors also active helping streamline focus changes software extensively testing , helping improve , writing issues pull requests GitHub, sharing benchmark results, helping improve software documentation.first edition book great success. first book popularize spatial analysis sf package tidyverse. enthusiastic tone reached wide audience, helped people various levels experience solving new problems moving next level. available entirely freely online addition printed volume gave large reach, enabled users try presented methodology datasets. addition , authors encouraged readership reach ways GitHub issues, social media posts, discussions discord channel. led 75 people contributing book’s source code one way , including several providing longer reviews contributing full sections, including Cloud-optimized GeoTIFFs, STAC openEO; sfheaders package; OGC APIs metadata; CycleHire shiny app. Discord, led lively spontaneous discussions threads include topics ranging highly technical “look built”.Beyond , authors initiated companion volume Geocomputation Python, stressing geocomputation happens data science languages, means restricted one . Geocomputation rise, part fostering growing geocomputation community, writing books like one indispensable.Edzer PebesmaMünster, Germany, May 2024","code":""},{"path":"preface.html","id":"preface","chapter":"序文","heading":"序文","text":"","code":""},{"path":"preface.html","id":"who-this-book-is-for","chapter":"序文","heading":"本書の対象","text":"本書は、オープンソースソフトウェアを使って地理データを分析、視覚化、モデル化したい人のための本である。\n強力なデータ処理、可視化、地理空間機能を持つ統計プログラミング言語である R をベースにしている。\n本書は幅広いトピックをカバーしており、特に様々な背景を持つ幅広い人々に興味を持っていただけると思われる。QGIS、ArcGIS、 GRASS、SAGA などのデスクトップ地理情報システム (GIS) を使って空間分析力を身につけた人が、強力な (ジオ) 統計・視覚化プログラミング言語とコマンドラインアプローチの利点にアクセスしたい (Sherman 2008)。\n\nadvent ‘modern’ GIS software, people want point click way life. ’s good, tremendous amount flexibility power waiting command line.\nQGIS、ArcGIS、 GRASS、SAGA などのデスクトップ地理情報システム (GIS) を使って空間分析力を身につけた人が、強力な (ジオ) 統計・視覚化プログラミング言語とコマンドラインアプローチの利点にアクセスしたい (Sherman 2008)。advent ‘modern’ GIS software, people want point click way life. ’s good, tremendous amount flexibility power waiting command line.地理学、リモートセンシング、計画、GIS、空間データ科学など、地理データを専門とする分野の大学院生・研究者地理学、リモートセンシング、計画、GIS、空間データ科学など、地理データを専門とする分野の大学院生・研究者地質学、地域科学、生物学と生態学、農業科学、考古学、疫学、輸送モデリング、および広義のデータ科学などの分野で地理データを扱っている研究者や大学院生で、研究に R のパワーと柔軟性を必要としている人。地質学、地域科学、生物学と生態学、農業科学、考古学、疫学、輸送モデリング、および広義のデータ科学などの分野で地理データを扱っている研究者や大学院生で、研究に R のパワーと柔軟性を必要としている人。都市・交通計画、物流、ジオマーケティング (店舗立地分析)、緊急時計画など、多様な空間データを扱うアプリケーションにおいて、Rなどのコマンドライン言語の再現性、スピード、柔軟性を必要とする公共、民間、第三セクターの応用研究者やアナリストの方都市・交通計画、物流、ジオマーケティング (店舗立地分析)、緊急時計画など、多様な空間データを扱うアプリケーションにおいて、Rなどのコマンドライン言語の再現性、スピード、柔軟性を必要とする公共、民間、第三セクターの応用研究者やアナリストの方本書は、ジオコンピュテーションに興味のある中級から上級の R ユーザーと、地理データを扱ったことのある R 初心者を対象にしている。\nR も地理データも初めてという方は、Chapter 2 と下に用意したリンクで、初心者の視点から空間データの本質を解説している。","code":""},{"path":"preface.html","id":"how-to-read-this-book","chapter":"序文","heading":"本書の読み方","text":"本書は 3 つのパートに分かれている。第 部: 基本機能では、R で地理データを扱うための知識を身につけることを目的としている。第 II 部: 拡張機能では、高度なテクニックを網羅する。具体的には、空間データの可視化、GIS へのブリッジ、空間データのプログラミング、統計学習。第 III 部: 応用では、実際の問題への対応。具体例として、交通解析、商圏分析、生態学。章ごとにだんだん難しくなっていく。\nこのため、第 部はすべて読んでいただき、第 II 部、第 III 部に取り組んでいただきたい。\n第 II 部、第 III 部は、順に読んでいただくとよいが、興味のある章だけを読むこともできる。\nR で地理解析を行う際の大きな障壁は、その学習曲線が急であることである。\n第I部の各章では、簡単なデータセットで再現可能なコードを提供することで、この問題に対処することを目的としている。教える・学ぶという観点でこの本の重要な点は、各章の最後にある演習である。\n演習を修了することで、地理空間に関するさまざまな問題に取り組むために必要なスキルを身につけ、自信を持つことができる。\n演習の解答は、 geocompr.github.io/solutions でホストされている Geocomputation R に付属するオンラインブックレットで見ることができる。\nこの冊子の作成方法、および _01-ex.Rmd などのファイルの解答を更新する方法については、この blog post を参照。\nその他のブログ記事と拡張例は、この本のサポートサイト（geocompx.org）を参照。急ぎたい場合は、Chapter 2 から始まる実用的な例題にすぐに取り組むこともできる。\nしかし、まずは Chapter 1 の Geocomputation R の広い文脈を読むことを勧める。\nまた、R に初めて触れる方は、各章で提供されるコードチャンクを実行しようとする前に、この言語についてもっと学ぶことを勧める（概念を理解するためにこの本を読む場合は別）。\nR の初心者にとって幸いなことに、R にはサポートするコミュニティがあり、助けとなるリソースが豊富に開発されている。\n特にお勧めのチュートリアルは、R Data Science (Grolemund Wickham 2016) と Efficient R Programming (Gillespie Lovelace 2016)、そして introduction R (R Core Team 2021) の 3 つである。","code":""},{"path":"preface.html","id":"why-r","chapter":"序文","heading":"なぜ R なのか？","text":"R の学習曲線は急であるが、本書で提唱するコマンドラインアプローチはすぐに成果を上げることができる。\nこの後の章で学ぶように、R は地理データに関するさまざまな課題に取り組むための効果的なツールである。\n練習を重ねれば、R が地理空間ツールボックスの中で、多くのアプリケーションに選ばれるプログラムになることを期待している。\nコマンドラインでコマンドを入力して実行する方が、デスクトップ GIS のグラフィカル・ユーザー・インターフェース (GUI) をマウスで操作するよりも速い場合が多くある。\n空間統計やモデリングなどのアプリケーションでは、R が唯一の現実的な方法だろう。Section 1.3 で概説したように、ジオコンピュテーションに R を使用する理由はたくさんある。\nR は、他の言語と比較して、多くの地理データ解析ワークフローで必要とされる対話的な使用に適している。\nR は、データサイエンス（データ加工、統計学習技術、データの可視化など）やビッグデータ（データベースや分散コンピューティングシステムとの効率的なインターフェースによる）の分野で急速に発展している分野に優れている。\nさらに、R は再現性のあるワークフローを可能にする。分析の基礎となるスクリプトを共有することで、他の人があなたの研究を発展させることができるのである。\n本書での再現性を確保するために、ソースコードを github.com/geocompx/geocompr で公開している。\n上記サイトでは、code/ フォルダに数値を生成するスクリプトファイルがある。\n図を生成するコードが書籍の本文に記載されていない場合、図を生成したスクリプトファイル名をキャプションに記載する（例えば、 Figure 13.2 のキャプションを参照）。Python、Java、C++ などの他の言語もジオコンピュテーションに使用でき、Section 1.4 で説明したように、R を使わずにジオコンピュテーションを学ぶための優れたリソースがある。\nR コミュニティが提供するパッケージ・エコシステム、統計機能、可視化オプション、強力な IDE のユニークな組み合わせは、いずれも提供されていない。\nさらに、1つの言語（R）の使い方を深く学ぶことで、他の言語でジオコンピュテーションを行うために必要な概念と自信を身につけることができる。","code":""},{"path":"preface.html","id":"real-world-impact","chapter":"序文","heading":"実世界への影響","text":"Geocomputation R は、地理データに現れる科学的、社会的、環境的な意味を含む幅広い問題に取り組むための知識とスキルを身につけることができる。\nSection 1.1 にあるように、ジオコンピュテーションは、コンピュータを使って地理データを処理することだけが目的ではない。\nまた、現実的なインパクトも重要である。\n本書の背景や動機に興味がある方は、Chapter 1 を参照。","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"序文","heading":"謝辞","text":"プルリクエストで直接貢献してくださった以下の方々をはじめ、コードのホスティングとコラボレーションサイトである GitHub を通じて直接、間接的に貢献してくださった以下の皆様に感謝する: prosoitos, florisvdh, babayoshihiko, katygregg, tibbles--tribbles, Lvulis, rsbivand, iod-ine, KiranmayiV, cuixueqin, defuneste, zmbc, erstearns, FlorentBedecarratsNM, dcooley, darrellcarvalho, marcosci, appelmar, MikeJohnPage, eyesofbambi, krystof236, nickbearman, tylerlittlefield, giocomai, KHwong12, LaurieLBaker, MarHer90, mdsumner, pat-s, sdesabbata, ahmohil, ateucher, annakrystalli, andtheWings, kant, gavinsimpson, Himanshuteli, yutannihilation, howardbaek, jimr1603, jbixon13, olyerickson, yvkschaefer, katiejolly, kwhkim, layik, mpaulacaldas, mtennekes, mvl22, ganes1410, richfitz, VLucet, wdearden, yihui, adambhouston, chihinl, cshancock, e-clin, ec-nebi, gregor-d, jasongrahn, p-kono, pokyah, schuetzingit, tim-salabim, tszberkowitz, vlarmet。\n表紙画像を作成しただけでなく、それを生成するコードも公開してくれた Marco Sciaini 氏に感謝する (本書の GitHub リポジトリの code/frontcover.R を参照)。\nさらに何十人もの人々が、問題提起やコメント、ソーシャルメディアを通じてのフィードバックなど、オンラインで貢献した。\n#geocompr ハッシュタグは生き続けます!CRC プレス社の John Kimmel 氏には、2 年以上にわたって私たちのアイデアを初期の書籍企画から 4 回の査読を経て製品化するまでに協力していただいたことに感謝する。\n本書の構成と内容を大幅に改善するために、詳細なフィードバックと専門知識を提供してくださった査読者の方々は、特筆に値する。また、この章では、Jena 大学の Patrick Schratz 氏と Alexander Brenning 氏との有意義な議論とご意見に感謝する。\nウェブサービスに関するセクションで専門的なご意見をいただいた国際連合食糧農業機関の Emmanuel Blondel 氏に感謝する。\nMichael Sumnerv 氏には、本書の多くの部分、特に第 10 章のアルゴリズムに関する議論に重要な示唆をいただいた。\nTim Appelhans 氏と David Cooley 氏は、可視化の章 (第 8 章) で重要な貢献をした。\nさらに、Katy Greggv 氏は、すべての章を校正し、本書の読みやすさを大きく向上させた。他にも数え切れないほど、さまざまな形で貢献された方がいるだろう。\n最後に、R によるジオコンピュテーションを可能にしているすべてのソフトウェア開発者に感謝する。\nEdzer Pebesma (sfパッケージを作成)、Robert Hijmans (raster を作成)、Roger Bivand (多くの R-spatial ソフトウェアの基礎を作った) は、R で高性能な地理計算を可能にした。","code":""},{"path":"intro.html","id":"intro","chapter":"1 はじめに","heading":"1 はじめに","text":"本書は、コンピュータの力を使って地理データで何かを行う方法について書かれている。\n地理データの読み書きと何らかの処理、静的およびインタラクティブな地図の作成、実世界の問題を解決するためのジオコンピュテーション の適用、地理現象のモデリングなど、交通や環境系など多様な空間スキルを教える。\n本書は、様々な地理的操作をどのようにリンクさせることができるかを、文章に挟まれた再現可能な「コードチャンク」で示す。このことにより、透明性のある、したがって科学的なワークフローも教えてくれる。本書は、ジオコンコンピュテーションのために既存のツールの利点を活用することだけにとどまらない。地理データ構造、および地理データ処理によってソフトウェアを理解し、新しいツールを開発する際に必要なソフトウェアを理解する。\n本書全体で解説するコマンドライン駆動のアプローチと、Chapter 11 で解説するプログラミングにより、ソフトウェアによって課された創造性の制約を取り除くことができる。\n本書を読み、演習問題を解いた後は、読者が現実の課題に取り組むためにスキルを応用し、地図とコードで作業をこなし、再現可能かつフリーソフトウェア活動に参加する準備ができることを期待している。地理空間用のフリー＆オープンソースソフトウェア (FOSS4G) はここ数十年の間に驚くほどのスピードで進歩している。\nOSGeo のような組織のおかげで、応用地理技術はもはや高価なハードウェアやソフトウェアを持つ人だけのものではなくなり、誰でも高性能なジオコンピュテーション向けソフトウェアをダウンロードして実行することができるようになったのである。\nQGIS などのオープンソースの地理情報システム (geographical information system, GIS) により、世界中で地理分析にアクセスできるようになった。\nGIS プログラムは強力であるが、本書で解説するコマンドライン・インターフェース (CLI) よりも、グラフィカル・ユーザー・インターフェース (graphical user interface, GUI) を重視する傾向がある。\n多くの GIS にみられる GUI 偏重は、完全な再現性のある作業を難しくするという、意図しない副作用がある。Chapter 10 で見るように、この問題は「ジオアルゴリズム」を呼び出すことで解決することができる。\n大きく異なる二つのアプローチを単純に比較すると、Table 1.1 に示すようになる。TABLE 1.1: ソフトウェアパッケージによる重点の違い (地理情報システム (GIS) の GUI と R の GUI)。R だけがジオコンピュテーションのための CLI を提供する言語ではない。\nPython (Geocomputation Python で解説)、Julia、JavaScript など、強力なジオコンピュテーション機能を持つコマンド環境は他にも存在する。\nしかし、R にはジオコンピュテーションを学ぶこと、さらに応用することに適した利点がある。Section 1.2 で説明するように、特に、統計学、モデル化、可視化の用途である。本書を執筆する動機は、科学研究における再現性の重要性 (下記注参照) である。\n再現性のある地理データ解析ワークフローをより身近なものにし、コマンドラインから利用できるオープンな地理空間ソフトウェアの力を実証することを目的としている。\nR は、他の言語へのインターフェースを提供し (Eddelbuettel Balamuta 2018)、多くの空間ソフトウェアライブラリにアクセスすることができる。これについては、Section 1.3 で簡単に説明し、Chapter 10 で実行してみたい。\nしかし、ソフトウェアの詳細に触れる前に、一歩下がってジオコンピュテーションとは何かを考えてみる価値がある。","code":""},{"path":"intro.html","id":"what-is-geocomputation","chapter":"1 はじめに","heading":"1.1 ジオコンピュテーションとは？","text":"ここでは、ジオコンピューテーションを以下のように定義する。再現性、柔軟性、ツール開発に重点を置き、地理データを使って問題を解決する研究、ソフトウェア開発、実用化の分野。ジオコンピューテーションという用語は比較的新しく、1996年の第 1 回学術会議で用いられた。1\nジオコンピュテーションを (当時) 一般的に使われていた「計量地理学」と区別したのは、その初期の提唱者が提唱した、「創造的・実験的」応用 (Longley et al. 1998) と新しいツールや手法の開発に重点を置いたことである (Openshaw Abrahart 2000)。\nこの用語を提唱 (かつ、おそらく定義も) した Stan Openshaw によると、「ジオコンピュテーションは、様々な異なるタイプのジオデータを使用し、「科学的」アプローチという全体的な文脈の中で、関連するジオツールを開発することである」(Openshaw Abrahart 2000)。\nこの初期の定義に基づき、R によるジオコンピューテーションは、データ分析やモデリングにとどまらず、学術的に興味深いだけでなく、有益な仕事のための新しいツールや手法の開発を含む。しかし、私たちのアプローチは当初の定義とは一点異なり、再現性とコラボレーションに重点を置いている。\n21世紀初頭は、まだ必要なハードウェア、ソフトウェア、データの入手が困難で、読者がコード例を再現することは非現実的だった。\nそれから今日まで、解析環境は急速に進展している。\nRAM を十分に (少なくとも 8 GB 以上) 搭載したノートパソコンがあれば、一般に公開されているデータセットにジオコンピュテーション用のソフトウェアをインストール・実行し、また本書を再現することができるはずである。\n1990年代から2000年代初頭にかけて、高性能なコンピュータが一般の人々にとって手の届かないものであったために存在したジオコンピュテーションに対する金銭的・ハード的な障害は、現在では取り除かれている。2\nまた、公的に入手できるデータセットが広く普及してきたことも、ジオコンピュテーションはより身近になってきた理由の一つである。この点については、Chapter 8 で扱う。\nこの分野の初期の作品とは異なり、本書で紹介するすべての研究は、本書と一緒に提供されるコードと spData のサンプルデータを使って再現可能である。spData などの R パッケージのインストールは Chapter 2 で解説する。ジオコンピュテーションは、地理情報科学 (GIScience) 、ジオマティクス、ジオインフォマティクス、空間情報科学、地理情報工学 (Longley 2015)、空間データサイエンス (Spatial Data Science, SDS) といった他の用語と密接に関連している。\nそれぞれの用語は、起源や主な適用分野は異なるものの、GIS の影響を受けた「科学的」 (再現性と反証可能性を意味する) アプローチに重点を置いている点で共通している。\n例えば、SDS は「データサイエンス」のスキルと大規模なデータセットを重視し、ジオインフォマティクスはデータ構造に焦点を当てる傾向がある。\nしかし、用語間の違いよりも重複の方が大きいため、すべてを包含する大まかな同義語としてジオコンピュテーションを使用している。\nこれらはすべて、地理データを応用科学的な作業に利用することを目的としている。\nしかし、この用語の初期の使用者とは異なり、私たちは「ジオコンピュテーション (Geocomputation) 」 (または Stan Openshaw が呼んだ「ジオコンピュテーション (GeoComputation、C が大文字になっている) 」) というまとまった学術分野があることを示そうとするものではない。ジオコンピュテーションは最近の言葉であるが、古い考えに影響を受けている。\n2000年以上の歴史を持つ地理学 の一部と見ることができる (Talbert 2014)。\nまた、1960年代に登場した GIS (Neteler Mitasova 2008) の延長線上にあるものでもある (Coppock Rhind 1991)。地理学は、コンピュータが発明されるずっと以前から、人類と自然界との関係を説明し、影響を与える上で重要な役割を担ってきた。\n著名な探検家・地理学者であり博識であった Alexander von Humboldt (彼の名前を冠した種、地物、大学などがある) が、この役割を以下のように説明している。\n1800年代初めに南米を旅行した際に自然地理学や植物地理学の伝統の基礎を築いただけでなく、自然界を保護するための政策への道も開いたのである (Wulf 2015)。\n本書は、現代のコンピュータとオープンソースソフトウェアの力を活用し、進化し続ける「地理学の伝統」 (Livingstone 1992) に貢献することを目的としている。本のタイトル案には Geography R と R GIS があったが、これらは古い学問分野とリンクしていることが分かる。\nそれぞれに利点がある。\n前者は、単なる空間データ以上のもので構成されているというメッセージを伝えている。\n後者は、この本が GIS として R を使い、地理データの空間演算を行うための本であることを伝えている (Bivand, Pebesma, Gómez-Rubio 2013)。\nしかし、GIS という言葉には、R の最大の強みを伝えきれない意味を含んでいる (Table 1.1 参照)。\nすなわち、地理データと地理以外のデータの処理、モデル化、可視化をシームレスに行うことができ、かつ再現性は GIS の能力を超えている。\nジオコンピュテーションは、再現可能なコード主導の環境で地理データを扱い、結果・方法・ツールをプログラムすることを示している。これが本書の狙いである。","code":""},{"path":"intro.html","id":"why-open-source","chapter":"1 はじめに","heading":"1.2 なぜオープンソースをジオコンピュテーションに使うのか？","text":"初期の地理学者は、気圧計、コンパス、六分儀 などのさまざまな道具を使って、世界に関する知識を深めてきた (Wulf 2015)。\n海上での経度計算が可能になったのは、1761年に発明されたマリン・クロノメーターからで、これにより船はより直進的な航路をとることができるようになった。\n今世紀に入る前、地理分析のためのデータやツールは不十分であった。現在では、研究や実務に際して、地理データの不足ではなく、逆に多すぎるデータと多すぎるツールに悩まされている。\nほとんどのスマートフォンに全地球測位 (GPS) 受信機が搭載されている。\n人工衛星や半自動運転車、市民科学者に至るまで、数多くのセンサーが世界のあらゆる場所を絶え間なく測定している。\nデータの生成速度が圧倒的に速く、自律走行車のような新技術は、毎日数百から数千ギガバイトのデータを生成する。\n人工衛星からのリモートセンシングデータが膨大になり、対応するデータを 1 台のコンピュータで解析することが難しくなったため、Chapter 10 で扱うように一台のコンピュータでは処理しきれない。\nこの「ジオデータ革命」によって、高性能なコンピュータ・ハードウェアと、ノイズから信号を処理・抽出するための効率的でスケーラブルなソフトウェアに対する需要が高まっている。\nオープンソースツールの進化により、膨大な地理データストアから直接またはアプリケーションプログラミングインターフェイス (Application Programming Interface, API) を介して地理データをインポートし処理することができる。急速に変化するハードウェア、ソフトウェア、およびデータ環境では、将来性のあるツールを選択することが重要である。\nオープンソースソフトウェアの大きな利点は、何千人もの潜在的な貢献者がいることによる開発速度と寿命である。\n毎日何百人もの人々がオープンソースプロジェクトにバグレポートを提出し、新機能やドキュメントの改善を提案している。このことと関係して、相互運用性が利点となる。\nプロプライエタリな製品は、保守が難しいモノリシックな「帝国」になりがちだが (先に述べた利点と関連している)、オープンソース・ソフトウェアは、組み合わせることができるため、モジュール化されたツール「連合」と言えるだろう。\nこのため、R のようなオープンソースのデータサイエンス言語は、高性能な可視化ライブラリやファイル形式へのインターフェースといった新しい開発を迅速に取り入れることができる。一方、プロプライエタリなソリューションは遅れを取らないように苦労している。もうひとつの大きな利点は、再現性である。\n発見を再現できることは科学研究にとって不可欠であり、オープンソースソフトウェアは、他の人が同じツールを使ってあなたの発見をチェックしたり、新しい文脈であなたの方法を適用したりすることを可能にするため、再現性という重要な障壁がない。\n誰でも無料でアクセスできるツールを使い、コードやデータを共有すると、研究結果を他の人がチェックし、それを基に構築できるようになる。これは、自分の作品を使ってもらったり、引用されたいと思うのであれば、大きなアドバンテージとなる。多くの人にとって、再現可能なコードの共有と組み合わされたオープンソースソフトウェアの最大の利点は、コミュニティである。\nプロプライエタリ・ソフトウェアのサポートは非常に使いづらいことが多い。一方、コミュニティでは質の高いサポートをすぐに得ることもある。\nコミュニティはフィードバックやアイデアを提供してくれ、Chapter 16 でも取り上げるように、独自のツールや手法を開発する手助けをしてくれる。R はオープンソースソフトウェアプロジェクトで、強力な言語であり、統計学者や開発者のコミュニティは常に進化し続けている (Wickham 2019)。\nR はオープンソースソフトウェアで再現可能なジオコンピュテーショ ンを可能にする唯一の言語ではない。\nR を使う理由の多くは、Python や Julia など、再現可能なデータサイエンスのための他のオープンソース言語にも当てはまる。\nしかし、R 独自の重要な利点がいくつかある。","code":""},{"path":"intro.html","id":"why-use-r-for-geocomputation","chapter":"1 はじめに","heading":"1.3 なぜ R をジオコンピュテーションに使うのか？","text":"R は、統計計算とグラフィックスのための、マルチプラットフォームなオープンソース言語・環境である (r-project.org/)。\nR は、幅広いパッケージによって高度な地理空間統計学 、モデリング、可視化もサポートしている。\nRStudio のような新しい統合開発環境 (integrated development environment、IDE) のおかげで、より多くの人が R を使えるようになった。特に、インタラクティブな視覚化専用のパネルがあることで地図の作成が容易になった。R の核は、オブジェクト指向の関数型プログラミング言語 (Wickham 2019)である。これに加えて、他のソフトウェアとの対話型インタフェースも設計されている (Chambers 2016)。\n後者には、GIS ソフトウェア、「ジオライブラリ」、関数の宝庫への「ブリッジ」も多く含まれている (Chapter 10 を参照)。\nそのため、C 言語、FORTRAN、Java などの (R と比較して) 低レベルな言語を習得する必要がなく、「ジオツール」を素早く作成するのに理想的である (Section 1.4 を参照)。\nこれは、GUI ベースの地理情報システムやプロプライエタリな地理情報システムが課す比喩的な「見えない壁」からの脱却のように感じられるだろう (GUI の定義については Table 1.1 を参照)。\nさらに、R は他の言語へのアクセスを容易にしている。\n例えば、パッケージの Rcpp は C++ の、reticulate は Python のコードを実行できるようにする。\nつまり、R はさまざまな地理空間プログラムへの「橋渡し役」として使用できるのである (Section 1.4 参照)。R の柔軟性と進化する地理的機能を示すもう一つの例は、インタラクティブな地図作成である。\nChapter 9 で見るように、R には「限られた対話型 (プロット) 機能しかない」 (Bivand, Pebesma, Gómez-Rubio 2013) というのは、もはや真実ではない。\nこれは、Figure 1.1 を作成する以下のコードチャンクで実証されている (プロットを生成する関数は、Section 9.4 でカバーされている)。\nFIGURE 1.1: 青いマーカーは著者の出身地を示している。ベースマップは、NASAが提供する夜の地球のタイル画像。オンライン版 (geocompr.robinlovelace.net) では、ズームインしたりポップアップをクリックするなどして、インタラクティブに楽しむことができる。\nインタラクティブな地図としてならともかく、数年前に R (あるいは他のどのプログラミング言語でも) を使って Figure 1.1 を作成するのは困難だっただろう。\nこれは R の柔軟性と、knitr や leaflet などの開発のおかげで、他のソフトウェアへのインターフェースとして使用できることを示すもので、本書を通じて繰り返し出てくるテーマである。\nそのため、R コードを使用することで、抽象的な概念だけでなく、現実の現象を表す再現性の高い事例を参考にしながらジオコンピュテーションの教育を行うことができる。R-spatial stack はインストールが簡単で、包括的でメンテナンスが行き届き、相互運用性の高いパッケージを備えている。\nR には、基本インストールに含まれる統計関数と、多くの最先端手法を実装した何百ものよくメンテナンスされたパッケージがある。\nR を使えば、驚くほど少ないコード行数で作業を開始することができ、デバッグやパッケージの依存関係の管理よりも、地理メソッドやデータに集中することができる。\nR の特に優れた点は、優れた地図作成パッケージのおかげで、出版物のような品質のインタラクティブマップを簡単に作成できることである。","code":"\nlibrary(leaflet)\npopup = c(\"Robin\", \"Jakub\", \"Jannes\")\nleaflet() |>\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\") |>\n  addMarkers(lng = c(-3, 23, 11),\n             lat = c(52, 53, 49), \n             popup = popup)"},{"path":"intro.html","id":"software-for-geocomputation","chapter":"1 はじめに","heading":"1.4 ジオコンピュテーションのためのソフトウェア","text":"R は地理計算のための強力な言語であるが、地理データ解析のための他の多くの選択肢があり、何千もの地理関数を提供している。\n地理計算のための他の言語を理解することは、特定のタスクに別のツールがより適切である場合を決定し、より広い地理空間エコシステムに R を配置するのに役立つだろう。\n本節では、Chapter 10 に向けて、ジオコンピュテーションのための言語 C++、 Java、 Python を簡単に紹介する。R (と Python) の重要な特徴として、インタプリタ型言語であることが挙げられる。\nREPL (Read-Eval-Print Loop) で対話的にプログラミングできるのがメリットである。\nコンソールに入力されたコードは、コンパイルという中間段階を待たずに、即座に実行され、結果が表示される。\n一方、C++ や Java などのコンパイルされた言語は、 (一度コンパイルされると) 動作が速くなる傾向がある。C++ は、QGIS、GRASS GIS、SAGA などの多くの GIS パッケージの基礎を提供しているので、賢明な出発点であると言えるだろう。\nよく書かれた C++ は非常に高速で、大規模な地理データセットを処理するようなパフォーマンスが重要なアプリケーションに適しているが、Python や R に比べて習得が困難である。\nC++ は、Rcpp パッケージによってよりアクセスしやすくなり、R ユーザーのために C プログラミングへの良い「入り方」を提供している。\nこのような低レベルの言語を使いこなすことで、新しい高性能な「ジオアルゴリズム」を生み出す可能性や、GIS ソフトウェアの仕組みをより深く理解することができる (Chapter 11 を参照)。\nしかし、R を使用してジオコンピュテーションを行う際には、必ずしも C++ を学ぶ必要はない。Python は、特に GRASS GIS、SAGA、QGIS などの多くのデスクトップ GIS が Python API を提供しているので、ジオコンピュテーションには重要な言語である (Chapter 10 を参照)。\nPython は、R と同様、データサイエンスでよく使われているプログラミング言語である。\n両言語はオブジェクト指向であり、重複する部分が多いため、R から Python へのアクセスを容易にする reticulate パッケージや、オープンソースデータサイエンスエコシステム全体の利益のためにポータブルライブラリをサポートする Ursa Labs イニシアチブなどの取り組みに繋がっている。実際には、R と Python のどちらにも強みがある。\nどちらを使うかは、ある程度、応用や結果の伝達の領域よりは重要ではない。\nどちらかを習得することで、もう一方を習得するためのスタートラインに立つことができるのである。\nしかし、ジオコンピュテーションにおいては、Python よりも R の方が大きな利点がある。\nR 言語自体の地理ラスタデータモデルのサポートが大幅に向上したこと (Chapter 2)、それに対応する可視化の可能性 (Chapter 2 と Chapter 9 参照) が含まれる。\n同様に重要なこととして、R は空間統計 を含む統計のための比類ないサポートを持っており、何百ものパッケージ (Python の比ではない) が何千もの統計手法をサポートしている。Python の大きな利点は、汎用プログラミング言語であることである。\nデスクトップソフトウェア、コンピュータゲーム、Web サイト、データサイエンスなど、多くの領域で使用されている。\nPython は、異なる (ジオコンピュテーション) コミュニティ間で唯一共有される言語であることが多く、多くの GIS プログラムをまとめる「接着剤」と見なすことができる。\nQGIS や ArcMap を含む多くのジオアルゴリズムは、Python のコマンドラインからアクセスできるため、コマンドライン GIS のスターター言語として適している3しかし、空間統計と予測モデリングでは、R は他の追随を許さない。\nこれは、R と Python のどちらかを選ばなければならないということではない。Python は、ほとんどの一般的な統計手法をサポートしており (ただし、空間統計の新しい開発は R の方が早くサポートする傾向がある) 、Python で学んだ多くの概念は R の世界にも適用することができる。\nR と同様に、Python も地理データの解析と操作をサポートしており、shapely、geopandas、rasterio、xarray などのパッケージがある。","code":""},{"path":"intro.html","id":"r-ecosystem","chapter":"1 はじめに","heading":"1.5 Rの空間エコシステム","text":"R で地理データを扱う方法はたくさんあり、この分野では何十ものパッケージがある。4\n本書では、この分野の最先端を学ぶと同時に、将来的にも通用する手法を確保するよう努めることとした。\nソフトウェア開発の多くの分野と同様に、R の空間エコシステムは急速に進化している (Figure 1.2)。\nR はオープンソースであるため、Isaac Newton が 1675 で述べたように、「巨人の肩の上に立つ」ことによって、これらの開発は容易に過去の研究の上に構築することができるのである。\nこの方法は、コラボレーションを促進し、「車輪の再発明」を避けることができるという利点がある。\n例えば、sf (Chapter 2 で取り上げている) というパッケージは、前身の sp の上に構築されたものである。R-spatial の開発時間が急増し、R コンソーシアムから、ベクタ形状の保存とアクセスのためのオープンソースの標準とモデルであるシンプルフィーチャのサポートの開発に対する助成金が授与された。\nその結果、sf パッケージ (Section 2.2.1) が誕生したのである。\n複数の場所で sf への絶大な関心を反映している。\n特に、長年にわたって蓄積された R-spatial の知恵を多く含むオープンアクセスのメールリストである R-sig-Geo Archives がそうである。\nFIGURE 1.2: 2013年初頭から現在までの、地理データを扱うための R パッケージのダウンロード数。y軸は、よく使われる cloud.r-project.org CRAN ミラーからの日次ダウンロードの平均数を91日間ローリングウィンドウ法で示している (対数スケール)。\nデータ処理パッケージ dplyr (2014年にリリース) に代表されるように、より広い R コミュニティのシフトが R の空間エコシステムのシフトに影響を与えたことは注目に値する。\ndplyr は 2016年 の後半に tidyverse という「メタパッケージ」に配置され、スタイルを共有する他のパッケージ (例えば、ggplot2 など) と並び、「整えられた (tidy) データ」に重点を置いている。\nロング型データと直感的な名前の関数に焦点を当てた tidyverse アプローチは、絶大な人気を誇っている。\nそのため、「整えられた地理的データ」が求められ、その一部を sf が担ってきている。\ntidyverse の明らかな特徴として、パッケージが調和して動作する傾向がある。\n同等の geoverse はないが、R-spatial エコシステムは、sf と terra を中心に集約されており、本書では両者を解説する。sf に依存するパッケージ は Table 1.2 の通りである。\nChapter 10 で解説する通り、パッケージ間でも他の言語との間でも相互運用性が高い。TABLE 1.2: sf に依存するパッケージで、前月の 1 日あたりの平均ダウンロード数が多い上位 5 位。 2023-11-14 時点で、 526 件のパッケージが sf をインポートしている。","code":""},{"path":"intro.html","id":"history-of-r-spatial","chapter":"1 はじめに","heading":"1.6 R-spatial の歴史","text":"最新の空間パッケージである sf を使うメリットはたくさんあるが、R の空間機能の歴史を知っておくことも重要である。\n古いパッケージには機能、ユースケース、教材が多いため、今でも十分役に立つ。\nR の空間機能は、S 言語の初期の空間パッケージが起源である (Bivand Gebhardt 2000)。\n1990年代には、数多くの S スクリプトが開発され、空間統計のためのパッケージも数多く開発された。\n2000 年までに、「点パターン解析、地球統計学、探索的空間データ解析、空間経済学」という様々な空間手法のための R パッケージが誕生した (Bivand Neteler 2000)。\nこれらのうち、特に spatial、sgeostat、splancs は CRAN でまだ利用可能である (B. S. Rowlingson Diggle 1993; B. Rowlingson Diggle 2017; Venables Ripley 2002; Majure Gebhardt 2016)。\n主要な空間パッケージは Ripley (2001) で説明されており、空間平滑化と補間と点パターン分析の R パッケージの概要が述べられている。\nそのうちの一つ（spatstat）は、最初のリリースから 20 年以上経った今でも活発にメンテナンスされている。続く解説では、空間統計の将来の展望を概説し (Bivand 2001)、人気のある spdep パッケージ (Bivand 2017) の開発の舞台を設定した。\n特筆すべきは、空間インターフェースの標準化、GIS とのデータ交換の効率的なメカニズム、座標参照系 (CRS) などの空間メタデータの取り扱いの必要性に言及したことである。\nこれらの目的はほぼ達成されている。maptools (Bivand Lewin-Koh 2017) もこの時期の重要なパッケージで、 Shapefile を読むための shapelib ライブラリへのインタフェースを提供し、sp に供給していた。\n空間パッケージの拡張レビューでは、基本的な点、線、多角形、ラスタ型と外部ライブラリへのインタフェースを含む「GDALが提供するデータオブジェクト」をサポートするクラスシステムが提案された (Bivand 2003)。\nこれらのアイデアは rgdal パッケージと sp パッケージで実現され、2008 年に初版が出版された Applied Spatial Data Analysis R (ASDAR) (Bivand, Pebesma, Gómez-Rubio 2013) の基礎となった。\nR の空間機能はそれ以来大幅に進化しているが、初期のパイオニアたちのアイデアの上に成り立っている。\n例えば、GDAL と PROJ へのインターフェースは、R の高性能な地理データ /O と CRS の変換機能を可能にしている。それぞれ Chaptger 7 と Chapter 8 で解説する。2003 年にリリースされた rgdal は、R 用の GDAL バインディングを提供し、以前は利用できなかった地理データ形式からインポートする機能を大幅に強化した。\n最初のリリースではラスタドライバのみをサポートしたが、その後の機能強化により、CRS (PROJ ライブラリ経由)、再投影、ベクタのインポートをサポートした。\nこれらの追加機能の多くは Barry Rowlingson によって開発され、2006 年に rgdal コードベースでリリースされた。2005 年にリリースされた sp は、R の空間対応を大きく進展させた。\nsp は、クラスとジェネリック関数を採用し、属性データだけでなく地理座標、点、線、ポリゴンやメッシュ (grid) を扱えるようになった。\nsp は S4 クラスであり、バウンディングボックス、座標参照系 (CRS) 、属性などの情報を Spatial オブジェクトのスロットに格納する。\nこれにより、データ操作で地理データを扱えるようになった。\nさらに、sp では、地理データのための summary() や plot() などのジェネリックメソッドが用意されている。その後の 10 年間で sp クラスは R の地理データ用として急速に普及し、これに依存するパッケージの数は2008年の約 20 から 2013 年には 100 以上に増加した (Bivand, Pebesma, Gómez-Rubio 2013)。\n2019年までに sp に依存するパッケージは 500 を超えた\nsf やその他のパッケージに移行するためその数は減少しているが、sp を使用する主な R パッケージには、空間・時空間地球統計学の gstat 、球面三角法の geosphere などの著名なパッケージもある (Pebesma Graeler 2023; Calenge 2006; Hijmans 2016)。rgdal と sp は空間に関する多くの問題を解決したが、2010年の Google Summer Code プロジェクト ((Bivand Rundel 2023)) で rgeos が開発されるまで、sp オブジェクトでジオメトリ操作を行うことができなかった。\ngIntersection() などの機能により、地理的なオブジェクト間の空間的な関係を見つけたり、その形状を変更したりすることが可能になった (sfによる幾何学的な操作の詳細については、Chapter 5 を参照)。\nsp エコシステムの限界は、ラスタデータのサポートが限定的であることであった。\nこれを克服したのが、2010年に初めてリリースされた raster である (Hijmans 2023b)。\nSection 2.3 にあるように、raster クラスシステムと関数によって、さまざまなラスタ操作が可能になり、現在では terra パッケージに実装されている。\nraster と terra の重要な機能は、RAM に収まらないほど大きなデータセットを扱うことができることであり、ディスク外操作でサポートしている。\nraster および terra は、Section 4.3.2 で説明されているように、マップ代数もサポートしている。このようなクラスシステムや手法の開発と並行して、専用の GIS ソフトのインターフェースとして R がサポートされるようになった。\nGRASS (Bivand 2000) とそれに続くパッケージ spgrass6 と rgrass7 と rgrass は、この方向性の顕著な例であった (Bivand 2016a, 2016b, 2023)。\nR と GIS の橋渡しの例としては、他に QGIS には qgisprocess (Dunnington et al. 2024)、SAGA には Rsagacmd (Pawley 2023) や RSAGA (Brenning, Bangs, Becker 2022)、ArcGIS には RPyGeo (Brenning 2012a, first published 2008) がある (Chapter 10 参照)。R-spatial の開発の大部分は解析と地理的な操作に集中しており、当初は可視化には焦点が当てられていなかった。\nbase と lattice の両方のプロット方式で地図を作る方法を提供していた sp であるが、高度な地図作成機能への要望が高まっていた。\n2009 年にリリースされた RgoogleMaps は、Google Maps や OpenStreetMap (Loecher Ropkins 2015) などのオンラインサービスの「ベースマップ」タイルの上に R 空間データを重ね合わせることができる。\nその後、ggplot2 (Kahle Wickham 2013) に同様の「ベースマップ」タイル機能を追加した ggmap パッケージがリリースされた。\ngpmap は ggplot2 で地図を作ることを容易にしたが、fortify 空間オブジェクトをロング形式のデータフレームに変換する必要があるため、その実用性は限られていた。\nこれは点に対してはうまくいくが、線やポリゴンに対しては計算効率が悪く、各座標 (頂点) が行に変換されるため、複雑な形状を表現するためには巨大なデータフレームが必要となるのである。\n地理的な可視化はベクタデータを中心に行われる傾向があるが、ラスタの可視化は raster でサポートされ、rasterVis のリリースでさらに盛り上がった (Lamigueiro 2018)。\nそれ以来、R での地図作成は話題となり、Chapter 9 で強調されているように、tmap、leaflet、mapview などの専用パッケージがよく使われている。Geocomputation R 第 1 版が出版された 2018 年以降、地理系 R パッケージの開発が加速している。\nraster パッケージの後継である terra は、2020 年に初めてリリースされ、ラスタデータセットを扱う R ユーザーにいくつかの利点をもたらしている [@-terra]。Section 2.3 で説明されているように、前任者よりも高速でより分かりやすいユーザーインターフェースを持っている。Section 2.2.9 で説明する通り、sf パッケージは、2021 年半ばより非投影データに対しては S2 球面幾何学計算を取り入れた。\nまた、2018年以降の R で地理データを表現し作業する方法として、stars と lidR パッケージが追加されている。\nこの展開は、R 環境外の新技術、新標準、ソフトウェアによるものである (Bivand 2021)。\n2018 年に始まった PROJ ライブラリ の変更により、CRS の proj-string 表現を、Section 2.4 と Chapter 7 で説明するように、Well Known Text に置き換えられなければならなくなった。\n2018 年に Geocomputation R の第 1 版が出版されて以来、空間データ可視化のためのいくつかのパッケージが開発・改良されてきた。\n例えば、rayshader パッケージは、レイトレーシングと複数のヒルシェーディング (Morgan-Wall 2021) を介して、印象的でアニメーションしやすい 3D ビジュアライゼーションの開発を可能にする。\n非常に人気のある ggplot2 パッケージは、ggspatial パッケージで、スケールバーと北矢印という新しい空間機能を得た (Dunnington 2021)。\ngganimate はスムーズでカスタマイズ可能な空間アニメーションを可能にする (Pedersen Robinson 2020)。既存の可視化パッケージも改良されたり書き直されたりしている。\n大きなラスタオブジェクトは　tmap　で自動的に縮小され、leafgl や mapdeck を含むパッケージのおかげで、高性能なインタラクティブマップが可能になっている。\n。\nmapsf パッケージ (cartography の後継) は依存関係を減らし、パフォーマンスを向上させるために書き直された (Giraud 2021)。また、tmap はバージョン 4 で大きなアップデートが行われ、内部コードのほとんどが改訂された。2021 年後半に、rgdal、rgeos、maptools の引退計画が発表され、2023 年 10 月に CRAN にアーカイブされた。\nこの 2023 年末の引退は、これらのパッケージを適用する既存のワークフローに大きな影響を与えただけでなく、それらに依存するパッケージにも影響を与えた。\nChapter 2 で説明する sf や terra のような最新の R パッケージは、本書で紹介するジオコンピュテーションのための強力で将来性のある基礎である。","code":""},{"path":"intro.html","id":"演習","chapter":"1 はじめに","heading":"1.7 演習","text":"E1. GIS、GDS、ジオコンピュテーションという用語について考察しなさい。あなたが行おうとしている内容は、どれが最も当てはまるか、理由とともに説明しなさい。E2. QGIS のようなグラフィカルユーザーインターフェース (GUI) ではなく、R のようにスクリプトでジオコンピュテーションを行う理由を 3 つ答えなさい。E3. 2000年に、Stan Openshaw が、ジオコンピュテーションは「便利で有益な実務」な面があると述べた。現実の課題と、地理データの解析、可視化、モデル化から得られるエビデンスから得られる解決策の例を考えなさい。紙と鉛筆 (またはコンピュータ) で、入力と出力を描き、ジオコンピュテーションがどのように役に立つかを示しなさい。","code":""},{"path":"spatial-class.html","id":"spatial-class","chapter":"2 地理データと R","heading":"2 地理データと R","text":"","code":""},{"path":"spatial-class.html","id":"prerequisites-02","chapter":"2 地理データと R","heading":"必須パッケージ","text":"この章から実際の作業がはじまるので、ソフトウェアが必要となる。\nまず、最新版の R (R 4.3.2 またはそれ以降のバージョン) がインストールされているコンピュータを用意しよう。\n文章を読むだけでなく、各章のコードを実行して、ジオコンピュテーションのスキルを身につけることを勧める。R のスクリプト、出力、その他ジオコンピュテーションに関連するものを保存するために、コンピュータに新しいフォルダを作成することから始めるとよいだろう。\nまた、学習をサポートするため、ソースコードをダウンロード またはクローンしておくと良い。\nR コードを書く/実行する/テストする際には、 RStudio (ほとんどの人に推奨) または VS Code などの統合開発環境 (integrated development environment, IDE) をインストールすることを強く勧める。R を初めて使う方は、R のコードを使ったジオコンピュテーションに入る前に、Garrett Grolemund の Hands Programming R や Introduction R などの R 入門リソースに従うことを勧める。\nこれらのリソースには R のインストール方法が詳細に書かれており、Comprehensive R Archive Network (CRAN) から最新バージョンをダウンロードすることも書かれている。\n下記の注記は、Mac と Linux について、ジオコンピュテーションするために R をインストールするための情報がある。\n作業内容を整理し (例: RStudio プロジェクト)、スクリプトに chapter-02-notes.R などのわかりやすい名前を付けて、学習しながらコードを記録するとよい。\nセットアップが完了したら、いよいよコードを実行する。\n以下のパッケージをまだインストールしていない場合、まず以下のコマンドで最初にこの章で使用する基礎的な R パッケージをインストールする。5本書の第 部を再現するために必要なパッケージは、コマンド remotes::install_github(\"geocompx/geocompkg\") でインストールできる。\nこのコマンドは、remotes パッケージの関数 install_packages() を使用して、GitHub コードホスティング、バージョン、およびコラボレーション プラットフォームでホストされているソース コードをインストールする。\n以下のコマンドを実行すると、本書全体を再現するために必要なすべての依存関係がインストールされる (警告: 数分かかる場合がある)。 remotes::install_github(\"geocompx/geocompkg\", dependencies = TRUE)この章で紹介するコードを実行するために必要なパッケージは、以下のように library() 関数で「読み込み」 (厳密には attach) をすることができる。library(sf) の出力では、GEOS などの主要な地理ライブラリのバージョンなどが表示される。これらのライブラリについては、Section 2.2.1 で説明する。他のパッケージには、この本で使用されるデータが含まれている。","code":"\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\ninstall.packages(\"spData\")\ninstall.packages(\"spDataLarge\", repos = \"https://geocompr.r-universe.dev\")\nlibrary(sf)         # ベクタデータ用クラスと関数\n#> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(terra)     # ラスタデータ用クラスと関数\nlibrary(spData)       # 地理データをロード\nlibrary(spDataLarge)  # 大きい地理データをロード"},{"path":"spatial-class.html","id":"intro-spatial-class","chapter":"2 地理データと R","heading":"2.1 イントロダクション","text":"この章では、基本的な地理データモデルであるベクタとラスタについて説明する。(訳注: 本書では、GIS で用いられる vector は「ベクタ」と訳し、R のデータ形式の vector は「ベクトル」と訳す。)\nそれぞれのデータモデルの背景にある理論や、データモデルが得意とする分野を紹介し、R での実際に演習する。ベクタデータモデルは、点、線、ポリゴンを使って世界を表現する。\nこれらのデータセットには、境界が明確に定義されているため、ベクタデータセットは通常、高い精度を持つ (ただし、Section 2.5 で見るように、必ずしも正確ではない)。\nラスタデータモデルは、表面を一定の大きさのセルに分割している。\nラスタデータセットは、ウェブ地図で使用される背景画像の基本であり、航空写真や衛星リモートセンシング装置の発明以来、地理データの重要なソースである。\nラスタは、空間的に特定のフィーチャ (feature、地物とも訳される) を特定の解像度で集約したもので、空間的に一貫性があり、拡張性がある (世界中の多くのラスタデータセットが利用可能)。どちらを使うべきだろうか？\nその答えは、アプリケーションの領域によって変わってくる。社会科学では、人間の居住地が不連続な境界線を持つ傾向があるため、ベクタデータが主流となる傾向がある環境科学において、リモートセンシングデータを使うことも多いため、ラスタが主流となっているラスタデータセットとベクタデータセットは、どちらも多く使われており、併用することも可能である。\n例えば、生態学者や人口統計学者などは、ベクタデータとラスタデータの両方を使うのが一般的である。\nさらに、この 2 つの形式を相互に変換することも可能である (Section 6 参照)。\nベクタデータとラスタデータのどちらを使用するかは、次の章で説明するように、使用する前に基本的なデータモデルを理解することが重要である。\n本書では、ベクタデータとラスタデータセットを扱うために、それぞれ sf と terra パッケージを使用している。","code":""},{"path":"spatial-class.html","id":"vector-data","chapter":"2 地理データと R","heading":"2.2 ベクタデータ","text":"地理ベクタデータモデルは、座標参照系 (coordinate reference system, CRS) 内に位置する点に基づいている。\n点は、バス停の位置のような独立したフィーチャ (地物) を表すことも、線やポリゴンのような複雑な幾何学的形状を形成するために連結されることもある。\n点ジオメトリ (geometry、訳註: ISO 19125 では「形状」や「幾何形状」と訳されているが、本書ではジオメトリとする。) はほとんどの場合 2 次元のみで構成される (3 次元のジオメトリは \\(z\\) の値が追加され、多くの場合、海抜高度を表す)。このシステムにおいて、例えば London は、座標 c(-0.1, 51.5) で表すことができる。\nつまり、その位置は原点から東に \\(-0.1\\) 度、北に \\(51.5\\) 度である。\nこの場合の原点は、地理的 (緯度経度) CRS の経度 (longitude) 0 度 (本初子午線) と緯度 (latitude) 0 度 (赤道) にある (Figure 2.1、左図)。\nまた、同じ点を投影した CRS では、 British National Grid の「東経/北緯」の値はおよそc(530000, 180000) で、ロンドンが CRS の原点から 530 km 東、180 km 北に位置することを意味している。\nこれは視覚的にも確認することができ、幅 100 km の灰色の格子線に囲まれた正方形の領域である「ボックス」が 5 個強、ロンドンを表す点と原点を分けている (Figure 2.1、右のパネル)。National Grid の起点が South West 半島の先の海上にあるため、英国内のすべての場所で正の東経と北緯の値を持つことになる。6\nCRS については、Section 2.4 と Chapter 7 で説明する。\nこの章では、座標が原点からの距離を表す 2 つの数値からなり、通常は \\(x\\) と \\(y\\) の次元であることを知っていれば十分である。\nFIGURE 2.1: 原点 (青丸) を基準にロンドン (赤 X) の位置を表したベクトル (点) データ。左図は、緯度経度 0° を原点とする地理的な CRS。右図は、South West Peninsula の西側の海を原点とする投影 CRS を表している。\nsf パッケージは、地理ベクタデータ用のクラスと、以下に説明する地理計算のための重要な低レベルライブラリへのコマンドラインインターフェースを一貫した方法で提供する。GDAL は、Chapter 8 でカバーされる広範な地理データ形式の読み取り、書き込み、操作のためのものである。PROJ は、Chapter 7 で扱う内容の根幹をなす座標系変換のための強力なライブラリ。GEOS は、投影型 CRS を持つデータに対してバッファや重心の計算などの操作を行う平面ジオメトリエンジン、Chapter 5 でカバーする。S2 は、Google が開発した C++ で書かれた球面幾何学エンジンである。s2 パッケージ経由で、この章の Section 2.2.9 と Chapter 7 でカバーする。これらのインターフェースに関する情報は、パッケージが最初にロードされたときに sf によって表示される。この章の最初にある library(sf) コマンドの下に表示されるメッセージ は、リンクされている GEOS、GDAL、PROJ ライブラリのバージョン (コンピュータや時期によって変わる) と S2 インターフェースがオンになっているかどうかという情報を教えてくれる。\n低レベルライブラリを当前のことと思っているが、様々な地理ライブラリとの緊密な連携がなえれば、再現性のあるジオコンピュテーションは不可能だったであろう。sf の特徴として、投影法未指定のデータで使用するデフォルトのジオメトリエンジンを変更することができる S2 のスイッチを切るには、sf::sf_use_s2(FALSE) というコマンドを使う。つまり、平面ジオメトリエンジン GEOS が、投影されていないデータに対するジオメトリ操作を含む、すべてのジオメトリ操作にデフォルトで使用されることになる。\nSection 2.2.9 で見るように、平面幾何学は二次元の空間に基づいている。\nGEOS などの平面ジオメトリエンジンは「平面」 (投影) 座標を、S2 などの球面ジオメトリエンジンは非投影 (緯度経度) 座標を想定している。この章では、後続の章に備え、sf クラスを紹介する (GEOS インターフェースは Chapter 5 章で、GDAL インターフェースは Chapter 8 で説明する)。","code":""},{"path":"spatial-class.html","id":"intro-sf","chapter":"2 地理データと R","heading":"2.2.1 シンプルフィーチャの概要","text":"シンプルフィーチャ (simple feature、ISO 19125 では、feature は「地物」、simple feature は「単純地物」と訳されこともあるが、本書ではシンプルフィーチャで統一する。) は、Open Geospatial Consortium (OGC) が開発・推奨するオープンスタンダードであり、その活動は後の章で再確認することになる (Section 8.2)。\nシンプルフィーチャは、様々なジオメトリ型を表現する階層的なデータモデルである。\n仕様でサポートされている 18 種類のジオメトリのうち、地理研究の大部分で使用されているのは 7 種類のみである (Figure 2.2 を参照)。\nこれらのコアなジオメトリ型は、R パッケージの sf で完全にサポートされている (Pebesma 2018)。7\nFIGURE 2.2: sf が完全にサポートするシンプルフィーチャ型\nsf は、点、線、ポリゴン、およびそれらの「複合」バージョン (同じ種類のフィーチャをまとめて 1 つのフィーチャとするもの) という一般的なベクタフィーチャのタイプをすべて表現できる (ラスタデータクラスは sf ではサポートされていない)。\nまた、sf はジオメトリコレクションをサポートしており、1 つのオブジェクトに複数のジオメトリ型を格納することができる。\nsf は、データクラスの sp (Pebesma Bivand 2023a)、GDAL と PROJ を通したデータ読み書きの rgdal (Bivand, Keitt, Rowlingson 2023)、GEOS を通した空間演算の rgeos (Bivand Rundel 2023) という 3 パッケージで提供されていたものと同じ機能 (およびそれ以上) を提供する。Chapter 1 の繰り返しになるが、R の地理パッケージは低レベルのライブラリとのインターフェースとの長い歴史がある。sf はこの伝統を受け継ぎ、最新バージョンの幾何演算 GEOS、地理データファイルの読み書きのための GDAL、CRS の表現と変換のための PROJ ライブラリと統一されたインターフェースを提供する。\ns2、すなわち「Google の球面幾何学ライブラリ s2 への R インタフェース」を通して、sf は高速で正確な「非平面形状の測定と操作」 (Bivand 2021) にアクセスすることができる。\n2021年6月に公開された sf バージョン 1.0.0 からは、地理 (経度・緯度) 座標系を持つジオメトリに対してデフォルトで s2 機能が使われるようになった。これは、sf 独自の特徴であり、Python パッケージ GeoPandas など、ジオメトリ演算に GEOS にしか対応していない空間ライブラリとは異なる。\ns2 については、以降の章で説明する。sf は、地理計算のための複数の強力なライブラリを単一のフレームワークに統合しており、これのおかげで高性能ライブラリを用いた再現可能な地理データ解析の世界への「参入障壁」がかなり下げられたと言える。\nsf の機能は、ウェブサイト (r-spatial.github.io/sf/)の 7 つの vignette で詳しく説明されている。\nこれらは、以下のようにオフラインで見ることができる。(訳注: sf などを含めた日本語版の vignette を参照。)最初の vignette で説明されているように、R のシンプルフィーチャオブジェクトはデータフレームに格納され、地理データは通常 ‘geom’ または ‘geometry’ という名前の特別な列を占める。\n本章の冒頭で読み込んだ spData (Bivand, Nowosad, Lovelace 2023) が提供する world データセットを使って、sf オブジェクトの内容とその動作について説明する。\nworld は、空間と属性の列を含む「sf データフレーム」で、属性列の名前は関数 names() によって返される (この例の最後の列は地理情報を含んでいる)。この geom 列の内容は、sf オブジェクトに空間的な力を与える。world$geom は、国別ポリゴンのすべての座標を含むリスト列である。\nsf オブジェクトは、plot() を使ってすぐにプロットすることができる。\nplot() は、R のデフォルト (Base R) の一部であり、他のパッケージが拡張することのできるジェネリック関数というものである。\nsf は、エクスポートされていない (ユーザからはほとんどの場合隠されている) plot.sf() 関数を含んでおり、以下のコマンドでは裏でこの関数が呼ばれ、Figure 2.3 を作成する。\nFIGURE 2.3: sf パッケージを用いた世界の地図で、各属性ごとのファセットを表示。\n多くの GIS プログラムの場合、地理的オブジェクトに対してデフォルトで単一のマップを作成する。一方、sf オブジェクトを plot() すると、データセットの各変数に対してマップが作成されることに注意しておこう。(訳注: このように、属性値を変えた同じ図を並べることを R では「ファセット」(facet) という。ファセットは、元々は切り子という意味。)\nこの動作は、さまざまな変数の空間分布を調べるのに有効であり、Section 2.2.3 で詳しく説明する。より広く言えば、地理的なオブジェクトを空間的な力を持つ通常のデータフレームとして扱うことは、特にデータフレームの扱いに慣れている場合、多くの利点がある。\n例えば、よく使われる summary() 関数では、world オブジェクト内の変数の概要が表示され、便利である。summary() コマンドでは、1 つの変数しか選択していないが、ジオメトリのレポートも出力される。\nこれは、sf オブジェクトのジオメトリ列の「スティッキー」な動作を示している。スティッキーとは、Section 3.2 で見るように、ユーザーが意図的に削除しない限り、ジオメトリが保持されるという性質のことである。\nこの結果は、world に含まれる非空間的データと空間的データの両方を簡単に要約したものである。すべての国に対して、平均寿命は 71 歳 (51 歳未満から 83 歳以上の範囲、中央値は 73 歳) である。このシンプルフィーチャオブジェクトの基本的な動作と内容を深く見てみると、「spatial data frame」と考えるのが妥当であることがわかる。以下のコードは、world オブジェクトの最初の 2 行と最初の 3 列だけを含む sf オブジェクトを返す方法を示している。\nこの出力は、通常の data.frame と比較して、2つの大きな違いがある。それは、追加の地理的メタデータ (Geometry type、Dimension、Bounding box、および座標参照系情報) が含まれていることと、ここでは geom という名前の「幾何学列」が存在することである。「シンプル」であるべきクラスシステムにしては、かなり複雑に見えるだろう。\nしかし、このように物事を整理し、ベクタ地理データセットを扱うのに sf を使うには、それなりの理由がある。sf パッケージがサポートする各ジオメトリ型について説明する前に、sf オブジェクトの構成要素を理解するために基本に立ち返ってみよう。\nSection 2.2.5 は、シンプルフィーチャオブジェクトが、特殊なジオメトリ列を持つデータフレームであることを示している。\nこれらの空間列は、geom または geometry という列名になっている。world$geom は、上記の world オブジェクトの空間要素を指す。\nこれらのジオメトリ列は、クラス sfc の「リスト列」である (Section 2.2.7 を参照)。\nsfc オブジェクトは、sfg (simple feature geometry、Section 2.2.6 で説明) というクラスの 1 つ以上のオブジェクトから構成されている。\nシンプルフィーチャの空間成分の働きを理解するためには、シンプルフィーチャのフィーチャを理解することが必須である。\nこのため、現在サポートされているシンプルフィーチャのフィーチャタイプについて Section 2.2.4 で説明した後、sfg および sfc オブジェクトをベースとする sf オブジェクトを使用して R でこれらを表現する方法について説明する。","code":"\nvignette(package = \"sf\") # どのような vignette があるか表示\nvignette(\"sf1\")         # sf の紹介\nclass(world)\n#> [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nnames(world)\n#>  [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n#>  [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"\nplot(world)\nsummary(world[\"lifeExp\"])\n#>     lifeExp                geom    \n#>  Min.   :50.6   MULTIPOLYGON :177  \n#>  1st Qu.:65.0   epsg:4326    :  0  \n#>  Median :72.9   +proj=long...:  0  \n#>  Mean   :70.9                      \n#>  3rd Qu.:76.8                      \n#>  Max.   :83.6                      \n#>  NA's   :10\nworld_mini = world[1:2, 1:3]\nworld_mini\n#> Simple feature collection with 2 features and 3 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -180 ymin: -18.3 xmax: 180 ymax: -0.95\n#> Geodetic CRS:  WGS 84\n#> # A data frame: 2 × 4\n#>   iso_a2 name_long continent                                                geom\n#> * <chr>  <chr>     <chr>                                      <MULTIPOLYGON [°]>\n#> 1 FJ     Fiji      Oceania   (((-180 -16.6, -180 -16.5, -180 -16, -180 -16.1, -…\n#> 2 TZ     Tanzania  Africa    (((33.9 -0.95, 31.9 -1.03, 30.8 -1.01, 30.4 -1.13,…"},{"path":"spatial-class.html","id":"why-simple-features","chapter":"2 地理データと R","heading":"2.2.2 なぜシンプルフィーチャなのか？","text":"シンプルフィーチャは、QGIS や PostGIS など、多くの GIS アプリケーションのデータ構造の根幹をなすデータモデルとして広く支持されている。\nデータモデルを使用することで、例えば空間データベースからのインポートや空間データベースへのエクスポートなど、他のセットアップとの相互移行が可能になることが大きなメリットである。\n具体的には、「なぜ sf パッケージを使うのか ?」という質問になる。\n答えはたくさんある (シンプルフィーチャモデルの利点と連動している)。高速なデータの読み書きが可能プロット性能の向上sf オブジェクトは、ほとんどの操作でデータフレームとして扱うことができるsf 関数名は比較的一貫性があり、直感的に理解できる (すべて st_ で始まる)。sf 関数は |> 演算子と組み合わせることができ、R パッケージの tidyverse コレクションとうまく機能する。sf が tidyverse パッケージをサポートしていることは、read_sf() 関数で分かる。この関数は、地理ベクタを読むための関数で、Section 8.3.1 で詳しく解説する。\n関数 st_read() は、Base R の data.frame に格納された属性を返すだけである (長いメッセージを表示する。以下のコードチャンクには表示していない。)。一方、read_sf() は、データを tidyverse の tibble として返し、表示は少ない。\n以下、実際の例を紹介する (地理ベクタデータの読み方については Section 8.3.1 を参照)。Chapter 3 で tidyverse 関数を使って sf オブジェクトを操作する方法を示す通り、sf は今や R での空間ベクタデータの分析に最適なパッケージである (spatstat パッケージは、多くの空間統計用の関数を提供するものの最適とは言えない。)。\nspatstat と terra は、空間統計のための多くの関数を提供するパッケージ・エコシステムであり、どちらもベクトル地理データクラスを持っているが、ベクタを扱う点においては sf と同じレベルの取り込みはしていない。\n多くの人気パッケージが sf をベースに構築されており、前章の Section 1.5 にあるように、1日あたりのダウンロード数でその人気が上昇していることが示されている。","code":"\nworld_dfr = st_read(system.file(\"shapes/world.gpkg\", package = \"spData\"))\n#> Reading layer `world' from data source \n#>   `/usr/local/lib/R/site-library/spData/shapes/world.gpkg' using driver `GPKG'\n#> Simple feature collection with 177 features and 10 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.6\n#> Geodetic CRS:  WGS 84\nworld_tbl = read_sf(system.file(\"shapes/world.gpkg\", package = \"spData\"))\nclass(world_dfr)\n#> [1] \"sf\"         \"data.frame\"\nclass(world_tbl)\n#> [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"spatial-class.html","id":"basic-map","chapter":"2 地理データと R","heading":"2.2.3 基本的な地図","text":"基本的な地図は sf の plot() で作成する。\nデフォルトでは、Figure 2.4 の左側のパネルに示されているように、オブジェクトの各変数に対して1つのサブプロット、複数パネルのプロットが作成される。\nプロットされるオブジェクトが単一の変数である場合、連続した色を持つ凡例または「キー」が生成される (右側のパネルを参照)。\nplot() では、 引数 col と border で色を指定することができる。\n\nFIGURE 2.4: sf を使ったプロット。複数変数 (左) と単一変数 (右)。\nプロットは、add = TRUE を設定することで、既存の画像にレイヤとして追加される。8\nこのことを示すために、また、属性と空間データ操作に関する Chapter 3 と Chapter 4 の内容を理解するために、次のコードチャンクはアジアの国々をフィルターして、1つのフィーチャに結合している。世界地図の上にアジア大陸をプロットすることができるようになった。\nadd = TRUE が動作するためには、最初のプロットは 1 つのファセットだけでなければならないことに注意しておこう。\n最初のプロットにキーがある場合は、reset = FALSE を使用する必要がある。\nFIGURE 2.5: 世界各国の上にレイヤとしてアジアを加えたプロット。\nsf の plot() メソッドでマップを修正する方法はいろいろある。\nsf は R の基本的な描画メソッド plot() を拡張しているので、plot() の引数は sf オブジェクトでも動作する (main = などの引数の情報は、?graphics::plot と ?par を参照)。9\nFigure 2.6 は、この柔軟性を、世界地図の上に、直径 (cex = で設定) が各国の人口を表す円を重ねることで表現している。\nこの図の非投影版は、以下のコマンドで作成できる (本章末の練習問題と、スクリプト 02-contplot.R を使って Figure 2.6 を再現することができる。)\nFIGURE 2.6: 国別大陸 (塗りつぶし色で表現) と2015年の人口 (円で表現、面積は人口に比例)\n上記のコードでは、関数 st_centroid() を使って、あるジオメトリ型 (ポリゴン) を別の型 (点) に変換している (Chapter 5 参照)。引数 cex で見た目を変化させることができる。\nsf の plot メソッドには、地理データに特有の引数もある。\nsf オブジェクトをコンテクストでプロットするために、以下の例で expandBB を使用してみよう。\nexpandBB は長さ 4 の数値ベクトルを取り、プロットの外接枠をゼロから相対的に下、左、上、右の順序で拡張する。\nこれを利用して、インドを巨大なアジアの隣国 (東にある中国に重点を置く) の文脈でプロットすると、次のコードチャンクで Figure 2.7 を生成する。 (プロットにテキストを追加することについては、以下の演習を参照)。10\nFIGURE 2.7: インドの文脈で、expandBB論を実証。\nプロットコードでインドを強調するために lwd を使用していることに注意。\nさまざまな種類のジオメトリを表現するためのその他の視覚化技術については、Section 9.2 を参照。なお、ジオメトリについては次のセクションで解説する。","code":"\nplot(world[3:6])\nplot(world[\"pop\"])\nworld_asia = world[world$continent == \"Asia\", ]\nasia = st_union(world_asia)\nplot(world[\"pop\"], reset = FALSE)\nplot(asia, add = TRUE, col = \"red\")\nplot(world[\"continent\"], reset = FALSE)\ncex = sqrt(world$pop) / 10000\nworld_cents = st_centroid(world, of_largest = TRUE)\nplot(st_geometry(world_cents), add = TRUE, cex = cex)\nindia = world[world$name_long == \"India\", ]\nplot(st_geometry(india), expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(st_geometry(world_asia), add = TRUE)"},{"path":"spatial-class.html","id":"geometry","chapter":"2 地理データと R","heading":"2.2.4 ジオメトリの型","text":"ジオメトリは、シンプルフィーチャを構成する基本的な要素である。\nR のシンプルフィーチャは、sf パッケージがサポートする 18 種類のジオメトリ型のいずれかを取ることができる。\nこの章では、最もよく使われる以下の 7 つの型に焦点を当てる: POINT、LINESTRING、POLYGON、MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION。\n一般に、シンプルフィーチャの符号化方式としては、WKB (well-known binary) や WKT (well-known text) が標準的である。\nWKB の表現は通常、コンピュータで読みやすい16進数の文字列である。\nこのため、GIS や空間データベースでは、ジオメトリオブジェクトの転送や保存に WKB を使用している。\n一方、WKT は、シンプルフィーチャを人間が読みやすいテキストマークアップで記述したものである。\nどちらの形式も交換可能であり、ここでは当然 WKT で表す。\n各ジオメトリ型の基本は点である。\n点 (point) とは、2 次元、3 次元、4 次元空間 (詳しくは vignette(\"sf1\") を参照、訳注: 日本語版) の座標で、次のようなものである (Figure 2.8 左図)。\nPOINT (5 2)\n線 (linestring、polyline) とは、例えば、点と点を結ぶ直線の列のことである (Figure 2.8 中央)。LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)ポリゴン (polygon) とは、閉じた非交差環を形成する点の並びのことである。\n「閉じた」とは、ポリゴンの最初と最後の点が同じ座標を持つことを意味する (Figure 2.8 右図)11\n穴のないポリゴン POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\nFIGURE 2.8: 点、線、ポリゴンのジオメトリ。\nここまでは、1 つのフィーチャに 1 つの幾何学的実体しかないジオメトリを作成した。\nしかし、シンプルフィーチャでは、各ジオメトリ型の「複合」バージョンを使用して、1 つのフィーチャに複数のジオメトリを存在させることもできる。複合点 MULTIPOINT (5 2, 1 3, 3 4, 3 2)複合線 MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))複合ポリゴン MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)))\nFIGURE 2.9: MULTI*ジオメトリの説明図。\n最後に、ジオメトリコレクションには、(複合の) 点や線を含むジオメトリの任意の組み合わせを含めることができる (Figure 2.10 を参照)。\nジオメトリコレクション GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))\nFIGURE 2.10: ジオメトリコレクションの説明図\n","code":""},{"path":"spatial-class.html","id":"sf","chapter":"2 地理データと R","heading":"2.2.5 sf クラス","text":"シンプルフィーチャは、フィーチャと非フィーチャという 2 つの部分からなる。\nFigure 2.11 は sf オブジェクトの作成方法を示している。ジオメトリは sfc オブジェクトから、属性は data.frame または tibble から取得される。12\nFIGURE 2.11: sf オブジェクトの構成。\nフィーチャ以外の属性は、フィーチャの名称や、測定値、グループなどの属性を表す。\n属性を説明するために、2023年6月21日の London の気温が 25℃ であることを表してみたい。\nこの例では、ジオメトリ (座標) と、3 つの異なるクラスを持つ属性 (地名、気温、日付) が含まれている。13\nクラス sf のオブジェクトは、属性 (data.frame) とシンプルフィーチャ列 (sfc) を組み合わせて、そのようなデータを表現するものである。\nこれらは、下図のように st_sf() で、London の例を作成する。このコードでは、何が起きるのだろうか? まず、座標を使ってシンプルフィーチャ (sfg) を作成した。\n次に、ジオメトリをシンプルフィーチャ列 (sfc) に変換し、CRS を設定した。\n第三に、属性を data.frame に格納し、sfc のオブジェクトと st_sf() で結合した。\nこの結果、以下に示すように、sf オブジェクトが生成される (一部の出力は省略している)。その結果、sf のオブジェクトは、実際には sf と data.frame という 2 つのクラスを持っていることがわかった。\nシンプルフィーチャとは、単純にデータフレーム (四角い表) であるが、空間属性がリスト列に格納されている。この列は、通常 Section 2.2.1 で説明するように、geometry または geom という名称である。\nこの二面性が、「シンプルフィーチャ」のコンセプトの中心である。\nほとんどの場合、sf は data.frame のように扱われ、動作することができる。\n要するに、シンプルフィーチャとはデータフレームに空間的な拡張を施したものである。","code":"\nlnd_point = st_point(c(0.1, 51.5))                # sfg object\nlnd_geom = st_sfc(lnd_point, crs = \"EPSG:4326\")   # sfc object\nlnd_attrib = data.frame(                          # data.frame object\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2023-06-21\")\n  )\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)   # sf object\nlnd_sf\n#> Simple feature collection with 1 features and 3 fields\n#> ...\n#>     name temperature       date         geometry\n#> 1 London          25 2023-06-21 POINT (0.1 51.5)\nclass(lnd_sf)\n#> [1] \"sf\"         \"data.frame\""},{"path":"spatial-class.html","id":"sfg","chapter":"2 地理データと R","heading":"2.2.6 シンプルフィーチャ・ジオメトリ (sfg)","text":"sfg クラスは、R のさまざまなシンプルフィーチャの種類を表する。点、線、ポリゴン (および複合点などの「複合」対応)、またはジオメトリコレクションである。\n通常は、既存の空間ファイルを読み込むだけなので、自分でジオメトリを作成する面倒な作業は必要ない。\nしかし、もし必要であれば、シンプルフィーチャオブジェクト (sfg) を一から作成するための関数群が用意されている。\nこれらの関数の名前はシンプルで一貫しており、すべて st_ という接頭辞で始まり、小文字でジオメトリ型の名前で終わる。点: st_point()線: st_linestring()ポリゴン: st_polygon()複合点: st_multipoint()複合線: st_multilinestring()複合ポリゴン: st_multipolygon()ジオメトリコレクション: st_geometrycollection()sfg オブジェクトは、3 つの基本的な R データ型から作成することができる。数値ベクトル: ひとつの点行列: 点の集合で、各行が点、多点、または線分を表す。リスト: 行列、マルチ文字列、ジオメトリコレクションなどのオブジェクトの集合体関数 st_point() は、数値ベクトルから単一の点を作成する。その結果、長さ 2、3、4 のベクトルからそれぞれ XY (2 次元座標)、XYZ (3 次元座標)、XYZM (3 次元に追加変数、通常は測定精度) の点タイプが生成されることがわかった。\nXYM タイプは、dim 引数 (dimension の略) で指定する必要がある。これに対して、複合点 (st_multipoint()) と複合線 (st_linestring()) のオブジェクトの場合は、行列 (matrix) を使用する。最後に、複合線、(複合) ポリゴン、ジオメトリコレクションの作成には、リストを使用する。","code":"\nst_point(c(5, 2))                # XY point\n#> POINT (5 2)\nst_point(c(5, 2, 3))             # XYZ point\n#> POINT Z (5 2 3)\nst_point(c(5, 2, 1), dim = \"XYM\") # XYM point\n#> POINT M (5 2 1)\nst_point(c(5, 2, 3, 1))          # XYZM point\n#> POINT ZM (5 2 3 1)\n# rbind 関数により行列の作成が簡単になった。\n## MULTIPOINT\nmultipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2))\nst_multipoint(multipoint_matrix)\n#> MULTIPOINT ((5 2), (1 3), (3 4), (3 2))\n## LINESTRING\nlinestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\nst_linestring(linestring_matrix)\n#> LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n## POLYGON\npolygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\nst_polygon(polygon_list)\n#> POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n## 穴あきポリゴン\npolygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))\npolygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))\npolygon_with_hole_list = list(polygon_border, polygon_hole)\nst_polygon(polygon_with_hole_list)\n#> POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4))\n## MULTILINESTRING\nmultilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                            rbind(c(1, 2), c(2, 4)))\nst_multilinestring(multilinestring_list)\n#> MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n## MULTIPOLYGON\nmultipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))),\n                         list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))))\nst_multipolygon(multipolygon_list)\n#> MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5)), ((0 2, 1 2, 1 3, 0 3, 0 2)))\n## GEOMETRYCOLLECTION\ngeometrycollection_list = list(st_multipoint(multipoint_matrix),\n                              st_linestring(linestring_matrix))\nst_geometrycollection(geometrycollection_list)\n#> GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2),\n#>   LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))"},{"path":"spatial-class.html","id":"sfc","chapter":"2 地理データと R","heading":"2.2.7 シンプルフィーチャ列 (sfc)","text":"1 つの sfg オブジェクトには、1 つのシンプルフィーチャだけが含まれている。\nシンプルフィーチャ列 (simple feature column, sfc) は、sfg オブジェクトのリストであり、さらに、使用中の CRS に関する情報を含めることができる。\n例えば、単純な2つのフィーチャを2つのフィーチャを持つ1つの物体に結合するには、st_sfc() 関数を使用することができる。\nsfc は sf データフレームのジオメトリ列を表すので、これは重要である。ほとんどの場合、sfc オブジェクトは、同じジオメトリ型のオブジェクトを含んでいる。\nしたがって、ポリゴン型の sfg オブジェクトをシンプルフィーチャ列に変換すると、ポリゴン型の sfc オブジェクトもできてしまい、st_geometry_type() で確認することができる。\n同様に、multilinestring のジオメトリ列は、multilinestring 型の sfc オブジェクトになる。また、異なるジオメトリ型の sfg オブジェクトから、sfc オブジェクトを作成することも可能である。前述したように、sfc のオブジェクトは、さらに CRS の情報を格納することができる。\nデフォルト値は、NA (Available) である。st_crs() で確認できる。sfc オブジェクトのすべてのジオメトリは、同じ CRS を持つ必要がある。\nCRS は st_sfc() (または st_sf()) の crs 引数で指定できる。これは crs = \"EPSG:4326\" のようなテキスト文字列として提供される CRS 識別子を取る (他の CRS 表現とこの意味の詳細については Section 7.2 を参照)。","code":"\n# sfc POINT\npoint1 = st_point(c(5, 2))\npoint2 = st_point(c(1, 3))\npoints_sfc = st_sfc(point1, point2)\npoints_sfc\n#> Geometry set for 2 features \n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 1 ymin: 2 xmax: 5 ymax: 3\n#> CRS:           NA\n#> POINT (5 2)\n#> POINT (1 3)\n# sfc POLYGON\npolygon_list1 = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\npolygon1 = st_polygon(polygon_list1)\npolygon_list2 = list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))\npolygon2 = st_polygon(polygon_list2)\npolygon_sfc = st_sfc(polygon1, polygon2)\nst_geometry_type(polygon_sfc)\n#> [1] POLYGON POLYGON\n#> 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n# sfc MULTILINESTRING\nmultilinestring_list1 = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                            rbind(c(1, 2), c(2, 4)))\nmultilinestring1 = st_multilinestring((multilinestring_list1))\nmultilinestring_list2 = list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), \n                            rbind(c(1, 7), c(3, 8)))\nmultilinestring2 = st_multilinestring((multilinestring_list2))\nmultilinestring_sfc = st_sfc(multilinestring1, multilinestring2)\nst_geometry_type(multilinestring_sfc)\n#> [1] MULTILINESTRING MULTILINESTRING\n#> 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n# sfc GEOMETRY\npoint_multilinestring_sfc = st_sfc(point1, multilinestring1)\nst_geometry_type(point_multilinestring_sfc)\n#> [1] POINT           MULTILINESTRING\n#> 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\nst_crs(points_sfc)\n#> Coordinate Reference System: NA\n# 'EPSG' CRS code を参照する識別子で CRS を設定\npoints_sfc_wgs = st_sfc(point1, point2, crs = \"EPSG:4326\")\nst_crs(points_sfc_wgs) # print CRS (only first 4 lines of output shown)\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#> ..."},{"path":"spatial-class.html","id":"sfheaders","chapter":"2 地理データと R","heading":"2.2.8 sfheaders パッケージ","text":"\nsfheaders は、sf オブジェクトの構築、変換、操作を高速化する R パッケージである (Cooley 2020)。\nこれは、ベクトル、行列、データフレームから sf オブジェクトを、 sf ライブラリに依存せずに高速に構築することに重点を置いており、 ヘッダファイルを通じてその基礎となる C++ コードを公開する (sfheaders という名前はこのためである。)。\nこのアプローチにより、他の人がコンパイルされた高速に動作するコードを使って拡張することが可能になる。\nsfheaders のコア関数すべてに対して、対応する C++ の実装があり、その説明は Cpp vignette で説明されている。\n多くの人にとって、R の関数は、パッケージの計算速度の恩恵を受けるのに十分すぎるほどだろう。\nsfheaders は sf とは別に開発されたが、完全に互換性があり、前のセクションで説明したタイプの有効な sf オブジェクトを作成することを目的としている。sfheaders の最も単純な使用例は、sfg、sfc、sf オブジェクトの構築例とともに、以下のコードチャンクで示されている。sfg_POINT に変換されたベクトルsfg_LINESTRING に変換された行列sfg_POLYGON に変換されたデータフレームまず、最も単純な sfg オブジェクトを作成する。これは、v というベクタに割り当てられた、1 つの座標ペアである。上の例では、sf が読み込まれていないときに sfg オブジェクト v_sfg_sfh が出力され、その基本的な構造を示している。\nsf が読み込まれた場合 (今回のように)、上記のコマンドの結果は sf のオブジェクトと区別がつかない。次の例は、sfheaders が行列とデータフレームから sfg オブジェクトを作成する方法を示している。オブジェクト v、m、df を再利用して、以下のように簡単な特徴列 (sfc) を作ることもできる (出力は示していない)。同様に、sf オブジェクトも以下のように作成することができる。いずれの例も CRS は定義されていない。\nもし、sf 関数を使った計算や幾何演算をする予定があるのなら、CRS を設定することを勧める (詳しくは、Chapter 7 参照)。また、sfheaders は sf オブジェクトの「分解」と「再構築」を得意としている。つまり、ジオメトリ列を、各頂点の座標とジオメトリフィーチャ (および複数フィーチャ) の ID に関するデータを含むデータフレームに変換するのである。\nChapter 5 で取り上げた、異なるタイプのジオメトリ列を「キャスト」する際に、高速かつ信頼性の高い処理を行うことができる。\nパッケージの documentation やこの本のために開発されたテストコードでのベンチマークでは、このような操作に対して sf パッケージよりもはるかに高速であることが示されている。","code":"\nv = c(1, 1)\nv_sfg_sfh = sfheaders::sfg_point(obj = v)\nv_sfg_sfh # sf をロードせずに出力\n#>      [,1] [,2]\n#> [1,]    1    1\n#> attr(,\"class\")\n#> [1] \"XY\"    \"POINT\" \"sfg\" \nv_sfg_sf = st_point(v)\nprint(v_sfg_sf) == print(v_sfg_sfh)\n#> POINT (1 1)\n#> POINT (1 1)\n#> [1] TRUE\n# matrices\nm = matrix(1:8, ncol = 2)\nsfheaders::sfg_linestring(obj = m)\n#> LINESTRING (1 5, 2 6, 3 7, 4 8)\n# data_frames\ndf = data.frame(x = 1:4, y = 4:1)\nsfheaders::sfg_polygon(obj = df)\n#> POLYGON ((1 4, 2 3, 3 2, 4 1, 1 4))\nsfheaders::sfc_point(obj = v)\nsfheaders::sfc_linestring(obj = m)\nsfheaders::sfc_polygon(obj = df)\nsfheaders::sf_point(obj = v)\nsfheaders::sf_linestring(obj = m)\nsfheaders::sf_polygon(obj = df)\ndf_sf = sfheaders::sf_polygon(obj = df)\nst_crs(df_sf) = \"EPSG:4326\""},{"path":"spatial-class.html","id":"s2","chapter":"2 地理データと R","heading":"2.2.9 S2 による球面幾何学操作","text":"球面幾何学エンジンは、世界が丸いという事実に基づいているが、2点間の直線やポリゴンに囲まれた面積の計算など、地盤計算のための簡単な数学的手続きは、平面 (投影) 幾何学を前提としている。\nsf バージョン 1.0.0 以降、R は Google の S2 球面幾何エンジンとのインターフェース s2 インターフェースパッケージにより、球面幾何の操作を「すぐに」(かつデフォルトで) サポートする。\nS2 は、離散型グローバルグリッドシステム (Discrete Global Grid System, DGGS) の例として、おそらく最もよく知られている。\nもう一つの例は、H3 グローバルな六角形の階層的な空間インデックスである (Bondaruk, Roberts, Robertson 2020)。S2 は、文字列を使って地球上の任意の場所を記述できる機能があり、これは役立つ場合もあるかもしれない。しかし、 sf が S2 を採用する利点は、距離、バッファ、面積計算などの計算のためのドロップイン関数を提供している点である。これは sf のドキュメントで説明されており、コマンド vignette(\"sf7\") で開くことができる (訳注: 日本語版)。sf は、S2 に対して、オンとオフの 2 つのモードで動作させることができる。\nS2 ジオメトリエンジンはデフォルトでオンになっている。以下のコマンドで確認することができる。ジオメトリエンジンをオフにした結果の例を以下に示す。この章で作成した india オブジェクトの周りにバッファを作成する (S2 をオフにしたときに発せられる警告に注意) (Figure 2.12)。\nFIGURE 2.12: S2 ジオメトリエンジンをオフにした場合の結果の例。インド周辺のバッファを同じコマンドで作成したが、紫色のポリゴンオブジェクトは S2 をオンにして作成したため、バッファは 1 m になった。大きい薄緑色のポリゴンは S2 をオフにして作成したため、バッファは 1 度となり、不正確。\nFigure 2.12 右は、1 度のバッファは、india ポリゴンの周囲を等しく囲んではいないため不正確である (詳細は、Section 7.4 を参照)。本書では、特に明記していない限り、S2 がオンになっていることを前提に説明する。\n以下のコマンドで再度オンにする。","code":"\nsf_use_s2()\n#> [1] TRUE\nindia_buffer_with_s2 = st_buffer(india, 1) # 1 メートル\nsf_use_s2(FALSE)\n#> Spherical geometry (s2) switched off\nindia_buffer_without_s2 = st_buffer(india, 1) # 1 メートル\n#> Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\n#> endCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n#> dist is assumed to be in decimal degrees (arc_degrees).\nsf_use_s2(TRUE)\n#> Spherical geometry (s2) switched on"},{"path":"spatial-class.html","id":"raster-data","chapter":"2 地理データと R","heading":"2.3 ラスタデータ","text":"空間ラスタデータモデルは、連続した格子状のセル (ピクセルとも呼ばれる; Figure 2.13 :) で世界を表現する。\nこのデータモデルでは、各セルが同じ一定の大きさを持つ、いわゆる正方形のグリッドを指すことが多く、本書では正方形のグリッドにのみ焦点を当てることにする。\nしかし、他にも回転格子、せん断格子、直線格子、曲線格子など、いくつかの種類の格子が存在する (Pebesma Bivand (2023c) の第 1 章、または Tennekes Nowosad (2024) の第 2 章を参照のこと)。ラスタデータモデルは通常、ラスタヘッダと、等間隔に並んだセル (ピクセルとも呼ばれる; Figure 2.13 :) を表す行列 (行と列を持つ) から構成される。14\nラスタヘッダは、CRS、範囲、原点を定義する。\n原点は、多くの場合、行列の左下隅の座標である (ただし、terra パッケージでは、デフォルトで左上隅が使用される (Figure 2.13:B))。\nヘッダは、列数、行数、セルサイズの解像度によって範囲を定義する。解像度 (resolution) は、以下の式から得られる。\\[\n\\text{resolution} = \\frac{\\text{xmax} - \\text{xmin}}{\\text{ncol}}, \\frac{\\text{ymax} - \\text{ymin}}{\\text{nrow}}\n\\]原点から出発して、セルの ID を使うか (Figure 2.13:B)、明示的に行と列を指定することで、1 つのセルに対して簡単にアクセスし、変更することができる。\nこの行列表現により、直方体のベクタポリゴンのように、各セルの角の 4 点の座標を明示的に保存する必要がなくなる (実際には、原点という 1 つの座標しか保存しない)。\nこれと地図代数 (Section 4.3.2) により、ラスタ処理はベクタデータ処理よりもはるかに効率的で高速に処理することができる。ベクタデータとは異なり、ラスタレイヤの1つのセルには1つの値しか入れることができない。15\n値は、数値またはカテゴリ (Figure 2.13:C) である。\nFIGURE 2.13: ラスタデータの種類\nラスタ地図は通常、標高、気温、人口密度、スペクトルデータなどの連続的な現象を表現する。\n土壌や土地被覆クラスなどの離散的なフィーチャも、ラスタデータモデルで表現することができる。\nラスタデータセットの両方の使い方を説明するために、Figure 2.14、ラスタデータセットでは、離散的なフィーチャの境界が不鮮明になることがあることを示している。\nアプリケーションの性質によっては、離散的なフィーチャのベクタ表現がより適切な場合もある。\nFIGURE 2.14: 連続ラスタとカテゴリラスタの例。\n","code":""},{"path":"spatial-class.html","id":"r-packages-for-working-with-raster-data","chapter":"2 地理データと R","heading":"2.3.1 ラスタデータを扱うための R パッケージ","text":"過去 20 年の間に、ラスタデータセットを読み込んで処理するためのパッケージがいくつか開発された。\nSection 1.6 にあるように、その中でも特に raster は 2010 年にリリースされ、R のラスタ機能を一変させ、terra や stars が開発されるまでこの分野の最高峰のパッケージとなった。\n最近開発されたこの 2 つのパッケージは、ラスタデータを扱うための強力で高性能な機能を備えており、両者の使用例にはかなりの重複がある。\nこの本では、古くて (多くの場合) 遅い raster に代わる terra に焦点を当てる。\nterra のクラスシステムについて学ぶ前に、このセクションでは terra と stars の類似点と相違点について説明する。この知識は、様々な状況下でどちらが最も適切かを判断するのに役立つ。まず、terra は最も一般的なラスタデータモデル (通常のグリッド) に焦点を当て、stars はあまり一般的ではないモデル (通常のグリッド、回転グリッド、シアーグリッド、レクチリニアグリッド、カーヴィリニアグリッドなど) も保存できるようになっている。\n通常、terra は 1 層または多層のラスタを扱うが16、stars パッケージはラスタデータキューブを保存する方法を提供する。一方、stars パッケージは、ラスタデータキューブ (多くのレイヤ (バンドなど)、多くの時間 (月など)、多くの属性 (センサータイプ とセンサータイプ B など) を持つラスタオブジェクト) を保存する方法を提供する。\n重要なのは、どちらのパッケージでも、データキューブのすべてのレイヤまたは要素が同じ空間寸法および範囲を持っている必要があることである。\n第二に、どちらのパッケージもラスタデータをすべてメモリに読み込むか、メタデータだけを読み込むかを選択できる。これは通常、入力ファイルのサイズに応じて自動的に行われる。\nしかし、両者はラスタ値の保存方法が大きく異なる。\nterra は C++ のコードをベースにしており、ほとんど C++ のポインタを使用している。\nstars は、小さいラスタでは配列のリストとして、大きいラスタでは単なるファイルパスとして値を格納する。\n第三に、stars の関数は sf のベクタオブジェクトや関数と密接に関係しており、一方 terra はベクタデータに対して独自のオブジェクトクラス、すなわち SpatVector を使用しているが、 sf も受け付ける。17\n第四に、両パッケージは、そのオブジェクトに対して様々な機能がどのように働くかについて、異なるアプローチを持っている。\nterra パッケージは、ほとんどが多数の組み込み関数に依存しており、各関数は特定の目的 (例えば、リサンプリングやトリミングなど) を持っている。\n一方、stars はいくつかの組み込み関数 (通常 st_ で始まる名前) を使用し、また既存の dplyr 関数 (例えば filter() や slice()) に対するメソッドも持ち、既存の R 関数 (例えば split() や aggregate()) に対する独自のメソッドを持つ。オブジェクトを terra から stars に変換する (st_as_stars() を使用)、またはその逆 (rast() を使用) が簡単である点が重要である。\nまた、stars パッケージの最も包括的な紹介として、Pebesma Bivand (2023c) を読むことを勧める。","code":""},{"path":"spatial-class.html","id":"an-introduction-to-terra","chapter":"2 地理データと R","heading":"2.3.2 terra の概要","text":"\nterra パッケージ は、R のラスタオブジェクトをサポートする。\nラスタデータセットの作成、読み込み、書き出し、操作、処理を行うための豊富な関数群を提供する。\nterra の機能は、より成熟した raster パッケージとほぼ同じであるが、いくつかの相違点がある。terra の関数は通常、raster の対応する関数よりも計算効率が高い。\n一方、raster クラスシステムは人気があり、他の多くのパッケージで使用されている。\n古いスクリプトやパッケージとの後方互換性を確保するために、2種類のオブジェクトをシームレスに変換することができる。例えば、raster パッケージの raster()、stack()、brick() などの関数がある (地理データを扱うための R パッケージの進化については、前の章を参照)。ラスタデータを操作するための関数に加え、terra はラスタデータセットを扱うための新しいツールを開発するための基盤となる低レベルの関数を多数提供している。\nまた、terra では、メインメモリに収まりきらないような大きなラスタデータセットを扱うことができる。\nこの場合、terra はラスタを小さなチャンクに分割し、ラスタファイル全体を RAM にロードする代わりに、それらを繰り返し処理することが可能である。terra の概念を説明するために、spDataLarge (Nowosad Lovelace 2023) のデータセットを使ってみよう。\nこれは、Zion 国立公園 (米国 Utah 州) のエリアをカバーする数個のラスタオブジェクトと 1 個のベクタオブジェクトから構成されている。\n例えば、srtm.tif はこの地域のデジタル標高モデルである (詳しくは、そのドキュメント ?srtm を参照)。\nまず、my_rast という名前の SpatRaster オブジェクトを作成しよう。コンソールにラスタの名前を入力すると、ラスタヘッダ (寸法、解像度、範囲、CRS) といくつかの追加情報 (クラス、データソース、ラスタ値の要約) が出力される。dim() は行、列、レイヤの数、ncell() はセル (ピクセル) の数、res() は空間解像度、ext() は空間範囲、crs() は CRS (ラスタ再投影は Section 7.8 で扱う) をそれぞれ報告する専用の関数である。\ninMemory() は、ラスタデータがメモリに格納されているか、ディスクに格納されているかを報告する。sources がファイルの位置を示す。","code":"\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast = rast(raster_filepath)\nclass(my_rast)\n#> [1] \"SpatRaster\"\n#> attr(,\"package\")\n#> [1] \"terra\"\nmy_rast\n#> class       : SpatRaster \n#> size        : 457, 465, 1  (nrow, ncol, nlyr)\n#> resolution  : 0.000833, 0.000833  (x, y)\n#> extent      : -113, -113, 37.1, 37.5  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#> source      : srtm.tif \n#> name        : srtm \n#> min value   : 1024 \n#> max value   : 2892"},{"path":"spatial-class.html","id":"basic-map-raster","chapter":"2 地理データと R","heading":"2.3.3 基本的な地図の作り方","text":"sf パッケージと同様に、terra も独自のクラスに対して plot() メソッドを提供する。\n次のコマンドでは、plot() 関数を用いて Figure 2.15 を作成する。\n\nFIGURE 2.15: 異本的なラスタのプロット。\nR でラスタデータをプロットすることはこのセクションの範囲外であるが、他にもいくつかのアプローチがある。plotRGB() terra パッケージの関数で、SpatRaster オブジェクトの 3 つのレイヤを基にプロットを作成する。ラスタおよびベクタオブジェクトの静的および対話型マップを作成するための tmap などのパッケージ (Chapter 9 を参照)関数、例えば rasterVis パッケージの levelplot() は、経時変化を視覚化する一般的な手法であるファセットを作成するためのものである。","code":"\nplot(my_rast)"},{"path":"spatial-class.html","id":"raster-classes","chapter":"2 地理データと R","heading":"2.3.4 ラスタクラス","text":"\nSpatRaster クラスは、terra のラスタオブジェクトを表する。\nR でラスタオブジェクトを作成する最も簡単な方法は、ディスクまたはサーバからラスタファイルを読み込むことである (Section 8.3.2)。\nterra パッケージは、GDAL ライブラリの助けを借りて、多数のドライバをサポートしている。\nファイルからのラスタは、通常、ヘッダとファイル自体へのポインタを除いて、完全に RAM に読み込まれるわけではない。ラスタは、同じ rast() 関数を使用して、ゼロから作成することもできる。\n次のコードチャンクで、新しい SpatRaster オブジェクトが生成してみよう。\n出来上がったラスタは、本初子午線と赤道を中心とした 36 個のセル (nrows と ncols で指定された 6 列と 6 行) から構成されている (xmin、xmax、ymin、ymax パラメータを参照)。\n各セルには値 (vals) が、1 はセル 1、2 はセル 2、といったように割り当てられている。\nrast() は、(matrix() とは異なり) 左上から行ごとにセルを埋めていく。つまり、一番上の行には 1 から 6、二番目の行には 7 から 12 の値が含まれる。\nラスタオブジェクトを作成する他の方法については、?rast を参照。行と列の数、範囲 (xmin, xmax, ymin, ymax) を考えると、解像度は 0.5 でなければならない。\n解像度の単位は基礎となる CRS の単位である。\nラスタオブジェクトのデフォルトの CRS は WGS84 なので、ここでは度である。\nしかし、crs 引数で他の CRS を指定することもできる。SpatRaster クラスは、複数のレイヤを処理することもできる。これは通常、1 つのマルチスペクトル衛星ファイルまたは時系列のラスタに対応する。nlyr() は、SpatRaster オブジェクトに格納されているレイヤの数を取得する。複数レイヤのラスタオブジェクトの場合、[[ と $ を使ってレイヤを選択できる。例えば、multi_rast$ landsat_1 や multi_rast[[\"landsat_1\"]] というように使う。\nterra::subset() も使うことができる。\n第 2 引数にレイヤ番号またはレイヤ名を指定する。逆の操作として、複数の SpatRaster オブジェクトを 1 つにまとめることも c 関数で可能である。","code":"\nsingle_raster_file = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nsingle_rast = rast(raster_filepath)\nnew_raster = rast(nrows = 6, ncols = 6, resolution = 0.5, \n                  xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n                  vals = 1:36)\nmulti_raster_file = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nmulti_rast = rast(multi_raster_file)\nmulti_rast\n#> class       : SpatRaster \n#> size        : 1428, 1128, 4  (nrow, ncol, nlyr)\n#> resolution  : 30, 30  (x, y)\n#> extent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\n#> coord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \n#> source      : landsat.tif \n#> names       : landsat_1, landsat_2, landsat_3, landsat_4 \n#> min values  :      7550,      6404,      5678,      5252 \n#> max values  :     19071,     22051,     25780,     31961\nnlyr(multi_rast)\n#> [1] 4\nmulti_rast3 = subset(multi_rast, 3)\nmulti_rast4 = subset(multi_rast, \"landsat_4\")\nmulti_rast34 = c(multi_rast3, multi_rast4)"},{"path":"spatial-class.html","id":"crs-intro","chapter":"2 地理データと R","heading":"2.4 地理・投影座標参照系","text":"\nベクタとラスタの空間データ型は、空間データに内在する概念を共有している。\nこのうち最も基本的なものは座標参照系 (CRS) で、データの空間要素が地球 (または他の天体) の表面とどのように関連しているかを定義するものである。\nCRSには、本章の冒頭で紹介したように、地理的なものと投影的なものがある (Figure 2.1 参照)。\nこのセクションでは、各タイプについて説明し、CRSの設定、変換、クエリについて深く掘り下げた Chapter 7 の基礎を築く。","code":""},{"path":"spatial-class.html","id":"geographic-coordinate-reference-systems","chapter":"2 地理データと R","heading":"2.4.1 地理座標参照系","text":"\nCRS は、地球上の任意の位置を経度と緯度という 2 つの値で特定する (Figure 2.17 左図)。\n経度 (longitude) は、本初子午線面からの角距離で東西方向の位置である。\n緯度 (latitude) は赤道面の北または南の角度距離である。\nそのため、地理的な CRS の距離はメートル単位ではない。\nこのことは、Section 7 で示されているように、重要な結果をもたらす。CRS における地球の表面は、球面または楕円体面で表現される。\n球体モデルは、地球がある半径の完全な球体であると仮定したもので、単純であるという利点があるが、実際は完全な球体ではないため不正確である。\n楕円体型モデルは、赤道半径と極半径の 2 つのパラメータで定義され、球体よりは正確になる。\n地球は圧縮されているので、これが適している。すなわち、赤道半径は極半径より約 11.5 km 長い (Maling 1992)。18楕円体は、CRS のより広い構成要素である測地系 (datum) の一部である。\nどの楕円体を使うか、直交座標と地表の位置の正確な関係などの情報が含まれている。\n測地系には、地心座標系 (WGS84 など) とローカル測地系 (NAD83 など) がある。\nこの 2 種類の測地系の例は、Figure 2.16 で見ることができる。\n黒線は地心測地系を表し、その中心は地球の重心に位置し、特定の場所に最適化されていない。\n紫色の破線で示されるローカル測地系は、楕円面を特定の位置の地表に合わせるようにずらしたものである。\nこれにより、例えば大きな山脈による地表の局所的な変動を、ローカル CRS で説明することができる。\nこれは Figure 2.16 に見られるように、フィリピンの地域にはローカル測地系が適合しているが、地球表面の他の大部分とはずれているのである。\nFigure 2.16 の二つの測地系は、ジオイド (地球平均海面のモデル) の上に載せられている。19\nFIGURE 2.16: ジオイドの上に表示された地心座標系およびローカル測地系データ (フォールスカラーと、スケールファクター1万による垂直方向の誇張)。ジオイドの画像は Ince et al. (2019) の作品から流用したものである。\n","code":""},{"path":"spatial-class.html","id":"projected-coordinate-reference-systems","chapter":"2 地理データと R","heading":"2.4.2 投影座標参照系","text":"\nすべての投影 CRS は、前節で説明した地理的 CRS に基づいており、地図投影に依存して、地球の 3 次元表面を投影 CRS の東方向距離 (easting) と北方向距離 (northing) (x と y) の値に変換している。\n投影 CRS は、暗黙のうちに平坦な表面上のデカルト座標に基づいている (Figure 2.17 右図)。\n投影 CRS には、原点、X 軸とY 軸、メートルなど線形単位がある。この推移は、何らかのデフォルメを加えないとできない。\nそのため、地表の面積、方向、距離、形状など、地表の一部の性質が歪んでしまう。\n投影座標参照系は、これらの特性のうち1つか2つしか保持できない。\n等面積投影は面積を、方位投影は方向を、等距離投影は距離を、コンフォーマ投影は局所形状を保持する。投影の種類は、円錐型、円筒型、平面型 (方位角型) の 3 つに大別される。\n円錐図法では、地球表面が 1 本の接線または 2 本の接線に沿って円錐に投影される。\nこの投影では接線に沿って歪みが最小になり、接線からの距離に応じて歪みが大きくなる。\nそのため、中緯度地域の地図に最も適している。\n円筒投影は、表面を円筒に写すものである。\nこの投影は、1 本の接線または 2 本の接線に沿って地表に接することによっても作成することができる。\n円筒形の投影は、全世界の地図を作成するときに最もよく使われる。\n平面投影は、地球上のある点または接線に沿った平面上にデータを投影するものである。\n極域の地図作成によく使われる。\nsf_proj_info(type = \"proj\") は、PROJ ライブラリでサポートされている利用可能な投影のリストを提供する。さまざまな投影法、その種類、特性、適性についての簡単な要約は www.geo-projections.com で見ることができる。\nChapter 7 では、CRS について説明し、ある CRS から別の CRS に投影する方法について拡張する。\n今は、以下を知っておけば十分である。座標系は地理的なオブジェクトの重要な構成要素であることデータがどの CRS であるか、また、地理的 (緯度経度) か投影 (通常はメートル) かは重要であり、R による空間およびジオメトリ操作の処理方法に影響を及ぼす。sf オブジェクトの CRS は関数 st_crs() で問い合わせることができ、terra オブジェクトの CRS は関数 crs() を用いて問い合わせることができる。\nFIGURE 2.17: ベクタデータ型の地理座標系 (WGS 84、左) と投影座標系 (NAD83 / UTMゾーン12N、右) の例。\n","code":""},{"path":"spatial-class.html","id":"units","chapter":"2 地理データと R","heading":"2.5 単位","text":"CRS の重要な特徴は、空間単位の情報を含んでいることである。\n家の寸法がフィートなのかメートルなのかが大事なように、地図でも同じことが言える。\nページや画面上の距離と地上での距離の関係を示すために、地図上にスケールバーやその他の距離表示を追加することは、地図作成の良い習慣である。\n同様に、ジオメトリデータやセルを測定する単位を正式に指定して文脈を示し、その後の計算が文脈に沿って行われるようにすることが重要である。新しい特徴として、sf オブジェクトのジオメトリデータは、単位をネイティブにサポートしている。\nこれは、sf における距離、面積、その他の幾何学的計算が、units パッケージ (Pebesma, Mailund, Hiebert 2016) で定義された units 属性に付属する値を返すことを意味している。\nこれは、単位の違いによる混乱を防ぎ (ほとんどの CRS はメートル、一部の CRS はフィート)、次元に関する情報を提供するのに有利である。\nルクセンブルクの面積を計算する以下のコードで、その様子を見てみよう。\n出力は平方メートル (m2) の単位で、結果が 2 次元空間を表していることがわかる。\nこの情報は属性として保存され (興味のある読者は attributes(st_area(luxembourg)) で確認できる)、人口密度 (単位面積当たりの人口、通常は 1 km2) など単位を使用する後の計算に反映させることができる。\n単位を報告することで混乱を防ぐことができる。\nluxembourg の例で言えば、単位が明記されていないままだと、単位がヘクタールであると誤って判断してしまう可能性がある。\nこの膨大な数字を消化しやすいサイズに変換するために、結果を 100 万 (1 平方キロメートルの中の平方メートル数) で割れば良いと思うかもしれない。しかし、その結果を再び平方メートルとするのは誤りである。\n解決策としては、units パッケージで正しい単位を設定することである。ラスタデータの場合、単位も同様に重要である。\nしかし、今のところ単位をサポートする空間パッケージは sf だけであり、ラスタデータを扱う場合は分析単位の変更 (例えば、ピクセル幅を帝国単位から 10 進数に変換するなど) に注意して取り組む必要がある。\nmy_rast オブジェクト (上記参照) は、単位に十進角を持つ WGS84 投影を使用している。\nその結果、その分解能も 10 進数で示されるが、res() 関数は単に数値ベクトルを返すだけなので、このことを知っている必要がある。ユニバーサル横メルカトル (Universal Transverse Mercator, UTM) 投影を使うと、単位が変わる。ここでも、res() コマンドは単位を持たない数値ベクタを返すので、UTM 投影の単位がメートルであることを知っておく必要がある。","code":"\nluxembourg = world[world$name_long == \"Luxembourg\", ]\nst_area(luxembourg) # 最近の sf の場合 s2 パッケージが必要\n#> 2.41e+09 [m^2]\nst_area(luxembourg) / 1000000\n#> 2409 [m^2]\nunits::set_units(st_area(luxembourg), km^2)\n#> 2409 [km^2]\nres(my_rast)\n#> [1] 0.000833 0.000833\nrepr = project(my_rast, \"EPSG:26912\")\nres(repr)\n#> [1] 83.5 83.5"},{"path":"spatial-class.html","id":"ex2","chapter":"2 地理データと R","heading":"2.6 演習","text":"E1. spData パッケージにある world オブジェクトのジオメトリ列に summary() 関数を使いなさい。出力は、ジオメトリ型は何か?国の数は?座標参照系 (CRS) は?E2. Section 2.2.3 の世界地図を作ったコードを実行しなさい。\n自分の画像と本の画像を比較し、似ている点を 2 点、異なる点を 2 点見つけなさい。引数 cex は何をするか (?plot 参照)?なぜ cex を sqrt(world$pop) / 10000 としたのか?追加: 別の方法で世界人口を可視化してみなさい。E3. plot() を使い、Nigeria の地図を作りなさい (Section 2.2.3 参照)。plot() 関数の引数 lwd、col、expandBB を調整しなさい。難: text() のドキュメントを読み、地図に注釈をつけなさい。E4. 10 行、10 列の空の SpatRaster を作り、my_raster となつけなさい。\nラスタに 0 から 10 の値を適当に割り当て、プロットしなさい。E5. パッケージから raster/nlcd.tif ファイルを読み込みなさい。\nファイルのプロパティからどのような情報を得られるか?E6. spDataLarge パッケージのファイル raster/nlcd.tif の CRS を確認しなさい。\nどのような情報を学ぶことができるか?","code":""},{"path":"attr.html","id":"attr","chapter":"3 属性データ操作","heading":"3 属性データ操作","text":"","code":""},{"path":"attr.html","id":"prerequisites-03","chapter":"3 属性データ操作","heading":"必須パッケージ","text":"この章では、以下のパッケージがインストールされ、ロードされている必要がある。この章は spData に依存している。コード例で使用されるデータセットをロードする。また、Section 3.2.5 でデータの「整頓 (tidy)」操作を実行したい場合は、tidyr パッケージ、またはその一部である tidyverse がインストールされていることを確認しておこう。","code":"\nlibrary(sf)     # Chapter 2 で紹介したベクタデータパッケージ\nlibrary(terra)  # Chapter 2 で紹介したラスタデータパッケージ\nlibrary(dplyr)  # データフレーム操作用 tidyverseパッケージ\nlibrary(spData) # Chapter 2 で紹介した空間データパッケージ"},{"path":"attr.html","id":"introduction-03","chapter":"3 属性データ操作","heading":"3.1 イントロダクション","text":"\n属性データとは、地理 (ジオメトリ) データに関連する空間以外の情報である。\nバス停を例にとると、その位置は通常、名称に加えて緯度・経度の座標 (ジオメトリデータ) で表現される。\n例えば、London の Elephant & Castle / New Kent Road の停留所の座標は、経度 \\(-0.098\\) 度、緯度 51.495 度で、Chapter 2 で説明した sfc の表現では POINT (-0.098 51.495) と表すことができる。\nこの章のトピックは、POINT フィーチャの属性のうち、name のような名称の属性 (シンプルフィーチャの用語を使用する) である。\nまた、ラスタデータにおける特定のグリッドセルの標高値 (属性) もその一例である。\nラスタデータモデルは、ベクタデータモデルと異なり、グリッドセルの座標を間接的に格納するため、属性情報と空間情報の区別が明確ではない。\nラスタ行列の 3 行 4 列目の画素を考えてみよう。\nその空間的な位置は、行列内のインデックスで定義される。原点から x 方向に 4 セル (地図上では通常東と右)、y 方向に 3 セル (通常南と下) 移動させる。\nラスタの解像度は、ヘッダで指定された各 x ステップと y ステップの距離を定義する。\nヘッダはラスタデータセットの重要な構成要素で、ピクセルと空間座標の関係を指定する (Chapter 4 も参照)。本章は、ベクタデータセットではバス停の名前、ラスタデータセットではピクセルの標高といった属性に基づいて地理的なオブジェクトを操作する方法を解説する。\nベクタデータの場合は、部分集合 (subset) や属性集計 (aggregate) といった手法になる (Section 3.2.1 から Section 3.2.3 を参照)。\nまた、Section 3.2.4 では、共有 ID を用いてデータをシンプルフィーチャに結合する方法、Section 3.2.5 では、新しい変数の作成方法を説明している。\nこれらの操作には、それぞれ空間的な等価性がある。\n例えば、R の [ 演算子は、属性に基づくオブジェクトの部分集合と空間オブジェクトの部分集合に同じように機能する。また、空間結合を使用して 2 つの地理データセットの属性を結合することもできる。\nこの章で学ぶスキルは他にも応用可能である。次のセクションでは、さまざまなベクタ属性操作を深く掘り下げた後、ラスタ属性データ操作を Section 3.3 でカバーする。\nラスタについては、連続およびカテゴリ属性を含むラスタレイヤの作成方法と、1 つまたは複数のレイヤからセル値を抽出する (ラスタ部分集合化) 方法を示す (Section 3.3.1)。\nSection 3.3.2 は、ラスタデータセット全体を要約するために使用できる「グローバル」ラスタ操作の概要を提供する。\nChapter 4 は、ここで紹介した方法を空間的な世界に拡張するものである。","code":""},{"path":"attr.html","id":"vector-attribute-manipulation","chapter":"3 属性データ操作","heading":"3.2 ベクタ属性操作","text":"\n地理ベクタデータセットは、R の基本クラスの data.frame を拡張した sf クラスにより対応されている。\nsf オブジェクトはデータフレームのように、属性変数 (’name’など) ごとに 1 列、観察または フィーチャ (たとえば、バス停ごと) ごとに 1 行を持つ。\nsf オブジェクトは基本的なデータフレームとは異なり、sfc クラス の geometry 列を持ち、1 行にさまざまな地理的実体 (「複合でない」および「複合」点、線、ポリゴン) を含むことができる。\nChapter 2 では、plot() や summary() などのジェネリック関数が sf オブジェクトでどのように動作するかを示した。\nsf はまた、sf オブジェクトが通常のデータフレームのように動作することを可能にするジェネリック関数を提供する。sf に対応するジェネリック関数は、以下で確認できる。これらの多く (aggregate()、cbind()、merge()、rbind()、[) は、データフレームを操作するためのものである。\n例えば、rbind() は、2 つのデータフレームの行を「上下に」結合する。\n$<- は、新しい列を作成する。\nsf オブジェクトの大きな特徴は、空間データと非空間データを同じように、data.frame の列として格納することである。sf オブジェクトのジオメトリ列は、通常 geometry または geom と呼ばれるが、任意の名前を使用することができる。\n例えば、次のコマンドは g という名前のジオメトリ列を作成する。st_sf(data.frame(n = world$name_long), g = world$geom)sf オブジェクトは、データフレーム用の tidyverse クラスである tbl_df と tbl を拡張することもできる。\nこのように sf は、データ解析に基本的な R や tidyverse 関数を使用するなど、R のデータ解析能力の全力を地理データに対して発揮することを可能にする。\n高性能データ処理パッケージ data.table も sf オブジェクトを処理できるが、issue Rdatatable/data.table#2273 で説明されているように、完全に互換ではない。\nこれらの機能を使用する前に、ベクタデータオブジェクトの基本的なプロパティを発見する方法をもう一度おさらいしておくとよいだろう。\nまずは R の基本関数を使って、spData パッケージの world データセットについて学習してみよう。\nworld は、10 個の地理とは関係ない列 (および 1 個のジオメトリリスト列) と、世界の国々を表す約 200 個の行を含んでいる。\n関数 st_drop_geometry() は、sf オブジェクトの属性データのみを保持し、ジオメトリを削除する。属性データを扱う前にジオメトリ列を削除すると便利である。属性データのみを扱いジオメトリ列は必ずしも必要ではない場合、データ操作のプロセスが速く実行できるからである。\nしかし、ほとんどの場合、ジオメトリ列を残すことは理にかなっている。ジオメトリ列が「スティッキー」 (意図的に削除しない限り、ほとんどの属性操作後も残っている) である理由となる。\nsf オブジェクトに対する非空間データ操作は、適切な場合にのみオブジェクトのジオメトリを変更する (例: 集計後に隣接するポリゴン間の境界をディゾルブする)。\n地理的属性データの操作に習熟するということは、データフレームの操作に習熟するということである。多くのアプリケーションにおいて、tidyverse のパッケージ dplyr (Wickham et al. 2023) は、データフレームを扱うための効果的なアプローチを提供する。\ntidyverse との互換性は、前身の sp になかった sf の利点であるが、落とし穴もあるのでハマることもある (詳しくは geocompx.org の補足 tidyverse-pitfalls vignetteを参照)。","code":"\nmethods(class = \"sf\") # sf オブジェクトのメソッド、最初の 12\n#> [1] [             [[<-          $<-           aggregate    \n#> [5] as.data.frame cbind         coerce        filter       \n#> [9] identify      initialize    merge         plot        \nclass(world) # sf オブジェクトであり、(tidy) データフレームである\n#> [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\ndim(world)  # ２次元オブジェクトで、 177 行 11 列\n#> [1] 177  11\nworld_df = st_drop_geometry(world)\nclass(world_df)\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nncol(world_df)\n#> [1] 10"},{"path":"attr.html","id":"vector-attribute-subsetting","chapter":"3 属性データ操作","heading":"3.2.1 ベクタ属性の部分集合","text":"R の基本的な部分集合 (subset) の作成方法には、演算子 [ と関数 subset() がある。\ndplyr の関数では、行の部分集合作成には filter() と slice() があり、列の部分集合作成には select() がある。\nどちらのアプローチも sf オブジェクトの属性データの空間成分を保持する。一方、演算子 $ や dplyr 関数 pull() を使って単一の属性列をベクトルとして返すと、これから説明するようにジオメトリデータが失われる。\nこのセクションでは、sf データフレームの部分集合作成に焦点を当てている。ベクトルや非地理データフレームの部分集合に関する詳細については、Introduction R (R Core Team 2021) の Section 2.7 と Advanced R Programming (Wickham 2019) の Chapter 4 を勧める。\n[ 演算子は、行と列の両方から部分集合を作成 (抽出) することができる。\nデータフレームオブジェクト名の直後の角括弧内に置かれたインデックスは、保持する要素を指定する。\nコマンド object[, j] は、「で表される行と、j で表される列を返す」という意味である。と j は通常、整数か TRUE と FALSE を含む (インデックスは、行や列名を示す文字列でもかまいない)。\n例えば、object[5, 1:3] は、「5 行目と 1 列目から 3 列目を含むデータを返す: 結果は 1 行目と 3 列目だけのデータフレームで、sf オブジェクトの場合は 4 番目のジオメトリ列も含めなさい」という意味である。\nまたは j を空白にすると、すべての行または列が返される。 world[1:5, ] は最初の 5 行と 11 列すべてを返す。\n以下の例は、Base R による部分集合の作成を示している。\n各コマンドが返す sf データフレームの行と列の数を推測し、自分のコンピュータで結果を確認してみよう (他の演習課題はこの章の最後を参照)。以下のコードチャンクでは、部分集合に logical ベクトルを使用することの有用性を示す。\nこれにより、表面積が 10,000 km2 より小さい国を含む新しいオブジェクト small_countries が作成される。中間値 i_small (小国を表すインデックスの略) は、world の面積の小さい 7 カ国の部分集合を作成するのに使う論理ベクトルである。\n中間オブジェクトを省略したより簡潔なコマンドでも、同じ結果が得られる。Base R 関数 subset() でも、同じ結果を得ることができる。\nBase R 関数は成熟し安定しており、また広く使用されているため、特に再現性と信頼性が重要視される文脈では確実な選択肢となる。\n一方、dplyr の関数は、特に RStudio のような列名の自動補完 を可能にするコードエディタと組み合わせたとき「tidy な」ワークフローを可能にする。一部の人々 (本書の著者も含む) は、こちらの方が対話式データ分析であり、直観的で生産的だと感じる。\nデータフレームの部分集合化する主要な関数 (sf データフレームを含む) を dplyr 関数で以下に示す。select() は、名前または位置によって列を選択する。\n例えば、次のコマンドで、name_long と pop の 2 つの列だけを選択することができる。注: Base R 関数での同等のコマンド (world [, c(\"name_long\", \"pop\")]) と同様に、スティッキーな geom 列が残る。\nselect() は、: 演算子の助けを借りて、列の範囲を選択することもできる。- 演算子で特定の列を削除することができる。new_name = old_name 構文で、列の部分集合と名前の変更を同時に行うことができる。上記のコマンドは、2 行のコードを必要とする Base R 関数のコードよりも簡潔であることは注目に値する。select() は、contains()、starts_with()、num_range() など、より高度な部分集合操作のための「ヘルパー関数」とも連動する (詳しくは ?select のヘルプページを参照)。ほとんどの dplyr 動詞はデータフレームを返すが、pull() で単一の列をベクトルとして抽出することができる。\nBase R でリスト部分集合演算子 $ と [[ を使っても同じ結果が得られる。以下の 3 つのコマンドは、同じ数値ベクトルを返す。slice() は、行に対して select() と同様のことを行う。\n例えば、次のコードチャンクは、1 行目から 6 行目までを選択する。filter() は、Base R の subset() 関数に相当する dplyr の関数である。\n例えば、面積がある閾値以下の国、平均寿命が高い国など、与えられた基準に合致する行のみを保持する。Table 3.1 に示すように、標準的な比較演算子のセットは、filter() 関数で使用することができる。TABLE 3.1: 論理値 (true/false) を返す比較演算。","code":"\nworld[1:6, ]    # 位置で行を抽出\nworld[, 1:3]    # 位置で列を抽出\nworld[1:6, 1:3] # 位置で行と列を抽出\nworld[, c(\"name_long\", \"pop\")] # 名称で列を抽出\nworld[, c(T, T, F, F, F, F, F, T, T, F, F)] # 論理値で抽出\nworld[, 888] # 存在しない列番号\ni_small = world$area_km2 < 10000\nsummary(i_small) # 論理ベクトル\n#>    Mode   FALSE    TRUE \n#> logical     170       7\nsmall_countries = world[i_small, ]\nsmall_countries = world[world$area_km2 < 10000, ]\nsmall_countries = subset(world, area_km2 < 10000)\nworld1 = select(world, name_long, pop)\nnames(world1)\n#> [1] \"name_long\" \"pop\"       \"geom\"\n# name_long から pop までの全ての列\nworld2 = select(world, name_long:pop)\n# subregion と area_km2 以外全ての列\nworld3 = select(world, -subregion, -area_km2)\nworld4 = select(world, name_long, population = pop)\nworld5 = world[, c(\"name_long\", \"pop\")] # 名称で列を抽出\nnames(world5)[names(world5) == \"pop\"] = \"population\" # 列めいを変更\npull(world, pop)\nworld$pop\nworld[[\"pop\"]]\nslice(world, 1:6)\nworld7 = filter(world, area_km2 < 10000) # 面積の小さい国\nworld7 = filter(world, lifeExp > 82)     # 平均寿命が高い"},{"path":"attr.html","id":"chaining-commands-with-pipes","chapter":"3 属性データ操作","heading":"3.2.2 パイプを使ったコマンドの連鎖","text":"\ndplyr 関数を使用するワークフローの鍵は、パイプ 演算子 %>% (R 4.1.0 以降では ネイティブパイプ |>) で、これは Unix パイプ | (Grolemund Wickham 2016) から名前を取ったものである。\nパイプは、直前の関数の出力が次の関数の第 1 引数になるため、表現力豊かなコードを実現する。\nこれは、world データセットからアジアの国だけがフィルタされ、次にオブジェクトが列 (name_long と continent) と最初の 5 行の部分集合にされる様子を示している (結果は示していない)。上のチャンクは、パイプ演算子によって、コマンドを明確な順序で記述できることを示している。\n上記を上から下へ (一行ずつ)、左から右へ実行する。\nパイプによる操作の代わりに、ネストされた関数呼び出しがあるが、これは読みにくい。別の方法として、操作を複数の行に分割する方法もある。この方法は、中間結果を明確な名前で保存し、後でデバッグのために検査することができるという利点がある (この方法は、冗長になり、対話型解析を行う際にグローバル環境が煩雑になるという欠点もある)。それぞれのアプローチには利点と欠点があり、プログラミングスタイルやアプリケーションによってその重要性は異なる。\n本章の焦点である対話的なデータ解析では、特にRStudio/VSCodeのパイプを作成するためのショートカットや変数名自動補完と組み合わせた場合に、パイプによる操作が高速で直感的であることがわかる。","code":"\nworld7 = world |>\n  filter(continent == \"Asia\") |>\n  select(name_long, continent) |>\n  slice(1:5)\nworld8 = slice(\n  select(\n    filter(world, continent == \"Asia\"),\n    name_long, continent),\n  1:5)\nworld9_filtered = filter(world, continent == \"Asia\")\nworld9_selected = select(world9_filtered, continent)\nworld9 = slice(world9_selected, 1:5)"},{"path":"attr.html","id":"vector-attribute-aggregation","chapter":"3 属性データ操作","heading":"3.2.3 ベクタ属性集計","text":"\n集計では、1 つ以上の「グループ化変数」、通常は集計対象のデータフレームの列からデータを要約する (地理的集計は次の章で扱う)。\n属性集約の例として、国レベルのデータから大陸ごとの人口を計算する (1 国につき 1 行)。\nworld データセットには、必要な要素が含まれている: pop 列と continent 列、それぞれ人口とグループ化変数である。\n目的は、各大陸の国別人口の sum() を見つけ、より小さなデータフレームにすることである (集約はデータ削減の一形態であり、大規模データセットを扱う際の初期段階として有効)。\nこれは、R の基本関数 aggregate()で、次のように行うことができる。結果は、各大陸につき 1 行の計 6 行と、各大陸の名前と人口を示す 2 列の非空間データフレームになる (人口の多い上位 3 大陸の結果は Table 3.2 を参照)。aggregate() は、ジェネリック関数である。ジェネリック関数とは、入力によって異なる動作をすることを意味している。\nsf は、aggregate.sf() というメソッドを提供している。これによって、aggregate() 関数の引数x に sf オブジェクトを与え、さらに の引数が与えられたとき、自動的に aggregate.sf() が呼ばれる。結果として world_agg2 オブジェクトは、世界の大陸 (および外洋) を表す 8 つのフィーチャを含む空間オブジェクトとなる。\ngroup_by() |> summarize() は aggregate() の dplyr 版である。\nグループ化する変数は group_by() 関数で指定し、集約式は summarize() 関数に渡す。コード例は以下の通り。この方法はより複雑に見えるだろうが、柔軟性、読みやすさ、新しい列名の制御という利点がある。\nこの柔軟性を示すのが、人口だけでなく、各大陸の面積や国数を計算する以下のコマンドである。上のコードチャンクで、pop、area_sqkm、n は結果の列名で、sum()、n() は集計関数である。\nこれらの集約関数は、大陸を表す行と、各大陸と関連する島を表す複数のポリゴンを含むジオメトリを持つ sf オブジェクトを返す (これは、Section 5.2.7 で説明するように、ジオメトリ操作 ‘union’ によって機能する)。\nこれまで学んだ dplyr 関数を組み合わせて、複数のコマンドを連結し、世界の国々の属性データを大陸別にまとめてみよう。\n次のコマンドは、人口密度を計算し (mutate())、大陸を含む国の数で並べ (arrange())、最も人口の多い 3 大陸だけを残し (slice_max())、その結果を Table 3.2 に表示する。TABLE 3.2: 人口の多い 3 大陸を国数で並べ替えて表示。","code":"\nworld_agg1 = aggregate(pop ~ continent, FUN = sum, data = world,\n                       na.rm = TRUE)\nclass(world_agg1)\n#> [1] \"data.frame\"\nworld_agg2 = aggregate(world[\"pop\"], by = list(world$continent), FUN = sum, \n                       na.rm = TRUE)\nclass(world_agg2)\n#> [1] \"sf\"         \"data.frame\"\nnrow(world_agg2)\n#> [1] 8\nworld_agg3 = world |>\n  group_by(continent) |>\n  summarize(pop = sum(pop, na.rm = TRUE))\nworld_agg4  = world |> \n  group_by(continent) |> \n  summarize(Pop = sum(pop, na.rm = TRUE), Area = sum(area_km2), N = n())\nworld_agg5 = world |> \n  st_drop_geometry() |>                      # 速くするためジオメトリを削除\n  select(pop, continent, area_km2) |> # 関心ある列のみの部分集合\n  group_by(Continent = continent) |> # 大陸でグループ化し要約\n  summarize(Pop = sum(pop, na.rm = TRUE), Area = sum(area_km2), N = n()) |>\n  mutate(Density = round(Pop / Area)) |>     # 人口密度を計算\n  slice_max(Pop, n = 3) |>                   # 上位３件のみ\n  arrange(desc(N))                          # 国数で並べ替え"},{"path":"attr.html","id":"vector-attribute-joining","chapter":"3 属性データ操作","heading":"3.2.4 ベクタ属性の結合","text":"異なるソースからのデータを組み合わせることは、データ作成において一般的な作業である。\n結合は、共有された「キー」変数に基づいてテーブルを結合することによって行われる。\ndplyr には、left_join() や inner_join() など、複数の結合関数がある。完全なリストは、vignette(\"two-table\")を参照 (訳注: 日本語版)。\nこれらの関数名は、データベース言語 SQL (Grolemund Wickham 2016, chap. 13) で使われている慣例に従っている。これらを使って、非空間データセットと sf オブジェクトを結合することが、このセクションの焦点である。\ndplyr の join 関数は、データフレームと sf オブジェクトで同じように動作する。唯一の重要な違いは、geometry リスト列である。\nデータ結合の結果は、sf または data.frame オブジェクトのいずれかになる。\n空間データに対する最も一般的な属性結合は、第1引数として sf オブジェクトを取り、第 2 引数として指定された data.frame から列を追加するものである。\n結合を実証するために、コーヒー生産に関するデータを world のデータセットと結合する。\nコーヒーのデータは spData パッケージの coffee_data というデータフレームに入っている (詳しくは ?coffee_data を参照)。\n以下のように、3 列になっている。\nname_long は主要なコーヒー生産国の名前、coffee_production_2016 と coffee_production_2017 は各年の 60 kg 袋単位のコーヒー生産量の推定値である。\n最初のデータセットを保持する「左結合」で、world と coffee_data を結合する。入力データセットが「キー変数」(name_long) を共有しているため、引数を使わなくても結合ができた (詳細は ?left_join を参照)。\nその結果、sf オブジェクトは、元の world オブジェクトと同じであるが、コーヒー生産に関する 2 つの新しい変数 (列インデックス 11 と 12 を持つ) が追加される。\nこれは、以下の plot() 関数で生成される Figure 3.1 のように、地図としてプロットすることができる。\nFIGURE 3.1: 世界の国別コーヒー生産量 (60 kg 袋千個)、2017年。出典: 国際コーヒー機関 国際コーヒー機関。\n結合を行うには、両方のデータセットで「キー変数」が供給される必要がある。\nデフォルトでは、dplyr は一致する名前のすべての変数を使用する。\nこの場合、coffee_data と world の両方のオブジェクトに name_long という変数が含まれており、Joining '= join_by(name_long)' というメッセージを説明している。\n変数名が同じでない場合、おおむね 2 つのオプションがある。どちらかのオブジェクトのキー変数の名前を変更し、一致するようにする。引数で結合変数を指定する。後者の方法は、coffee_data の名前を変更したバージョンで以下に示す。なお、元のオブジェクトの名前は保持され、world_coffee と新しいオブジェクト world_coffee2 は同一であることを意味する。\n結果について、元のデータセットと同じ行数となる。\ncoffee_data には 47 行のデータしかないが、world_coffee と world_coffee2 には 177 の国別レコードがすべてそのまま保存されている。\nこれは、行が一致しない場合、新たなコーヒー生産量変数として NA の値を割り当てるためである。\nキー変数が一致する国だけを残したい場合はどうすればいいのだろうか？\nその場合、内部結合を使用することができる。coffee_data の結果が 47 行であるのに対し、inner_join() の結果は 45 行しかないことに注意しておこう。\n残りの列はどうなったのだろうか？\n一致しなかった行は、setdiff() 関数を用いて、以下のように特定することができる。その結果、Others が world のデータセットに存在しない 1 行を占め、Democratic Republic Congo の名前がもう 1 行を占めることがわかった。\nが省略され、結合が見落とされている。\n次のコマンドは、stringr パッケージの文字列照合 (regex) 関数を使用して、Congo, Dem. Rep. がどうあるべきかを確認するものである。この問題を解決するために、coffee_data の新バージョンを作成し、名前を更新する。\ninner_join() を更新すると、コーヒー生産国全 46 カ国を含む結果が返される。また、非空間データセットから始めて、シンプルフィーチャオブジェクトから変数を追加するという、逆方向の結合も可能である。\nこれは、coffee_data オブジェクトから始まり、オリジナルの world データセットから変数を追加するもので、以下のように示される。\n前の結合とは対照的に、結果はシンプルフィーチャオブジェクトではなく、tidyverse の tibble という形のデータフレームになる。\njoin の出力はその最初の引数に一致する傾向がある。ここでは、属性結合のほとんどのケースをカバーした。\nより詳しくは、Grolemund Wickham (2016) の Relational data の章、本書に付属する geocompkg パッケージの join vignette 、および data.table などのパッケージによる結合を説明したドキュメントを読むことを勧める。\nさらに、空間結合については次の章で説明する (Section 4.2.5)。","code":"\nworld_coffee = left_join(world, coffee_data)\n#> Joining with `by = join_by(name_long)`\nclass(world_coffee)\n#> [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nnames(world_coffee)\n#>  [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n#>  [4] \"region_un\"              \"subregion\"              \"type\"                  \n#>  [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n#> [10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n#> [13] \"coffee_production_2017\"\nplot(world_coffee[\"coffee_production_2017\"])\ncoffee_renamed = rename(coffee_data, nm = name_long)\nworld_coffee2 = left_join(world, coffee_renamed, by = join_by(name_long == nm))\nworld_coffee_inner = inner_join(world, coffee_data)\n#> Joining with `by = join_by(name_long)`\nnrow(world_coffee_inner)\n#> [1] 45\nsetdiff(coffee_data$name_long, world$name_long)\n#> [1] \"Congo, Dem. Rep. of\" \"Others\"\ndrc = stringr::str_subset(world$name_long, \"Dem*.+Congo\")\ndrc\n#> [1] \"Democratic Republic of the Congo\"\ncoffee_data$name_long[grepl(\"Congo,\", coffee_data$name_long)] = drc\nworld_coffee_match = inner_join(world, coffee_data)\n#> Joining with `by = join_by(name_long)`\nnrow(world_coffee_match)\n#> [1] 46\ncoffee_world = left_join(coffee_data, world)\n#> Joining with `by = join_by(name_long)`\nclass(coffee_world)\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"attr.html","id":"vec-attr-creation","chapter":"3 属性データ操作","heading":"3.2.5 属性の作成と空間情報の削除","text":"\n既にある列を元に新しい列を作りたい場合はよくある。\n例えば、各国の人口密度を計算したい。\nそのためには、人口列 (ここでは pop) を面積列 (ここでは area_km2) (単位面積は平方キロメートル) で割る必要がある。\nBase R を使って、以下のように書いてみよう。\nあるいは、dplyr の関数である mutate() または transmute() を使うこともできる。\nmutate() は、sf オブジェクトの最後から 2 番目の位置に新しい列を追加する (最後の列はジオメトリ用に予約されている)。mutate() と transmute() の違いは、後者が他の既存の列をすべて削除することである (スティッキーなジオメトリ列を除く)。\ntidyr パッケージ (pivot_longer() を始め、データセットを再形成するための多くの便利な関数を提供する) の unite() は、既存の列を貼り合わせる。\n例えば、continent と region_un の列を結合して、con_reg という新しい列を作成したい。\nさらに、入力列の値をどのように結合するかを定義するセパレータ (ここでは、コロン :)、および元の列を削除するかどうか (ここでは、TRUE) を定義することができる。ここでできた sf オブジェクトは、各国の大陸と地域を表す con_reg という新しい列を持ち、例えば、アルゼンチンやその他の南米諸国は South America:Americas となる。\ntidyr の separate() 関数は unite() の逆を行う: 正規表現か文字位置のどちらかを使って 1 つの列を複数の列に分割する。\n列の名前を変更するには、dplyr 関数 rename() と基本関数 setNames() が便利である。\n1 つ目は、古い名前を新しい名前に置き換えるものである。\n例えば、次のコマンドは、長い name_long 列の名前を、単に name に変更する。\nsetNames() はすべての列の名前を一度に変更し、各列にマッチする名前を持つ文字ベクタを必要とする。\nこれは下図に示すように、同じ world オブジェクトを出力しているが、非常に短い名前になっている。\nこれらの属性データ操作は、いずれもシンプルフィーチャの形状を保持するものである。\n集計を高速化するためなど、ジオメトリを削除することが理にかなっている場合もある。\nselect(world, -geom) などのコマンドで手動で行うのではなく、st_drop_geometry() で行ってみよう。\n20","code":"\nworld_new = world # 元データを上書きしない\nworld_new$pop_dens = world_new$pop / world_new$area_km2\nworld_new2 = world |> \n  mutate(pop_dens = pop / area_km2)\nworld_unite = world |>\n  tidyr::unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE)\nworld_separate = world_unite |>\n  tidyr::separate(con_reg, c(\"continent\", \"region_un\"), sep = \":\")\nworld |> \n  rename(name = name_long)\nnew_names = c(\"i\", \"n\", \"c\", \"r\", \"s\", \"t\", \"a\", \"p\", \"l\", \"gP\", \"geom\")\nworld_new_names = world |>\n  setNames(new_names)\nworld_data = world |> st_drop_geometry()\nclass(world_data)\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"attr.html","id":"manipulating-raster-objects","chapter":"3 属性データ操作","heading":"3.3 ラスタオブジェクトを操作","text":"シンプルフィーチャであるベクタデータ (点、線、ポリゴンを空間上の離散的な実体として表現する) とは対照的に、ラスタデータは連続的な面を表現する。\nこのセクションでは、ラスタオブジェクトの動作を、Section ?? を基にゼロから作成することによって説明する。\nラスタデータセットはユニークな構造のため、Section 3.3.1 で示すように、部分集合の作成やその他の操作は異なる。\n次のコードは、Section 2.3.4 で使用したラスタデータセットを再作成し、その結果を Figure 3.2 に示している。\nこれは、elev (標高を表す) という名前のラスタの例を作成するために、rast() 関数がどのように動作するかを示している。結果は、6 行 6 列 (引数 nrow と ncol で指定）のラスタオブジェクトと、XとY方向の最小と最大の空間範囲 (xmin、xmax、ymin、ymax) となる。\nvals 引数は、各セルが含む値を設定する。この場合、1 から 36 までの数値データである。\nラスタオブジェクトは、R のクラス logical または factor 変数のカテゴリ値も含むことができる。\n次のコードでは、Figure 3.2 に示すラスタデータセットを作成する。\nラスタオブジェクトは、対応するルックアップテーブルまたは「ラスタ属性テーブル」(Raster Attribute Table, RAT) をデータフレームのリストとして格納し、cats(grain) (詳しくは ?cats()) を使って表示することができる。\nこのリストの各要素は、ラスタのレイヤである。\nまた、関数 levels() を使って、新しい因子レベルの取得や追加、既存の因子の置き換えを行うことも可能である。\nFIGURE 3.2: 数値 (左) とカテゴリ値 (右) を持つラスタデータセット。\n","code":"\nelev = rast(nrows = 6, ncols = 6, resolution = 0.5, \n            xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n            vals = 1:36)\ngrain_order = c(\"clay\", \"silt\", \"sand\")\ngrain_char = sample(grain_order, 36, replace = TRUE)\ngrain_fact = factor(grain_char, levels = grain_order)\ngrain = rast(nrows = 6, ncols = 6, resolution = 0.5, \n             xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n             vals = grain_fact)\ngrain2 = grain # 元データを書き換えない\nlevels(grain2) = data.frame(value = c(0, 1, 2), wetness = c(\"wet\", \"moist\", \"dry\"))\nlevels(grain2)"},{"path":"attr.html","id":"raster-subsetting","chapter":"3 属性データ操作","heading":"3.3.1 ラスタ部分集合","text":"ラスタの部分集合は、R の基本演算子である [ で行われ、さまざまな入力を受け入れることができる。\n行・列のインデックス作成セル ID座標別の空間オブジェクトここでは、非空間的な操作である最初の 2 つのオプションのみを示す。\n空間オブジェクトをサブセットする必要がある場合、あるいは出力が空間オブジェクトである場合、これを空間サブセットと呼ぶことにする。\n後者の 2 つのオプションについては、次章で紹介する (Section 4.3.1)。\nまず、二つの部分集合作成方法を以下のコマンドで示す。\n両者は、いずれもラスタオブジェクト elev の左上のピクセルの値を返す (結果は示していない)。複数レイヤのラスタオブジェクトを部分集合作成すると、各レイヤのセル値が返される。\n例えば two_layers = c(grain, elev); two_layers [1] は 1 行 2 列のデータフレームを返す。\nすべての値を抽出するには、values() を使用することもできる。部分集合作成操作と連動して、既存の値を上書きすることでセルの値を変更することができる。\n例えば、次のコードチャンクは、elev の左上のセルに 0 を設定する (結果は表示していない)。角括弧を空にすると、ラスタのすべての値を取得する values() のショートカット版である。\nまた、複数のセルをこの方法で修正することも可能である。レイヤが複数あるラスタの値の置き換えは、列がレイヤと同じ数、行が置き換え可能なセルと同じ数の行列で行うことができる (結果は示していない)。","code":"\n# 行 1, 列 1\nelev[1, 1]\n# cell ID 1\nelev[1]\nelev[1, 1] = 0\nelev[]\nelev[1, c(1, 2)] = 0\ntwo_layers = c(grain, elev) \ntwo_layers[1] = cbind(c(1), c(4))\ntwo_layers[]"},{"path":"attr.html","id":"summarizing-raster-objects","chapter":"3 属性データ操作","heading":"3.3.2 ラスタオブジェクトのまとめ","text":"terra は、ラスタ全体の記述統計量を抽出するための関数を含んでいる。\nラスタオブジェクトの名前を入力してコンソールに印刷すると、ラスタの最小値と最大値が返される。\nsummary() は、一般的な記述統計量を提供する。 すなわち、連続ラスタでは最小値、最大値、四分位値、NA の件数、カテゴリラスタでは各クラスのセルの数である。\n標準偏差 (下記参照) やカスタム要約統計などのさらなる要約操作は、global() で計算することができる。\nさらに、freq() 関数を使用すると、カテゴリ値の頻度表を取得することができる。ラスタ値の統計は、様々な方法で可視化することができる。\nboxplot()、density()、hist()、pairs() などの特定の関数は、以下のコマンドで作成されたヒストグラムで示されるように、ラスタオブジェクトでも動作する (図示していない)。\n目的の可視化機能がラスタオブジェクトで動作しない場合、values() (Section 3.3.1) の助けを借りて、プロットするラスタデータを抽出することができる。記述的ラスタ統計は、いわゆるグローバルなラスタ演算に属する。\nこれらの操作やその他の典型的なラスタ処理操作は、マップ代数スキームの一部であり、次の章 (Section 4.3.2) で説明する。\nパッケージ間で関数名が衝突することがある(例えば、extract()\nという名前の関数が terra と tidyr\nの両方のパッケージに存在する場合など)。\nパッケージをロードする順番を変えると、予想しない結果が発生することがある。\n関数名の衝突を防ぐには、パッケージをロードせずに名前空間を明示する方法\n(例: tidyr::extract()) や、detach()\nで問題となるパッケージをアンロードする方法がある。\n例えば、以下のコマンドは terra\nパッケージをアンロードする (これは RStudio\nの右下ペインにデフォルトで存在する Packages\nタブでも行うことができる):\ndetach(“package:terra”, unload = TRUE, force = TRUE)。\nforce\n引数は、他のパッケージがそのパッケージに依存している場合でも、そのパッケージを切り離すことを保証する。\nしかし、これは切り離されたパッケージに依存しているパッケージの使い勝手を悪くする可能性があるため、推奨されない。\n","code":"\nglobal(elev, sd)\nfreq(grain)\n#>   layer value count\n#> 1     1  clay    10\n#> 2     1  silt    13\n#> 3     1  sand    13\nhist(elev)"},{"path":"attr.html","id":"演習-1","chapter":"3 属性データ操作","heading":"3.4 演習","text":"演習では、spData パッケージの us_states と us_states_df というデータセットを使用する。\nこのデータと、属性を操作するためのパッケージを読み込むため、 library(spData) などのコマンドを実行しておく必要がある。us_states は、(sf クラスの) 特別なオブジェクトで、アメリカ合衆国の州のジオメトリと複数の属性 (name、region、area、 population など) がある。\nus_states_df は、(data.frame の) データフレームであり、アメリカ合衆国各州の name とその他の変数 (2010年と2015年の年収中央値や貧困度合など) で、Alaska、Hawaii、Puerto Rico も含んでいる。\nこのデータは、米国国勢調査局のもので、ドキュメントは ?us_states と ?us_states_df とすることで読むことができる。E1. us_states オブジェクトから NAME 列のみを含む us_states_name という新しいオブジェクトを、Base R ([) または tidyverse (select()) 構文を使用して作成しなさい。\n新しいオブジェクトのクラスは何か？E2. us_statesオブジェクトから、人口データを含む列を選択しなさい。\n別のコマンドを使用して同じことをしなさい（ボーナス: 3 つの方法を見つけなさい）。\nヒント: dplyr の contains や matches などのヘルパー関数を使用してみる (?contains を参照)。E3. 以下の特徴を持つ状態をすべて見つけなさい (ボーナス、見つけた後でプロットしなさい)。中西部 (Midwest) 地域に属する。西 (West) 地域に属し、面積が250,000km2未満でかつ、2015年の人口が 5,000,000 人を超える (ヒント: 関数 units::set_units() または .numeric() を使用する必要があるかもしれない)。南 (South) 地域に属し、面積が 150,000 km2 を超え、2015年の総人口が 7,000,000 人を超える。E4. us_states データセットにおける2015年の総人口は?\n2015年の総人口の最小値と最大値は?E5. 各地域にはいくつの州があるのか?E6. 2015年の各地域の総人口の最小値と最大値は？\n各地域の2015年の総人口は？E7. us_states_df の変数を us_states に追加し、us_states_stats という新しいオブジェクトを作成しなさい。\nどの関数を使用したか?\n両方のデータセットでどの変数がキーであったか?\n新しいオブジェクトのクラスは何か?E8. us_states_df は us_states より2行多い。\n多い部分をどのように発見するか (ヒント: dplyr::anti_join() 関数を使ってよい。)?E9. 各州の2015年の人口密度は？\n各州の2010年の人口密度は？E10. 各州の人口密度は2010年から2015年の間にどれだけ変化したか？\nその変化をパーセンテージで計算し、地図に表しなさい。E11. us_states の列の名称を小文字にしなさい (ヒント: 関数 tolower() と colnames() を使うと良い)。E12. us_states と us_states_df を使って us_states_sel という新しいオブジェクトを作成しなさい。\nこの新しいオブジェクトには、median_income_15 と geometry の 2 つの変数だけにしなさい。\nmedian_income_15 列の名前を Income に変更しなさい。E13. 各州の2010年から2015年の間の貧困ボーダー以下の住民数の変化を計算しなさい。(ヒント: 貧困レベルの列に関するドキュメントは ?us_states_df を参照)。\nボーナス: 各州の貧困レベル以下で暮らす住民のパーセンテージの変化を計算しなさい。E14. 2015年、各地域の貧困ボーダー以下で暮らす人々の州の最小数、平均数、最大数は?\nボーナス: 貧困ボーダー以下で暮らす人の増加が最も大きかった地域は?E15. 9 行 9 列、解像度 0.5 度 (WGS84) のラスタをゼロから作成しなさい。\nそれを乱数で埋めなさい。\n4 つのコーナーセルの値を抽出しなさい。E16. 例にあるラスタ grain の最も一般的なクラスは何か?E17. spDataLarge パッケージの dem.tif ファイルのヒストグラムと箱ひげ図をプロットしなさい。 (system.file(\"raster/dem.tif\", package = \"spDataLarge\")).","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(terra)\nlibrary(spData)\ndata(us_states)\ndata(us_states_df)"},{"path":"spatial-operations.html","id":"spatial-operations","chapter":"4 空間データ操作","heading":"4 空間データ操作","text":"","code":""},{"path":"spatial-operations.html","id":"prerequisites-04","chapter":"4 空間データ操作","heading":"必須パッケージ","text":"この章では、Chapter 3 で使用したものと同じパッケージが必要である。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)"},{"path":"spatial-operations.html","id":"introduction-04","chapter":"4 空間データ操作","heading":"4.1 イントロダクション","text":"ジオコンピュテーションにおいて、ベクタデータセット間の空間結合やラスタデータセットのローカルおよびフォーカル演算などの空間演算は重要な要素である。\nこの章では、空間オブジェクトがその位置と形状に基づいて、さまざまな方法で変更できることを紹介する。\n空間的な操作の多くは、非空間的 (属性的) な操作に相当するため、前の章で示したデータセットの部分集合や結合といった概念がここでも適用できる。\nこれは特にベクタ操作に当てはまる。ベクタ属性の操作に関する Section 3.2 は、空間的な対応である空間部分集合 (Section 4.2.1 で取り上げている) を理解するための基礎となるものである。\n空間結合 (Section 4.2.5、4.2.6、4.2.8) と属性集計 (Section 4.2.7) には、前の章で説明した非空間的な対応関係がある。しかし、空間演算は非空間演算と異なる点がいくつもある。\n例えば空間結合は、ターゲットデータセットと交差する、または一定の距離内にあるモノのマッチングなど、結合方法は多数ある。一方、前章の Section 3.2.4 で説明した属性結合は、1 つの方法でしかできない　(ただし fuzzyjoin パッケージのドキュメントで説明した、ファジー結合を使う場合は別)。\nオブジェクト間の空間的関係のさまざまなタイプ (intersect と disjoint を含む) については、Section 4.2.2 で説明する。\n空間オブジェクトのもう一つのユニークな側面は距離である。すべての空間オブジェクトは空間を通じて関連しており、距離計算を行うことでこの関係の強さを調べることができる。これは、Section 4.2.3 と Section 4.2.4 のベクタデータで説明する。ラスタの空間演算として、部分集合は Section 4.3.1 で説明する。\nマップ代数は、ラスタのセルの値を、周囲のセル値を参照して、あるいは参照せずに変更する操作を対象とする。\n多くのアプリケーションに不可欠なマップ代数の概念は、Section 4.3.2 で紹介している。ローカル、フォーカル、ゾーンのマップ代数演算については、それぞれ Section 4.3.3、Section 4.3.4、Section 4.3.5 のセクションで解説している。\nラスタデータセット全体を表す要約統計量を生成するグローバルマップ代数操作と、ラスタの距離計算については、Section 4.3.6 で説明する。\n次に、地図代数とベクタ操作を Section 4.3.7 で議論する。\nSection 4.3.8,1 では、2 つのラスタデータセットを合成する方法について説明し、再現可能な例を挙げて実演している。","code":""},{"path":"spatial-operations.html","id":"spatial-vec","chapter":"4 空間データ操作","heading":"4.2 ベクタデータに対する空間演算","text":"ここでは、sf パッケージのシンプルフィーチャとして表現されたベクタ地理データに対する空間演算の概要を説明する。\nSection 4.3 は、terra パッケージのクラスと関数を使用したラスタデータセットの空間演算を紹介する。","code":""},{"path":"spatial-operations.html","id":"spatial-subsetting","chapter":"4 空間データ操作","heading":"4.2.1 空間部分集合","text":"空間部分集合とは、空間オブジェクトを取り出し、別のオブジェクトと空間的に関連するフィーチャだけを含む新しいオブジェクトを返す処理である。\n属性部分集合 (Section 3.2.1 で説明) と同様に、sf データフレームの部分集合は、角括弧を使用して作成できる ([) を使い、x[y, , op = st_intersects] という文法を使う。ここで x は行の部分集合が返される sf オブジェクト、y は「部分集合・オブジェクト」、op = st_intersects は部分集合を行うために使われる位相関係 (二項述語 binary predicate としても知られている) を指定するオプションの引数である。\nop 引数がないときに使用されるデフォルトの位相関係は、st_intersects()である。つまり、コマンド x[y, ] は x[y, , op = st_intersects] と同じであり x[y, , op = st_disjoint] とは異なる (これらのトポロジカル関係の意味と他のトポロジカル関係は次のセクションで説明する)。\ntidyverse の filter() 関数も使用できるが、この方法は、以下の例で見るように、より冗長である。\n空間部分集合を示すために、spData パッケージの nz と nz_height データセットを使用する。これは、New Zealand の 16 の主要地域と 101 の最高地点に関する地理データをそれぞれ含み (Figure 4.1)、投影座標参照系で表示されるものである。\n次のコードでは、Canterbury を表すオブジェクトを作成し、空間部分集合を使用して、対象地域のすべての高点を返す。\nFIGURE 4.1: 赤い三角形はNew Zealand の 101 の High Point を表し、中央のカンタベリー地域付近に集まっている (左)。カンタベリーのポイントは、[ 部分集合演算子で作成された (グレーでハイライト、右)。\n属性による部分集合と同様に、コマンド x[y, ] (nz_height[canterbury, ] と同等) は、ソース オブジェクトの内容を使用して、ターゲットのフィーチャの部分集合を返す。\nしかし、y がクラス logical または integer のベクトルである代わりに、空間部分集合では x と y の両方が地理オブジェクトでなければならない。\n具体的には、この方法で空間部分集合に使用されるオブジェクトは、クラス sf または sfc を持つ必要がある。nz と nz_height は共に地理ベクタデータフレームで、クラス sf を持つ。操作の結果、ターゲット nz_height オブジェクトの中で canterbury 地域と交差する (この場合はその中にある点) フィーチャを表す別の sf オブジェクトが返される。空間部分集合には、ターゲットオブジェクトのフィーチャが選択される部分集合オブジェクトとどのような空間的関係を持たなければならないかを決める、様々な位相関係を用いることができる。\nこれには、Section 4.2.2 で見るように、touches、crosses、within が含まれる。\nデフォルトの設定 st_intersects は、ソース「部分集合」オブジェクトに touches、crosses、within するターゲット内のフィーチャを返す「全て」の位相関係である。\nop = 引数で別の空間演算子を指定することもできる。次のコマンドでは、st_intersects() の逆で、カンタベリーと交差しない点を返す (Section 4.2.2 を参照)。ベクタデータの空間部分集合についてこれだけ知っておけば多くの応用例を使うことができる。\nst_intersects() や st_disjoint() 以外の位相関係をすぐに学びたいのであれば、残りを飛ばして次の章 (Section 4.2.2) に飛んでも構わない。\nここからは、その他の部分集合化の方法などの詳細について説明する。空間部分集合を行うもう一つの方法は、位相演算子によって返されるオブジェクトを使用することである。\nこれらのオブジェクトは、それ自体、例えば、連続する領域間の関係のグラフネットワークを探索する際に有用であるが、以下のコードチャンクで示されるように、部分集合にも使用できる。上記のコードチャンクは、クラス sgbp のオブジェクト (疎な幾何学二項述語、空間演算における長さ x のリスト) を作成し、それを論理ベクタ sel_logical (TRUE と FALSE の値のみを含み、dplyr のフィルタ関数でも使用できるもの) に変換している。\n関数 lengths() は、nz_height のどのフィーチャが y の 任意の物体と交差しているかを特定する。\nこの場合、1 が最も大きな値であるが、より複雑な操作を行う場合には、例えば、ソースオブジェクトの 2 つ以上のフィーチャと交差するフィーチャのみを部分集合するような方法を用いることができる。sf オブジェクトと dplyr データ操作コードの互換性を高めるために作成された sf 関数 st_filter() でも同じ結果を得ることができる。この時点で、(行名以外は) 同じバージョンの canterbury_height が 3 つある。1 つは [ 演算子を用いて作成し、もう 1 つは中間選択オブジェクトを介して作成し、最後は sf の便利な関数 st_filter() を用いて作成した。\n\n\n次のセクションでは、二つのフィーチャが空間的に関連しているかどうかを識別する、さまざまな種類の空間関係を探る。二項述語とも言う。","code":"\ncanterbury = nz |> filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\nnz_height[canterbury, , op = st_disjoint]\nsel_sgbp = st_intersects(x = nz_height, y = canterbury)\nclass(sel_sgbp)\n#> [1] \"sgbp\" \"list\"\nsel_sgbp\n#> Sparse geometry binary predicate list of length 101, where the\n#> predicate was `intersects'\n#> first 10 elements:\n#>  1: (empty)\n#>  2: (empty)\n#>  3: (empty)\n#>  4: (empty)\n#>  5: 1\n#>  6: 1\n....\nsel_logical = lengths(sel_sgbp) > 0\ncanterbury_height2 = nz_height[sel_logical, ]\ncanterbury_height3 = nz_height |>\n  st_filter(y = canterbury, .predicate = st_intersects)"},{"path":"spatial-operations.html","id":"topological-relations","chapter":"4 空間データ操作","heading":"4.2.2 位相関係","text":"位相関係は、オブジェクト間の空間関係を表す。\n「二項位相関係」 (binary topological relationships) とは、2 次元以上の点 (一般的には点、線、ポリゴン) の順序集合で定義される 2 つの物体間の空間関係について論理的に記述したもの (答えは TRUE か FALSE しかない) である (Egenhofer Herring 1990)。\nこのように言うと、かなり抽象的に聞こえるだろうが、実際、位相関係の定義と分類は、1966年に初めて書籍として出版された数学的基礎に基づいている (Spanier 1995)。 代数的位相幾何学の分野は2000年以降も続いている (Dieck 2008)。位相関係は数学的な起源を持つが、一般的な空間的関係をテストするためによく使われる関数を視覚化することで、直感的に理解することが可能である。\nFigure 4.2 は、様々なジオメトリペアとその関連性を示している。\nFigure 4.2 の 3 番目と 4 番目のペア (左から右、そして下) は、ある関係では順序が重要であることを示している。\n関係 equals、intersects、crosses、touches、overlaps は対称であり、function(x, y) が真なら function(y, x) も真となる。一方、contains と within など幾何学の順序が重要である関係は、そうではない。\n各ジオメトリペアには、次節で説明する FF2F11212 のような「DE-9IM」文字列があることを確認しておこう。\n\nFIGURE 4.2: Egenhofer Herring (1990)の Figure 1 と Figure 2 を参考にした、ベクトル幾何学間のトポロジー関係。関数(x, y)が真となる関係が、各ジオメトリのペアについて印刷されており、xはピンク、yは青で表されている。各ペアの空間的関係の性質は、Dimensionally Extended 9-Intersection Model 文字列で記述されている。\nsf では、異なる種類の位相関係をテストする関数を「二項述語」と呼ぶ。これは、コマンド vignette(\"sf3\") (訳注: 日本語版)、およびヘルプページ ?geos_binary_pred で見ることができる。\n位相関係が実際にどのように機能するかを見るために、Figure 4.2 で説明した関係を基に、前の章 (Section 2.2.4) で学んだベクタのジオメトリの表現方法の知識を統合して、簡単な再現性のある例を作ってみよう。\nなお、ポリゴンの頂点の座標 (x、y) を表す表形式のデータを作成するために、R の基本関数 cbind() を使って、座標点を表す行列、POLYGON、そして最後に sfc オブジェクトを作成する (Chapter 2 で説明)。次のコマンドで、空間的な関係を示す追加の形状を作成する。これらの形状は、上で作成したポリゴンの上にプロットすると、Figure 4.3 に示すように、互いに空間的に関連するようになる。\n関数 st_as_sf() と引数 coords を使って、座標を表す列を含むデータフレームから、点を含む sf オブジェクトに効率的に変換していることに注目してみよう。\nFIGURE 4.3: 点、線、ポリゴンのオブジェクトを配置し、トポロジー関係を表現。\n簡単なクエリを作ってみよう。point_sf の点のうち、ポリゴン polygon_sfc と何らかの形で交差しているものはどれか？\nこの問題は、見れば答えることができる (点 1 は接していて、点 3 は中にある)。\nこの質問には、空間についての関数 st_intersects() を用いて、次のように答えることができる。その結果は、直感と一致するはずである。\n1 点目と 3 点目は真 (1)であるが、2 点目は偽 (空のベクタで表される) が返されポリゴンの境界の外にある。\n予想外なのは、結果がベクトルのリストという形になっていることだろう。\nこの出力は疎行列 (sparse matrix) であり、関係が存在する場合にのみ登録され、複数フィーチャに対する位相幾何学的操作のメモリ要件を軽減する。\n前節で見たように、TRUE または FALSE の値からなる密な行列 (dense matrix) が返されるのは、sparse = FALSE の時である。上記の出力では、各行がターゲット (引数 x) オブジェクトのフィーチャ、各列が選択オブジェクト (y) のフィーチャを表している。\nこの場合、y オブジェクト polygon_sfc にはフィーチャが 1 つしかないので、Section 4.2.1 で見たように部分集合に使える結果は 1 列だけである。st_intersects() は、フィーチャが接触しているだけの場合でも TRUE を返す。intersects は、Figure 4.2 に示されているように、多くのタイプの空間的関係を識別する「全て捕まえる」トポロジー操作である。\nFigure 4.2 より限定的な質問としては、どの点がポリゴン内にあるか、どのフィーチャが y と共有の境界線上にあるか、またはそれを含んでいるか、などがある。\nこうした問いには、次のように答えることができる (結果は示していない)。点 1 は境界ポリゴンに接触しているが、境界ポリゴン内にはない。\nst_intersects() の反対は st_disjoint() で、これは選択したオブジェクトと空間的に何ら関係のないオブジェクトだけを返す (注: [, 1] は結果をベクトルに変換する)。関数 st_is_within_distance() は、選択オブジェクトにほぼ接触しているフィーチャを検出する。この関数には、さらに dist という引数がある。\nターゲットオブジェクトが選択されるまでに必要な距離を設定することができる。\n‘within distance’ という二値空間述語は以下のコードチャンクで示され、その結果、すべての点がポリゴンから 0.2 単位以内にあることが示される。点 2 は polygon_sfc の最も近い頂点から 0.2 単位以上離れているが、距離を 0.2 に設定すると、まだ選択されていることに注意されたい。\nこれは、距離が最も近い辺まで測定されるためで、この場合、Figure 4.3 の点 2 の真上にあるポリゴンの部分である。\n(点 2 とポリゴンの実際の距離が 0.13 であることは、コマンド st_distance(point_sf, polygon_sfc) で確認できる。)","code":"\npolygon_matrix = cbind(\n  x = c(0, 0, 1, 1,   0),\n  y = c(0, 1, 1, 0.5, 0)\n)\npolygon_sfc = st_sfc(st_polygon(list(polygon_matrix)))\npoint_df = data.frame(\n  x = c(0.2, 0.7, 0.4),\n  y = c(0.1, 0.2, 0.8)\n)\npoint_sf = st_as_sf(point_df, coords = c(\"x\", \"y\"))\nst_intersects(point_sf, polygon_sfc)\n#> Sparse geometry binary predicate... `intersects'\n#>  1: 1\n#>  2: (empty)\n#>  3: 1\nst_intersects(point_sf, polygon_sfc, sparse = FALSE)\n#>       [,1]\n#> [1,]  TRUE\n#> [2,] FALSE\n#> [3,]  TRUE\nst_within(point_sf, polygon_sfc)\nst_touches(point_sf, polygon_sfc)\nst_disjoint(point_sf, polygon_sfc, sparse = FALSE)[, 1]\n#> [1] FALSE  TRUE FALSE\nst_is_within_distance(point_sf, polygon_sfc, dist = 0.2, sparse = FALSE)[, 1]\n#> [1] TRUE TRUE TRUE"},{"path":"spatial-operations.html","id":"distance-relations","chapter":"4 空間データ操作","heading":"4.2.3 距離関係","text":"前のセクションで示したトポロジー関係はバイナリ (フィーチャが他のフィーチャと交差するかしないか) であるが、距離関係は連続である。\n2 つの sf オブジェクト間の距離は st_distance() で計算される。これは、Section 4.2.6 の距離ベースの結合でも裏で使用されている。\nこれは、Section 4.2.1 で作成された、New Zealand で最も高い地点と Canterbury 地域の地理的な重心との間の距離を求める以下のコードチャンクに示されている。\n結果について、2 点驚くべき点がある。units がある。つまり、 100,000 メートルである。100,000 インチではない。戻り値はたとえ一つでも行列で返る。この 2 つ目の機能は、st_distance() のもう 1 つの便利な機能である、オブジェクト x と y のすべてのフィーチャの組み合わせ間の距離行列を返す機能を示唆している。\nこれは、nz_height に含まれる最初の 3 つのフィーチャと、オブジェクト co で表される New Zealand の Otago と Canterbury の地域との間の距離を求める以下のコマンドに示されている。nz_height の 2 番目と 3 番目のフィーチャーと co の 2 番目のフィーチャ間の距離はゼロであることに注意。\nこれは、点と多角形の間の距離は、ポリゴンの任意の部分 までの距離を指すという事実を示している。\nnz_height の 2 番目と 3 番目の点は Otago の中にあり、プロットすることで確認できる (結果は示していない)。","code":"\nnz_highest = nz_height |> slice_max(n = 1, order_by = elevation)\ncanterbury_centroid = st_centroid(canterbury)\nst_distance(nz_highest, canterbury_centroid)\n#> Units: [m]\n#>        [,1]\n#> [1,] 115540\nco = filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n#> Units: [m]\n#>        [,1]  [,2]\n#> [1,] 123537 15498\n#> [2,]  94283     0\n#> [3,]  93019     0\nplot(st_geometry(co)[2])\nplot(st_geometry(nz_height)[2:3], add = TRUE)"},{"path":"spatial-operations.html","id":"DE-9IM-strings","chapter":"4 空間データ操作","heading":"4.2.4 DE-9IM 文字列","text":"前節で示した二項述語の根底には、DE-9IM (Dimensionally Extended 9-Intersection Model) というものがある。\n名前からして暗号のようであり、簡単なテーマではない。しかし、空間的な関係をよりよく理解することができ、カスタム空間述語を作成することも可能になる。\nこのモデルは当初、発明者によって「2 つのフィーチャの境界、内部、外部の交点の次元」を意味する「DE + 9IM」と表示されていたが (Clementini Di Felice 1995)、現在は「DE-9IM」と表記されている (Shen, Chen, Liu 2018)。\nDE-9IM は、ユークリッド空間の 2 次元オブジェクト (点、線、ポリゴン) に適用される。つまり、このモデル (および GEOS のようなそれを実装したソフトウェア) は、投影座標参照系でデータを扱うことを前提としている。DE-9IM 文字列がどのように機能するかを示すために、Figure 4.2 の最初のジオメトリペアの様々な関連性を見てみよう。\nFigure 4.4 は、各オブジェクトの内部、境界、外部のあらゆる組み合わせの交点を示す 9 交差点モデル (9IM) を示している。最初のオブジェクト x の各コンポーネントを列とし、y の各コンポーネントを行として配置すると、各要素間の交点が強調されたファセット図形が作成される。\nFIGURE 4.4: 次元拡張 9 交差モデル (Dimensionally Extended 9 Intersection Model, DE-9IM) の仕組みを説明する図。凡例にない色は、異なる構成要素間の重なりを表している。太い線は2次元の交わりを強調する。例えば、オブジェクト x の境界とオブジェクト y の内部の交わりは、中央上部のファセットで示されている。\nDE-9IM 文字列は、各タイプの関係の次元から導き出される。\nこの場合、Figure 4.4 の赤い交点は、 Table 4.1 に示すように、0 (点)、1 (線)、2 (ポリゴン) の次元を持つ。TABLE 4.1: ジオメトリ x、y の内部、境界、外部の関係。この行列を「行単位」で一列にすると (つまり、1 行目、2 行目、3 行目の順に連結する)、文字列 212111212 が得られる。\nもうひとつの例で、このシステムを紹介する。\nFigure 4.2 に示す関係 (3 列目 1 行目のポリゴンペア) は、DE-9IM システムでは以下のように定義できる。大きなオブジェクト x の内部と y の内部、境界、外部との交点は、それぞれ 2、1、2 の次元を持つ大きなオブジェクト x の境界と y の内部、境界、外部との交点はそれぞれ F、F、1 の次元を持ち、ここで ‘F’ は ‘false’ を意味し、オブジェクトは不連続であるx の外部と y の内部、境界、外部との交点はそれぞれ F、F、2 の次元を持つ。大きなオブジェクトの外部は y の内部や境界に接触しないが、小さなオブジェクトと大きなオブジェクトの外部は同じ面積をカバーするこれら 3 つの構成要素を連結すると、文字列 212 , FF1 , FF2 が作成される。\nこれは、関数 st_relate() で得られた結果と同じである ( Figure 4.2 の他の形状がどのように作成されたかは、この章のソースコードを参照されたい)。DE-9IM 文字列を理解することで、新しい二値空間述語を開発することができる。\nヘルプページ ?st_relate では、チェスの駒を利用して、ポリゴンが境界を共有する「クイーン」 (queen) と点のみを共有する「ルーク」 (rook、将棋でいう飛車と同じ動き、ただし隣接するセルのみ) 関係に対する関数定義がそれぞれ記載されている。\n「クイーン」の関係は、「境界-境界」の関係 (Table 4.1 の 2 列目と 2 行目のセル、または DE-9IM 文字列の 5 番目の要素) が空であってはならないという意味で、パターン F***T**** に対応し、「ルーク」の関係では同じ要素が 1 でなければならない (線形交点を意味する) ことを意味している (Figure 4.5 参照)。\nこれらは以下のように実装されている。先に作成したオブジェクト x をベースに、新たに作成した関数を用いて、グリッドの中央のマスに対して、どの要素が「クイーン」「ルーク」であるかを以下のように調べることができる。\nFIGURE 4.5: 9つの形状を持つグリッドの中央の正方形に対する「クイーン」 (左) と「ルーク」 (右) の関係を見つけるためのカスタムバイナリ空間述語のデモ。\n","code":"\nxy2sfc = function(x, y) st_sfc(st_polygon(list(cbind(x, y))))\nx = xy2sfc(x = c(0, 0, 1, 1, 0), y = c(0, 1, 1, 0.5, 0))\ny = xy2sfc(x = c(0.7, 0.7, 0.9, 0.7), y = c(0.8, 0.5, 0.5, 0.8))\nst_relate(x, y)\n#>      [,1]       \n#> [1,] \"212FF1FF2\"\nst_queen = function(x, y) st_relate(x, y, pattern = \"F***T****\")\nst_rook = function(x, y) st_relate(x, y, pattern = \"F***1****\")\ngrid = st_make_grid(x, n = 3)\ngrid_sf = st_sf(grid)\ngrid_sf$queens = lengths(st_queen(grid, grid[5])) > 0\nplot(grid, col = grid_sf$queens)\ngrid_sf$rooks = lengths(st_rook(grid, grid[5])) > 0\nplot(grid, col = grid_sf$rooks)#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily"},{"path":"spatial-operations.html","id":"spatial-joining","chapter":"4 空間データ操作","heading":"4.2.5 空間結合","text":"2 つの非空間データセットを結合する場合、Section 3.2.4 で説明されているように、共有の「キー」変数に依存する。\n空間データ結合も概念的には同様であるが、変数ではなく前節で説明した空間関係に依存する。\n属性データの場合と同様に、結合では、ソースオブジェクト (y) からターゲットオブジェクト (結合関数の引数 x ) に新しい列を追加する。\n例として、地球上にランダムに分布する 10 個の点があり、そのうちの陸地にある点はどの国のものかを調べたいとする。\nこのアイデアを再現可能な例として実装することで、地理データを扱うスキルが身に付き、空間結合がどのように機能するかを知ることができる。\nまず、地表にランダムに散らばる点を作ることから始めよう。Figure 4.6 で示したシナリオでは、random_points オブジェクト (左上) には属性データがないのに対し、world (右上) には凡例で示した国名のサンプルを含む属性があることがわかる。\n空間結合は、以下のコードチャンクに示すように、st_join() で実装されている。\n出力は、random_joined のオブジェクトで、Figure 4.6 (左下) に図示されている。\n結合データセットを作成する前に、空間部分集合を用いて、ランダムな点を含む国だけを含む world_random を作成し、結合データセットで返される国名の数が4であることを検証している ( Figure 4.6 右上)。\nFIGURE 4.6: 空間結合の図解。ソースワールドオブジェクト (右上) からランダムポイント (左上) に新しい属性変数が追加され、最後のパネルで表されるデータになる。\nデフォルトでは、st_join() は左結合を行う。つまり、結果は y にマッチしない行を含む x の全ての行を含むオブジェクトとなる (Section 3.2.4 を参照)。Inner Join をする場合には、left = FALSE とする。\n空間部分集合と同様に、st_join() で使用されるデフォルトの位相演算子は st_intersects() である。これは join 引数を設定することで変更できる (詳細は ?st_join を参照)。\n上の例では、ポリゴンレイヤからポイントレイヤへの列の追加を示しているが、ジオメトリの種類に関係なく同じ方法で行える。\nこのような場合、例えば x にポリゴンが含まれ、それぞれが y の複数のオブジェクトと一致する場合、空間結合では y の一致するオブジェクトごとに新しい行が作成されるため、重複フィーチャが作られる。","code":"\nset.seed(2018) # 再現できるように seed を設定\n(bb = st_bbox(world)) # 世界の境界\n#>   xmin   ymin   xmax   ymax \n#> -180.0  -89.9  180.0   83.6\nrandom_df = data.frame(\n  x = runif(n = 10, min = bb[1], max = bb[3]),\n  y = runif(n = 10, min = bb[2], max = bb[4])\n)\nrandom_points = random_df |> \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:4326\") # 座標と CRC を設定\nworld_random = world[random_points, ]\nnrow(world_random)\n#> [1] 4\nrandom_joined = st_join(random_points, world[\"name_long\"])"},{"path":"spatial-operations.html","id":"non-overlapping-joins","chapter":"4 空間データ操作","heading":"4.2.6 距離結合","text":"2 つの地理データセットが交差 (intersect) していなくても、近接関係により地理的に強い関係がある場合がある。\nすでに spData パッケージに含まれているデータセット cycle_hire と cycle_hire_osm が良い例となる。\nこれらをプロットすると、Figure 4.7 に示すように、しばしば密接に関連しているが、接触していないことがわかる。このベースバージョンは、以下のコードで作成される。\n以下のように、同じ点であるかどうかを、st_intersects() を使って確認することができる。\nFIGURE 4.7: 公式データ (青) とOpenStreetMapのデータ (赤) に基づく、ロンドンにおける自転車レンタルポイントの空間分布。\ncycle_hire_osm の変数 capacity を cycle_hire に含まれる公式の「ターゲット」データに結合する必要があるとする。\nこのとき、オーバーラップしない結合が必要である。\n最も簡単な方法は、トポロジカル演算子 st_is_within_distance() を使用することである。これは、下の例で 20 m で示している。\n距離は、球面幾何エンジン S2 を使っていれば、非投影座標系 (例えば WGS84 の緯度経度) であってもメートル法を使うことができる。これは、 sf のデフォルトである (Section 2.2.9 参照)。これは、ターゲットオブジェクト cycle_hire の中に、閾値距離 cycle_hire_osm 内に 438 個の点があることを示している。\nそれぞれの cycle_hire_osm ポイントに関連する値を取得する方法は？\n解答は再び st_join() だが、引数 dist を追加する (20 m 以下に設定)。結合結果の行数がターゲットより多いことに注意。\nこれは、cycle_hire の一部のサイクルレンタル・ステーションが、cycle_hire_osm で複数のマッチングを行っているためである。\nChapter 3 で学習した集約方法を用いると、重なった点の値を集約して平均値を返し、対象と同じ行数を持つオブジェクトが得られる。近くのステーションの収容台数は、ソース cycle_hire_osm のデータの収容台数のプロットとこの新しいオブジェクトの結果を比較することで検証できる (プロットは表示していない)。この結合の結果、単純なフィーチャの属性データは空間演算で変更されたが、各フィーチャに関連するジオメトリは変更されていない。","code":"\nplot(st_geometry(cycle_hire), col = \"blue\")\nplot(st_geometry(cycle_hire_osm), add = TRUE, pch = 3, col = \"red\")\nany(st_intersects(cycle_hire, cycle_hire_osm, sparse = FALSE))\n#> [1] FALSE\nsel = st_is_within_distance(cycle_hire, cycle_hire_osm, \n                            dist = units::set_units(20, \"m\"))\nsummary(lengths(sel) > 0)\n#>    Mode   FALSE    TRUE \n#> logical     304     438\nz = st_join(cycle_hire, cycle_hire_osm, st_is_within_distance, \n            dist = units::set_units(20, \"m\"))\nnrow(cycle_hire)\n#> [1] 742\nnrow(z)\n#> [1] 762\nz = z |> \n  group_by(id) |> \n  summarize(capacity = mean(capacity))\nnrow(z) == nrow(cycle_hire)\n#> [1] TRUE\nplot(cycle_hire_osm[\"capacity\"])\nplot(z[\"capacity\"])"},{"path":"spatial-operations.html","id":"spatial-aggr","chapter":"4 空間データ操作","heading":"4.2.7 空間的な集計","text":"属性データの集約と同様に、空間データの集約では、集約された出力は集約されていない入力よりも少ない行数で済む。\n平均値や合計値などの統計的な集約関数は、変数の複数の値を要約し、グループ化変数ごとに単一の値を返す。\nSection 3.2.3 は、aggregate() と group_by() |> summarize() が属性変数に基づいてデータを集約する方法を示したが、このセクションでは、同じ関数が空間オブジェクトでどのように機能するかを示す。\naggregate() group_by() |> summarize() New Zealand の例に戻って、各地域の高所の平均的な高さを求めよう。ソース (この場合は y または nz) のジオメトリが、ターゲットオブジェクト (x または nz_height) の値がどのようにグループ化されるかを定義する。\nこれは、Base R の aggregate() メソッドで 1 行のコードで書くことができる。前のコマンドの結果は、 (空間) 集約オブジェクト (nz) と同じジオメトリを持つ sf オブジェクトである。これは、コマンド identical(st_geometry(nz), st_geometry(nz_agg)) で確認することができる。\n先の操作の結果を Figure 4.8 に示す。これは、New Zealand の16の地域それぞれにおける nz_height のフィーチャの平均値を示している。\nまた、次のように st_join() の出力を「tidy」関数 group_by() と summarize() にパイプすることでも、同じ結果を生成することができる。\nFIGURE 4.8: New Zealand の各地域の上位 101 の高さの平均値。\nnz_agg オブジェクトは、集計オブジェクト nz と同じジオメトリを持つが、関数 mean() を用いて、各地域の x の値をまとめた列が新たに追加されている。\nここでは、mean() の代わりに、median()、sd() など、グループごとに単一の値を返す他の関数を使用することも可能である。\n注: aggregate() と group_by() |> summarize() のアプローチの違いの一つは、前者は一致しない地域名に対して NA の値を返すのに対し、後者は地域名を保持することである。\nしたがって、「tidy」アプローチは、集計関数や結果の列名の点でより柔軟である。\n新しいジオメトリも作成する集計操作については、Section 5.2.7 で説明している。","code":"\nnz_agg = aggregate(x = nz_height, by = nz, FUN = mean)\nnz_agg2 = st_join(x = nz, y = nz_height) |>\n  group_by(Name) |>\n  summarize(elevation = mean(elevation, na.rm = TRUE))"},{"path":"spatial-operations.html","id":"incongruent","chapter":"4 空間データ操作","heading":"4.2.8 不一致レイヤを結合","text":"空間一致 (Spatial congruence) は、空間的集計に関連する重要な概念である。\n集合体 (ここでは y と呼ぶ) は、2 つのオブジェクトが境界を共有している場合、ターゲットオブジェクト (x) と一致している。\n行政区域のデータでは、大きな単位、例えばイギリスの Middle Layer Super Output Area (MSOAs) や他の多くのヨーロッパ諸国の地区が、多くの小さな単位で構成されていることがよくあることである。対照的に、不一致 (incongruent) 集約オブジェクトは、ターゲットと共通の境界を共有しない (Qiu, Zhang, Zhou 2012)。\nこれは、Figure 4.9 で説明されている空間集約 (およびその他の空間操作) において問題となる。各サブゾーンの重心を集約すると、正確な結果を得ることができない。\n面積補間は、単純な面積加重法や「ピクノフィラティック」 (pycnophylactic) 法などのより洗練されたアプローチを含む様々なアルゴリズムを使用して、1 セットの面積単位から別の単位に値を転送することによってこの問題を克服している (Waldo R. Tobler 1979)。\nFIGURE 4.9: 大きな凝集帯 (半透明の青い枠) に対して、一致する面単位 (左) と不一致する面単位 (右)。\nspData パッケージには、incongruent ( Figure 4.9 の右側のパネルにある黒い縁取りのある色のついたポリゴン) と aggregating_zones ( Figure 4.9 の右側のパネルにある半透明の青い縁取りのある 2 つのポリゴン) という名前のデータセットが含まれている。\nここで、incongruent の value 列が、百万ユーロ単位の地域総所得を指すと仮定しよう。\n基礎となる 9 つの空間ポリゴンの値を、aggregating_zones の 2 つのポリゴンにどのように移せばいいのだろうか？このための最も簡単で有用な方法は、面積加重空間補間で、incongruent オブジェクトから aggregating_zones の新しい列に、重なり合う面積に比例して値を転送する。入力と出力のフィーチャの空間交差が大きければ大きいほど、対応する値も大きくなる。\nこれは、以下のコードチャンクに示すように、st_interpolate_aw() で実装されている。この場合、所得が小さなゾーンに均等に分布していると仮定すると、総所得はいわゆる空間的に広範な変数 (面積とともに増加する) なので、集計ゾーンに入る交差点の値を合計することは意味がある (したがって、上記の警告メッセージが表示されるの)。\nこれは、平均所得やパーセンテージのような空間的に集中しがちな変数では異なるだろうが、面積が大きくなればなるほど増加するわけではない。\nst_interpolate_aw() は、空間的に集約された変数でも同様に動作する。extensive パラメータを FALSE に設定すると、集約の際に合計関数ではなく平均を使用する。","code":"\niv = incongruent[\"value\"] # 転送する値だけを残す\nagg_aw = st_interpolate_aw(iv, aggregating_zones, extensive = TRUE)\n#> Warning in st_interpolate_aw.sf(iv, aggregating_zones, extensive = TRUE):\n#> st_interpolate_aw assumes attributes are constant or uniform over areas of x\nagg_aw$value\n#> [1] 19.6 25.7"},{"path":"spatial-operations.html","id":"spatial-ras","chapter":"4 空間データ操作","heading":"4.3 ラスタデータに対する空間演算","text":"このセクションでは、ラスタデータセットを操作するためのさまざまな基本メソッドを紹介した Section 3.3 をベースに、より高度で明示的な空間ラスタ操作を実演する。また、Section 3.3 で手動で作成したオブジェクト elev と grain を使用する。\nこれらのデータセットは、読者の便宜を図るため spData パッケージにも含まれている。","code":"\nelev = rast(system.file(\"raster/elev.tif\", package = \"spData\"))\ngrain = rast(system.file(\"raster/grain.tif\", package = \"spData\"))"},{"path":"spatial-operations.html","id":"spatial-raster-subsetting","chapter":"4 空間データ操作","heading":"4.3.1 空間部分集合","text":"前の章 (Section 3.3) では、特定のセル ID や行と列の組み合わせに関連する値を取得する方法を紹介した。\nラスタオブジェクトは、位置 (座標) などの空間オブジェクトを抽出することも可能である。\n部分集合に座標を使用するには、terra の関数 cellFromXY() で座標をセル ID に「変換」することができる。\n別の方法として、terra::extract() (注意: tidyverse の中にも extract() という関数がある) を使って値を抽出することができる。\n以下に、座標 0.1, 0.1 に位置する点を覆うセルの値を求める方法を示す。\nラスタオブジェクトは、以下のコードのように、別のラスタオブジェクトの内部に部分集合することもできる。これは、Figure 4.10 に示すように、2 番目のラスタ (ここでは clip) の範囲内にある最初のラスタオブジェクト (この場合は elev) の値を取得することになる。\nFIGURE 4.10: 元のラスタ (左)、ラスタマスク (中)、ラスタをマスクした出力 (右)。\n上記の例では、特定のセルの値を返したが、多くの場合、ラスタデータセットの部分集合操作による空間出力が必要である。\nこれは、[ 演算子で、drop を FALSE に設定して行うことができる。\n以下のコードは、elev の先頭行の 2 つのセル、つまり最初の 2 行をラスタオブジェクトとして返す (出力の最初の 2 行のみ表示)。空間部分集合のもう一つの一般的な使用例は、logical (または NA) 値のラスタを使用して、同じ範囲と解像度の別のラスタをマスクする場合である (Figure 4.10 に図示)。\nこの場合、[、mask() 関数を使用することができる (結果は示していない)。上記のコードでは、rmask というマスクオブジェクトを作成し、NA と TRUE にランダムな値を割り当てている。\n次に、elev のうち、TRUE となる値を rmask に保持したい。\nつまり、elev を rmask でマスクしたい。上記の方法は、一部の値 (例えば、間違っていると予想される値) を NA に置き換えるために使用することも可能である。これらの操作は、実際のところ、2 つのラスタをセル単位で比較するローカルの論理操作である。\n次のサブセクションでは、これらの操作と関連する操作についてより詳しく説明する。","code":"\nid = cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))\nelev[id]\n# the same as\nterra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))\nclip = rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            resolution = 0.3, vals = rep(1, 9))\nelev[clip]\n# we can also use extract\n# terra::extract(elev, ext(clip))\nelev[1:2, drop = FALSE]    # spatial subsetting with cell IDs\n#> class       : SpatRaster \n#> dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#> ...\n# ラスタのマスクを作成\nrmask = elev\nvalues(rmask) = sample(c(NA, TRUE), 36, replace = TRUE)\n# 空間的に部分集合を作成\nelev[rmask, drop = FALSE]     # [ operator を使用\n# mask も使うことができる\n# mask(elev, rmask)\nelev[elev < 20] = NA"},{"path":"spatial-operations.html","id":"map-algebra","chapter":"4 空間データ操作","heading":"4.3.2 マップ代数","text":"\n「マップ代数」(map algebra) という用語は、1970年代後半に、地理的なラスタデータおよび (あまり目立たないが) ベクタデータを分析するための「規則、機能、および技術のセット」を表すために作られたものである (Tomlin 1994)。\nここでは、マップ代数をより狭く定義し、周囲のセル、ゾーン、またはすべてのセルに適用される統計関数を参照して、ラスタセル値を修正または要約する操作としている。ラスタデータセットは暗黙的に座標を保存しているだけなので、マップ代数演算は高速になる傾向があり、そのため古いことわざでは「ラスタは高速だがベクタは補正が効く」とされている。\nラスタデータセットのセルの位置は、その行列の位置と、データセットの解像度および原点 (ヘッダに格納) を使用して計算することができる。\nしかし、処理にあたっては、処理後のセル位置が変わらないことを確認すれば、セルの地理的な位置はほとんど関係ない。\nさらに、2つ以上のラスタデータセットが同じ範囲、投影、解像度を共有している場合、それらを行列として処理することができる。これは、マップ代数が terra パッケージで動作する方法である。\nまず、ラスタデータセットのヘッダを照会し、 (マップ代数演算が複数のデータセットに対して行われる場合) データセットの互換性を確認する。\n第二に、マップ代数はいわゆる一対一の位置対応を保持しており、セルは移動できないことを意味している。\nこれは、行列の掛け算や割り算などで値の位置が変わる行列代数とは異なる。マップ代数 (またはラスタデータによる地図作成) では、ラスタ操作を4つのサブクラスに分け (Tomlin 1990)、それぞれが1つまたは複数のグリッドを同時に処理するようにしている。ローカル (Local)、つまりセル単位の操作フォーカル (Focal)、つまり近傍 (Nighborhood) オペレーション。\n多くの場合、出力セルの値は、3×3 の入力セルブロックの結果ゾーン (Zonal) 演算は、フォーカル演算と似ているが、新しい値を計算する周囲の画素グリッドは不規則なサイズと形状を持つことができる。グローバル (Global) またはラスタ単位の操作。\nつまり、出力セルは 1 つまたは複数のラスタ全体から潜在的にその値を引き出す。このトポロジーは、マップ代数演算を、各ピクセル処理ステップに使用するセル数と出力の種類によって分類したものである。\nなお、ラスタ演算は、地形、水文解析、画像分類などの分野ごとの分類方法もある。\n以下では、各タイプのマップ代数演算の使用方法について、動作例を参照しながら説明する。","code":""},{"path":"spatial-operations.html","id":"local-operations","chapter":"4 空間データ操作","heading":"4.3.3 ローカル操作","text":"\nローカル操作は、1 つまたは複数のレイヤにおけるすべてのセル単位の操作で構成されている。\nこれには、ラスタからの値の加算や減算、ラスタの二乗や乗算が含まれる。\nラスタ代数では、特定の値 (下の例では 5) より大きいラスタセルをすべて見つけるなどの論理演算も可能である。\nterra パッケージは、以下のように、これらすべての操作に対応している (Figure 4.11)。\nFIGURE 4.11: elev ラスタオブジェクトのさまざまなローカル操作の例: 2つのラスタの加算、二乗、対数変換の適用、論理演算の実行。\nローカル演算のもう一つの良い例は、デジタル標高モデルを低標高 (クラス 1)、中標高 (クラス 2)、高標高 (クラス 3) にグループ化するように、数値の間隔をグループに分類することである。\nclassify() コマンドを使って、まず再分類行列を作る必要がある。ここで、最初の列はクラスの下限、2 番目の列は上限に対応する。\n3 列目は、1 列目と 2 列目で指定した範囲の新しい値を表している。ここでは、0～12、12～24、24～36 の範囲のラスタ値をそれぞれ 1、2、3 に再分類している。classify() 関数は、カテゴリ別ラスタのクラス数を減らしたい場合にも使用できる。\nChapter 14 では、いくつかの追加的な再分類を実施する予定である。直接算術演算子するだけでなく、app()、tapp()、lapp() 関数も使用することができる。\nより効率的であるため、大規模なラスタデータが存在する場合に適している。\nさらに、出力ファイルを直接保存することも可能である。\n関数 app() は、ラスタの各セルに関数を適用し、複数のレイヤの値を 1 つのレイヤにまとめる (合計を計算するなど) ために使用される。\ntapp() は、app() の拡張で、ある操作を行いたいレイヤの部分集合 (index の引数を参照) を選択することができるようになっている。\n最後に、関数 lapp() は、レイヤを引数として各セルに関数を適用することができる。lapp() のアプリケーションを以下に示す。正規化差分植生指数 (normalized difference vegetation index, NDVI) の算出は、よく知られたローカル (ピクセル単位) のラスタ処理である。\n正の値は生きた植物が存在することを示す(ほとんどが 0.2 以上)。\nNDVI は、Landsat や Sentinel などの衛星システムから得られるリモートセンシング画像の赤色および近赤外 (near-infrared、NIR) バンドから算出されるものである。\n植物は可視光線、特に赤色光を強く吸収し、近赤外光を反射する。以下は、NDVI の式である。\\[\n\\begin{split}\nNDVI&= \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red}}\\\\\n\\end{split}\n\\]Zion 国立公園のマルチスペクトル衛星ファイルについて、NDVI を計算してみよう。ラスタオブジェクトは、Landsat 8 からの青、緑、赤、近赤外 (NIR) の 4 つの衛星バンドを持っている。\n重要なのは、Landsat レベル 2 のプロダクトはディスクスペースを節約するために整数として保存されているため、計算を行う前に浮動小数点数に変換する必要があるということだ。\nそのためには、スケーリング係数 (0.0000275) を適用し、元の値にオフセット (-0.2) を加える必要がある。21適切な値は 0 から 1 の範囲になければならない。\n実際には範囲外のデータがあるが、その原因はおそらく雲やその他の大気の影響によるものだと思われる。\n以下の通り、負値を 0 に置き換える。次のステップは、NDVI の計算式を R の関数に実装することである。この関数は、2 つの数値引数 (nir と red) を受け取り、NDVI 値を含む数値ベクトルを返す。\nlapp() の fun 引数として使用することができる。\nこの関数が想定するのは 2 つのバンド (元のラスタの 4 つではない) であり、それらは NIR、赤の順である必要があることを覚えておく必要がある。\nそのため、入力ラスタを部分集合し、計算を行う前に multi_rast [[c(4, 3)] ] で部分集合してから計算を行う。その結果を右図 (Figure 4.12 ) に示すように、同じ領域の RGB 画像 (同図の左図) と比較することができる。\nこれにより、NDVI 値が最も大きいのは北部の密林地帯、最も低いのは北部の湖と雪山の尾根に関連していることがわかる。\nFIGURE 4.12: Zion 国立公園の衛星ファイルの例で計算された RGB 画像 (左) と NDVI 値 (右)\n予測マッピングも、ローカルラスタ操作の興味深い応用例である。\n応答変数は、例えば、種の豊富さ、地滑りの存在、木の病気、作物の収穫量など、空間における測定または観測された点に対応する。\nその結果、様々なラスタ (標高、pH、降水量、気温、土地被覆、土壌等級など) から、宇宙や空中の予測変数を簡単に取得することができるようになる。\nその後、lm()、glm()、gam() または機械学習技術を使用して、予測因子の関数として応答をモデル化する。\nしたがって、ラスタオブジェクトの空間予測は、予測ラスタ値に推定係数を適用し、出力ラスタ値を合計することで行うことができる (Chapter 15 を参照)。","code":"\nelev + elev\nelev^2\nlog(elev)\nelev > 5\nrcl = matrix(c(0, 12, 1, 12, 24, 2, 24, 36, 3), ncol = 3, byrow = TRUE)\nrcl\n#>      [,1] [,2] [,3]\n#> [1,]    0   12    1\n#> [2,]   12   24    2\n#> [3,]   24   36    3\nrecl = classify(elev, rcl = rcl)\nmulti_raster_file = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nmulti_rast = rast(multi_raster_file)\nmulti_rast = (multi_rast * 0.0000275) - 0.2\nmulti_rast[multi_rast < 0] = 0\nndvi_fun = function(nir, red){\n  (nir - red) / (nir + red)\n}\nndvi_rast = lapp(multi_rast[[c(4, 3)]], fun = ndvi_fun)"},{"path":"spatial-operations.html","id":"focal-operations","chapter":"4 空間データ操作","heading":"4.3.4 フォーカル操作","text":"\nローカルな機能とは、1 つのセル (複数の層からなる可能性もある) を対象とするものであるのに対し、フォーカルな機能とは、中心 (焦点) のセルとその近隣のセルを考慮したものである。\n近傍領域 (カーネル、フィルタ、移動窓とも呼ばれる) は、通常、3×3 セル (つまり、中心セルとその周囲 8 個の近傍セル) のサイズであるが、ユーザーが定義する他のサイズや形状 (必ずしも長方形ではない) をとることができる。\nフォーカル操作は、指定された近傍領域内のすべてのセルに集約関数を適用し、対応する出力を中心セルの新しい値として使用し、次の中心セル (Figure 4.13) へと進む。\nこの操作は、空間フィルタリング (spatial filtering) や畳み込み (convolution) などと呼ばれることもある (Burrough, McDonnell, Lloyd 2015)。R では、focal() 関数で空間フィルタリングを行うことができる。\n移動窓の形状を matrix で定義する。その値は重みに対応する (以下のコードチャンクの w パラメータを参照)。\n次に、fun パラメータで、この近傍領域に適用したい関数を指定することができる。\nここでは、最小値を選んでいるが、sum()、mean()、var() など、他の要約関数も使用可能である。main() 関数は、プロセス中の NA を削除すべきか (na.rm = TRUE)、しないか (na.rm = FALSE こちらがデフォルト) などの追加引数も受け付ける。\nFIGURE 4.13: 焦点演算による入力ラスタ (左) と出力ラスタ (右) -3×3の移動窓で最小値を求める。\n期待通りの出力が得られるかどうか、すぐに確認することができる。\nこの例では、最小値は常に移動窓の左上隅でなければならない (入力ラスタは、左上隅から始まるセルの値を行単位で1つずつ増加させることによって作成したことを思い出してほしい)。\nこの例では、重み付け行列は 1 だけで構成されており、各セルの出力に対する重みが同じであることを意味しているが、これは変更可能である。画像処理では、焦点関数やフィルタが重要な役割を担っている。\nローパスフィルタやスムージングフィルタは、平均関数を用いて極端な部分を除去する。\nカテゴリデータの場合、平均値を最頻値に置き換えることができる。\nそれに対して、ハイパスフィルタはフィーチャを強調する。\nここでは、ライン検出のラプラスフィルタやソーベルフィルタがその例として挙げられるだろう。\nR での使い方は、focal() のヘルプページで確認してみよう (この章の最後の演習でも使用する)。傾斜、アスペクト、流れ方向などの地形特性を計算する地形処理では、焦点関数に依存している。\nterrain() は、これらの指標の計算に使用することができる。ただし、勾配を計算する Zevenbergen Thorne 法を含むいくつかの地形アルゴリズムは、この terra 関数には実装されていない。\nその他、曲率、寄与率、湿潤指数など多くのアルゴリズムが、オープンソースのデスクトップ型地理情報システム (GIS) ソフトウェアに実装されている。\nChapter 10 は、このような GIS 機能を R 内からアクセスする方法を示している。","code":"\nr_focal = focal(elev, w = matrix(1, nrow = 3, ncol = 3), fun = min)"},{"path":"spatial-operations.html","id":"zonal-operations","chapter":"4 空間データ操作","heading":"4.3.5 ゾーン操作","text":"\nゾーン演算は、フォーカル演算と同様に、複数のラスタセルに集計関数を適用する。\nしかし、前節で紹介したフォーカル操作の場合の近傍窓とは対照的に、ゾーン操作の場合は、通常カテゴリ値で構成される第二ラスタがゾーンフィルター/u> (または「ゾーン」) を定義する。\nそのため、ゾーンフィルタを定義するラスタセルは、必ずしも隣接している必要はない。\n粒径ラスタはその良い例で、Figure 3.2 の右側のパネルに示されているように、異なる粒径がラスタ全体に不規則に広がっていることがわかる。\n最後に、ゾーン操作の結果は、ゾーンごとにグループ化された要約表となる。このため、この操作は、GIS の世界ではゾーン統計とも呼ばれる。\nこれは、デフォルトでラスタオブジェクトを返すフォーカルオペレーションとは対照的である。次のコードチャンクは、zonal() 関数を使用して、各粒度クラスに関連する平均標高を計算する。これは、各カテゴリの統計値 、ここでは各粒度クラスの平均高度を返す。\n注: .raster 引数を TRUE に設定することで、各ゾーンの統計情報を計算したラスタを取得することも可能である。","code":"\nz = zonal(elev, grain, fun = \"mean\")\nz\n#>   grain elev\n#> 1  clay 14.8\n#> 2  silt 21.2\n#> 3  sand 18.7"},{"path":"spatial-operations.html","id":"global-operations-and-distances","chapter":"4 空間データ操作","heading":"4.3.6 グローバルな操作と距離","text":"グローバル操作は、ラスタデータセット全体が 1 つのゾーンに相当するゾーン操作の特殊なケースである。\n最も一般的なグローバル操作は、最小値や最大値など、ラスタデータセット全体の記述統計である。これらの操作については、Section 3.3.2 ですでに説明した。それ以外にも、距離や重みのラスタの計算にもグローバル操作は有効である。\n最初のケースでは、各セルから特定のターゲットセルまでの距離を計算することができる。\n例えば、最も近い海岸までの距離を計算したい場合がある (terra::distance() も参照)。\nまた、地形も考慮したい。つまり、純粋な距離だけでなく、海岸に行くときに山脈を越えないようにしたいのである。\nそのためには、標高が 1 メートル増えるごとにユークリッド距離が「伸びる」ように、標高で距離に重みをつければよい (この章の演習 E8 と E9 で実際に行う。)。\nVisibility と viewshed の計算もグローバル操作に属する (Chapter 10 の演習では、viewshed ラスタを計算する)。","code":""},{"path":"spatial-operations.html","id":"map-algebra-counterparts-in-vector-processing","chapter":"4 空間データ操作","heading":"4.3.7 ベクタ処理における写像代数の対応","text":"多くのマップ代数演算はベクタ処理に対応するものである (Liu Mason 2009)。\n最大距離のみを考慮した距離ラスタの計算 (グローバル演算) は、ベクタバッファ演算 (Section 5.2.5) と同等である。\nラスタデータの再分類 (入力に応じてローカルまたはゾーン関数) は、ベクタデータの融合 (dissolve) (Section 4.2.5) と同等である。\n2 つのラスタを重ね合わせ (ローカル操作)、一方がマスクを表す NULL または NA の値を含む場合、ベクタクリッピング (Section 5.2.5) に類似している。\n空間クリッピングとよく似たものに、2 つのレイヤを交差させるものがある (Section 4.2.1 )。\n違いは、これら 2 つのレイヤ (ベクタまたはラスタ) は、単に重複する領域を共有することである (例として Figure 5.8 を参照)。\nただし、表現には注意が必要である。\nラスタデータモデルとベクタデータモデルでは、同じ言葉でも微妙に意味が異なることがある。\n集計とは、ベクタデータの場合はポリゴンを分解することであり、ラスタデータの場合は解像度を上げることである。\nゾーン演算は、あるラスタのセルを、別のラスタのゾーン (カテゴリ) に合わせて、集約関数 (上記参照) を使って分解することができる。","code":""},{"path":"spatial-operations.html","id":"merging-rasters","chapter":"4 空間データ操作","heading":"4.3.8 ラスタのマージ","text":"\nNDVI を計算し (Section 4.3.3 参照)、さらに、調査地域内の観測の標高データから地形属性を計算したいとする。\nこのような計算には、リモートセンシングの情報が必要である。\nリモートセンシング画像は、特定の空間範囲をカバーするシーンに分割されることが多く、1つの調査エリアが複数のシーンにまたがっていることがよくある。\nそして、調査対象シーンをマージする必要がある。\n一番簡単なのは、これらのシーンをマージする、つまり並べることである。\nこれは例えば、デジタル標高データで可能である。\n以下のコードでは、まずオーストリアとスイスの Shuttle Radar Topography Mission (SRTM) 標高データをダウンロードする (国番号については、geodata 関数 country_codes() を参照)。\n第二段階では、2 つのラスタを 1 つに統合する。terra の merge() コマンドは、2 つの画像を合成し、重なった場合は、最初のラスタの値を使用する。重複する値が互いに対応しない場合、マージはほとんど意味がない。\nこのようなケースは、撮影日が異なるシーンの分光画像を合成する場合によくある。\nmerge() コマンドはそのまま使え、出来上がった画像にはっきりとした枠が表示される。\n一方、mosaic() コマンドでは、オーバーラップする領域に対して関数を定義することができる。\n例えば、平均値を計算することもできる。この場合、マージされた結果における明確な境界線は滑らかになるだろうが、ほとんどの場合、それを消すことはできない。R によるリモートセンシングの詳しい紹介は、Wegmann, Leutner, Dech (2016) を参照。","code":"\naut = geodata::elevation_30s(country = \"AUT\", path = tempdir())\nch = geodata::elevation_30s(country = \"CHE\", path = tempdir())\naut_ch = merge(aut, ch)"},{"path":"spatial-operations.html","id":"演習-2","chapter":"4 空間データ操作","heading":"4.4 演習","text":"E1. Canterbury は、New Zealand の最高峰 101 地点のほとんどを含む地域であることは、Section 4.2 で述べたとおりである。\nCanterbury 地方には、これらの高地がいくつあるだろうか？ボーナス: その結果を plot() 関数を使って次のように表示しなさい。、まず、New Zealand 全土を示し、canterbury 地域を黄色で強調し、高地を赤い十字 (ヒント: pch = 7)、かつ他の地域の高地は青い円で表示しなさい。異なる pch 値の図解を含む詳細については、ヘルプページ ?points を参照。E2. nz_height高地の数が 2 番目に多いのはどの地方で、いくつあるか？E3. この質問を全地域に一般化すると、ニュージーランドの 16 地方のうち、国内最高地点トップ 100 に属する地点を含む地域はいくつあるか? どの地方か?ボーナス: これらの地域を、点の数と名前の順に並べた表を作成しなさい。E4. 空間述語の知識を試すために、アメリカの州と他の空間オブジェクトとの関係を調べ、プロットしてみよう。この練習の出発点は、アメリカのコロラド州を表すオブジェクトを作成することである。次のコマンドで実行する。\ncolorado = us_states[us_states$NAME == \"Colorado\",] (base R)、または filter() 関数 (tidyverse) を使って、結果のオブジェクトをアメリカの州のコンテキストでプロットしなさい。Colorado 州と地理的に交差するすべての州を表す新しいオブジェクトを作成し、その結果をプロットしなさい (ヒント: これを行う最も簡潔な方法は、部分集合化メソッド [] を使用する)。Colorado 州に接する (境界を共有する) すべてのオブジェクトを表すもう 1 つのオブジェクトを作成し、その結果をプロットしなさい (ヒント: Base R の空間部分集合操作中に、引数 op = st_intersects やその他の空間関係を使用できる)。ボーナス: 東海岸に近い Columbia 特別区の重心から、アメリカの西海岸に California 州の重心までの直線を作成し（ヒント: Chapter 5 で説明した関数 st_centroid()、st_union()、st_cast()が役に立つ)、この東西に長い直線がどの州を横切るかを特定しなさい。E5. dem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\")) を使用し、標高を低 (<300)、中、高 (>500) の 3 つのクラスに再分類しなさい。\n次に、NDVI ラスタ(ndvi = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))) を読み込み、各標高クラスの平均 NDVI と平均標高を計算しなさい。E6. rast(system.file(\"ex/logo.tif\", package = \"terra\")) にライン検出フィルタを適用しなさい。\n結果をプロットしなさい。\nヒント: ?terra::focal() を読むと良い。E7. ランドサット画像の正規化差分水分指数 (Normalized Difference Water Index; NDWI; (green - nir)/(green + nir)) を計算しなさい。\nspDataLargeパッケージが提供するランドサット画像を使用しなさい (system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))。\nまた、このエリアの NDVI と NDWI の相関を計算しなさい (ヒント: layerCor() 関数を使用できる)。E8. StackOverflow の投稿では、raster::distance() を使って最も近い海岸線までの距離を計算する方法が紹介されている。\n似たようなことを terra::distance() を使ってやってみよう。 Spain のデジタル標高モデルを取得し、全国の海岸までの距離を表すラスタを計算しなさい (ヒント: geodata::elevation_30s() を使う)。\n得られた距離をメートルからキロメートルに変換しなさい。\n注意: 操作 (aggregate()) の計算時間を短縮するために、入力ラスタのセルサイズを大きくすることが賢明かもしれない。E9. 距離ラスタを標高ラスタで加重することによって、上記の演習で使用したアプローチを修正しなさい。100 高度メートルごとに、海岸までの距離が 10 km 増加する。\n次に、ユークリッド距離 (E7) を使って作成したラスタと標高で重み付けしたラスタの差を計算し、可視化しなさい。","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spData)"},{"path":"geometry-operations.html","id":"geometry-operations","chapter":"5 ジオメトリ演算","heading":"5 ジオメトリ演算","text":"","code":""},{"path":"geometry-operations.html","id":"prerequisites-05","chapter":"5 ジオメトリ演算","heading":"必須パッケージ","text":"この章では、Chapter 4 と同じパッケージを使用するが、Chapter 2 でインストールされた spDataLarge を追加している。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)"},{"path":"geometry-operations.html","id":"introduction-05","chapter":"5 ジオメトリ演算","heading":"5.1 イントロダクション","text":"これまで本書では、地理データセットの構造 (Chapter 2)、地理以外の属性 (Chapter 3) と空間関係 (Chapter 4) に基づく操作方法について説明してきた。\nこの章では、バッファ作成、ベクタの簡略化と変換、ラスタデータの集計や最サンプルなど、空間オブジェクトの地理的要素の操作に重点を置いている。\nこの本を読めば (そして最後にある演習を試した後)、sf オブジェクトのジオメトリ列と、他の地理的オブジェクトとの関係でラスタに表されたピクセルの範囲と地理的位置を理解し、コントロールできるようになるはずである。Section 5.2 は、「単項」と「二項」演算によるベクタジオメトリの変換を扱う。\n単項演算 (unary operation) とは、単体のジオメトリに対して、線分やポリゴンの単純化、バッファや中心点の作成、「アフィン変換」による単体のジオメトリの移動・拡大・縮小・回転などを行う (Section 5.2.1 ～ Section 5.2.4 を参照)。\n一方、二項変換 (binary transformations) とは、あるジオメトリを別のジオメトリの形状に基づいて変更するもので、Section 5.2.5 で切り取り (clip) を、Section 5.2.7 で結合 (union) を説明する。\nジオメトリ型の変換 (例えば、ポリゴンからラインへの変換) は、Section 5.2.8 で実際に行う。Section 5.3 は、ラスタオブジェクトの幾何学変換を扱っている。\nこれは、基本となる画素のサイズと数を変更し、新しい値を割り当てるというものである。\nラスタの解像度 (ラスタ集計・分解ともいう)、範囲、原点を変更する方法を教える。\nこれらの操作は、異なるソースのラスタデータセットの位置合わせを行う場合に特に有効である。\n整列されたラスタオブジェクトは、ピクセル間の一対一の対応を共有し、Section 4.3.2 で説明されているマップ代数演算を使用して処理することができる。Section 6 では、ベクタオブジェクトとラスタオブジェクトの間の操作をカバーする。\nラスタ値をベクタジオメトリで「マスク」し、「抽出」する方法を紹介する。\n重要なのは、ラスタデータを「ポリゴン化」し、ベクタデータを「ラスタ化」する方法を示し、この2つのデータモデルをより互換性のあるものにすることである。","code":""},{"path":"geometry-operations.html","id":"geo-vec","chapter":"5 ジオメトリ演算","heading":"5.2 ベクタデータに対するジオメトリ操作","text":"ここでは、ベクタ (sf) オブジェクトのジオメトリを何らかの方法で変更する操作について説明する。\n前の章 (Section 4.2) で紹介した空間データ操作よりも高度なもので、ここではジオメトリを掘り下げていくことがある。\nこのセクションで説明する関数は、クラス sf のオブジェクトに加えて、クラス sfc のオブジェクトにも作用する。","code":""},{"path":"geometry-operations.html","id":"simplification","chapter":"5 ジオメトリ演算","heading":"5.2.1 簡略化","text":"\n簡略化とは、通常、縮尺の小さい地図で使用するために、ベクタオブジェクト (線やポリゴン) を一般化する処理のことである。\nオブジェクトを単純化するもう一つの理由は、それらが消費するメモリ、ハードディスク容量、ネットワーク帯域幅の量を減らすためである。\nインタラクティブ地図として公開する前に、複雑な形状を簡略化することが賢明だろう。\nsf パッケージは st_simplify() を提供する。これは Douglas-Peucker アルゴリズムの実装を使用して、頂点数を削減するものである。\nst_simplify() は、dTolerance を使用することで、一般化のレベルを地図で使われている単位で制御することができる (詳細は Douglas Peucker 1973)。\nFigure 5.1 は、セーヌ川とその支流を表すジオメトリ LINESTRING を簡略化したものである。\n以下のコマンドで簡略化したジオメトリを作成してみよう。\nFIGURE 5.1: seine のオリジナルと簡略化した形状の比較。\nここでできた seine_simp オブジェクトは、元の seine のコピーであるが、頂点の数は少なくなっている。\nこれは明らかで、以下の検証のように、結果は視覚的にシンプルになり (Figure 5.1、右)、元のオブジェクトよりもメモリ消費が少ない。\n簡略化はポリゴンにも適用できる。\n下の例は、米国本土 us_states を表している。st_simplify() の制限として、ジオメトリ単位でオブジェクトを簡略化することが挙げられる。\nこのため、「トポロジー」が失われ、Figure 5.2 (パネル右上) に示すような、重なり合った「穴のあいた」面単位になってしまうのである。\nrmapshaper の ms_simplify() が代替となる。\nデフォルトでは、Douglas-Peucker アルゴリズムのいくつかの制限を克服した Visvalingam アルゴリズムが使用される (Visvalingam Whyatt 1993)。\n\n次のコードチャンクは、この関数を使用して、us_states を簡略化している。\n結果は入力 (引数 keep で設定) の 1% の頂点しか持たないが、keep_shapes = TRUE を設定したため、オブジェクトの数はそのままである。22\n簡略化の代わりに、ポリゴンや線のジオメトリの境界を平滑化するという方法もあり、smoothr パッケージで実装されている。\n平滑化はジオメトリのエッジを補間するため、必ずしも頂点の数が少なくなるわけではないが、ラスタを空間的にベクトル化したジオメトリを扱うときに特に有用である (このトピックは Chapter 6 で説明する)。\nSmoothr は、Gaussian kernel 回帰、Chaikin’s corner cutting アルゴリズム、スプライン補間の 3 つの平滑化手法を実装しており、パッケージ vignette とwebで説明されている。\nst_simplify() と同様に、平滑化アルゴリズムは「トポロジー」を保存しないことに注意。\nsmoothr の主要関数は smooth() であり、 method 引数は使用する平滑化手法を指定する。\n以下は、Gaussian kernel 回帰を使用して、method=ksmooth を使用して米国の州の境界線を滑らかにする。\n引数 smoothness は、形状を滑らかにするために使用するガウスの帯域幅を制御する。デフォルト値は 1。最後に、元のデータセットと 2 つの簡易版を視覚的に比較してみよう。\nFigure 5.2 で、Douglas-Peucker (st_simplify)、Visvalingam (ms_simplify)、Gaussian kernel 回帰 (smooth(method=ksmooth) アルゴリズムの出力に違いがあることがわかる。\nFIGURE 5.2: ポリゴンの簡略化。sf (右上)、rmapshaper (左下)、smoothr (右下) の各パッケージの関数で生成された簡略版と元のアメリカ合衆国のジオメトリ形状を比較。\n","code":"\nseine_simp = st_simplify(seine, dTolerance = 2000) # 2000 m\nobject.size(seine)\n#> 18096 bytes\nobject.size(seine_simp)\n#> 9112 bytes\nus_states_simp1 = st_simplify(us_states, dTolerance = 100000) # 100 km\n# 保持するポイントの割合 (0～1、デフォルト 0.05)\nus_states_simp2 = rmapshaper::ms_simplify(us_states, keep = 0.01,\n                                          keep_shapes = TRUE)\nus_states_simp3 = smoothr::smooth(us_states, method = \"ksmooth\", smoothness = 6)#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily"},{"path":"geometry-operations.html","id":"centroids","chapter":"5 ジオメトリ演算","heading":"5.2.2 重心","text":"\n重心 (centroid) 演算は、地理的な物体の中心を特定するものである。\n統計的な中心傾向の測定 (「平均」の平均値や中央値の定義を含む) と同様に、物体の地理的な中心を定義する方法はたくさんある。\nいずれも、より複雑なベクタオブジェクトの一点表現を作成する。重心の操作で最もよく使われるのは、地理的重心である。\nこのタイプの重心操作 (「セントロイド」とも呼ばれる) は、空間オブジェクトにおける質量の中心を表す (指の上で皿のバランスをとることを想像してほしい)。\n地理的重心は、複雑な形状をシンプルな点で表現したり、ポリゴン間の距離を推定したりと、さまざまな用途に利用されている。\nこれらは、以下のコードで示すように、sf 関数 (st_centroid()) で計算することができる。では、New Zealand の地域とセーヌ川の支流の地理的重心 (Figure 5.3 の黒い点で示される) を生成してみよう。地理的重心は、親オブジェクトの境界の外にあることもある (ドーナツを想像すればよい)。\nこのような場合、親オブジェクトに点があることを保証するために、point surface オペレーションを使用することができ (例: 島国のような不規則なマルチポリゴンオブジェクトのラベル付け)、Figure 5.3 の赤い点に示されている。\nこれらの赤い点は、常に親オブジェクトの上にあることに確認しておこう。\nst_point_on_surface() を使い、以下のように作成する。23\nFIGURE 5.3: New Zealand の地域 (左) とセーヌ川 (右) のデータセットの重心 (黒い点) と「サーフェス上の点」(赤い点)。\n重心には、他にもチェビシェフ中心やビジュアル中心などが存在する。\nここでは深入りしないが、Chapter 11 で見るように、R を使って計算することが可能である。","code":"\nnz_centroid = st_centroid(nz)\nseine_centroid = st_centroid(seine)\nnz_pos = st_point_on_surface(nz)\nseine_pos = st_point_on_surface(seine)"},{"path":"geometry-operations.html","id":"buffers","chapter":"5 ジオメトリ演算","heading":"5.2.3 バッファ","text":"\nバッファとは、フィーチャから一定距離内の領域を表すポリゴンのことである。\n入力が点、線、ポリゴンのいずれであっても、出力はバッファとなる。\n簡略化 (可視化やファイルサイズの縮小によく使われる) とは異なり、バッファ作成は地理的なデータ解析に使われる傾向がある。\nこの線から所定の距離内にある点はいくつあるか?\nこの新店舗から移動可能な距離にあるのは、どのような層なのだろうか?\nこのような疑問には、関心のある地理的要素の周囲にバッファを作成することで回答し、可視化することができる。Figure 5.4 は、セーヌ川と支流を囲む様々な大きさの緩衝地帯 (5 km と 50 km)を示している。\nこれらのバッファは以下のコマンドで作成できる。コマンド st_buffer() は少なくとも 2 つの引数を必要とする。入力ジオメトリと、CRS の単位 (この場合はメートル) で指定された距離である。\nFIGURE 5.4: Seine データセット周辺の 5 km (左) と 50 km (右) のバッファ。ジオメトリフィーチャごとに 1 つのバッファが作成されることを反映した色に注目。\nst_buffer() には、追加の引数がある。\n最も重要な引数は、以下の通り。nQuadSegs (GEOS エンジン使用時): これは「1 象限あたりの分割数」を意味し、デフォルトでは 30 に設定されている (バッファで作られる円は \\(4 \\times 30 = 120\\) ラインで構成されることを意味する)。\nこの引数が有用な例外的なケースとしては、バッファ操作の出力によって消費されるメモリが大きな懸念材料である場合 (この場合は減らす)、または非常に高い精度が必要な場合 (この場合は増やす)。max_cells (S2 エンジン使用時): 値が大きいほどバッファが滑らかになるが、時間がかかるendCapStyle と joinStyle (GEOS エンジン使用時): バッファの縁の見た目を制御するsingleSide (GEOS エンジン使用時): バッファを入力ジオメトリの片側につくるか両側につくるかを制御する","code":"\nseine_buff_5km = st_buffer(seine, dist = 5000)\nseine_buff_50km = st_buffer(seine, dist = 50000)#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily"},{"path":"geometry-operations.html","id":"affine-transformations","chapter":"5 ジオメトリ演算","heading":"5.2.4 アフィン変換","text":"\nアフィン変換とは、直線と平行を保存する変換のことである。\nただし、角度や長さは必ずしも保存されるとは限らない。\nアフィン変換には、特に、平行移動、拡大縮小、回転が含まれる。\nさらに、これらを任意に組み合わせて使用することも可能である。\nアフィン変換はジオコンピュテーションに不可欠な要素である。\n例えば、ラベルの配置には平行移動が必要であり、非連続領域のカルトグラムでは拡大縮小が使用され (Section 9.6 参照)、歪んだ地図や間違った投影に基づいて作成されたジオメトリを再投影または改善する際には多くのアフィン変換が適用される。\nsf パッケージは sfg と sfc のクラスのオブジェクトに対してアフィン変換を実装している。平行移動は、すべての点を地図単位で同じ距離だけ移動させる。\nベクタオブジェクトに数値ベクタを追加することで、実現できるだろう。\n例えば、以下のコードでは、すべての y 座標を北に 10 万メートル移動させ、x 座標はそのままにしている (Figure 5.5 左図)。拡大縮小は、オブジェクトを係数倍ずつ拡大または縮小する機能である。\nグローバルにもローカルにも適用可能である。\nグローバル拡大縮小は、すべてのジオメトリの位相関係を維持したまま、原点座標を基準としてすべての座標値を増減させる。\nsfg、sfc オブジェクトの減算または乗算によって行うことができる。ローカル拡大縮小はジオメトリを独立して扱い、ジオメトリが拡大縮小されるポイント (重心など) を必要とする。\n以下の例では、各ジオメトリは重心を中心に2倍に縮小されている (Figure 5.5 中央)。\nそのために、まず各オブジェクトは、その中心が 0, 0 ((nz_sfc - nz_centroid_sfc)) の座標となるように移動される。\n次に、ジオメトリのサイズを半分に縮小する (* 0.5)。\n最後に、各オブジェクトの重心を入力データの座標に戻す (+ nz_centroid_sfc)。2 次元座標の回転には、回転行列が必要である。\\[\nR =\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\  \n\\sin \\theta & \\cos \\theta \\\\\n\\end{bmatrix}\n\\]時計回りにポイントを回転させる。\n回転行列は R で次のように実装することができる。rotation 関数は、1 つの引数 (回転角度, angle) を度単位で受け取る。\n重心のような選択された点を中心に回転させることができる (Figure 5.5 右図)。\nその他の例については、vignette(\"sf3\") を参照 (訳注: 日本語版)。\nFIGURE 5.5: アフィン変換 (平行移動、拡大縮小、回転)。\n最後に、新しく作成されたジオメトリは、st_set_geometry() 関数を使用して古いジオメトリを置き換えることができる。","code":"\nnz_sfc = st_geometry(nz)\nnz_shift = nz_sfc + c(0, 100000)\nnz_centroid_sfc = st_centroid(nz_sfc)\nnz_scale = (nz_sfc - nz_centroid_sfc) * 0.5 + nz_centroid_sfc\nrotation = function(a){\n  r = a * pi / 180 # 度をラジアンに変換\n  matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2, ncol = 2)\n} \nnz_rotate = (nz_sfc - nz_centroid_sfc) * rotation(30) + nz_centroid_sfc#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\nnz_scale_sf = st_set_geometry(nz, nz_scale)"},{"path":"geometry-operations.html","id":"clipping","chapter":"5 ジオメトリ演算","heading":"5.2.5 切り取り (clip)","text":"\n空間切り取り (clip) は、空間部分集合の作成の一種で、影響を受けるフィーチャの少なくとも一部の geometry 列を変更するものである。切り取りは、\n点よりも複雑なフィーチャである線、ポリゴン、およびそれらの複合にしか適用できない。\nコンセプトを説明するために、まずは簡単な例で説明する。\n中心点が互いに 1 単位離れていて、半径が 1 である 2 つの重なり合った円を用意しよう (Figure 5.6)。\nFIGURE 5.6: 重なり合った円。\nどちらかの円を選択するのではなく、x と y の両方で覆われた空間を選択したい。\nこれは、関数 st_intersection()、左手と右手の円を表す x と y というオブジェクトを使って説明することができる (Figure 5.7)。\nFIGURE 5.7: 重なり合った円はグレーで表示され、円同士が交差していることを示す。\n次のコードチャンクは、x と y を表すベン図のすべての組み合わせに対して、これがどのように機能するかを示している。これは、書籍 R Data Science の Figure 5.1 から着想を得ている (Grolemund Wickham 2016)。\nFIGURE 5.8: 論理演算子の空間的等価性。\n","code":"\nop = par(mar = rep(0, 4))\nb = st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # 点を二つ作成\nb = st_buffer(b, dist = 1) # 点を円に変換\nplot(b, border = \"gray\")\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3) # テキストを追加\nop = par(mar = rep(0, 4))\nx = b[1]\ny = b[2]\nx_and_y = st_intersection(x, y)\nplot(b, border = \"gray\")\nplot(x_and_y, col = \"lightgray\", border = \"gray\", add = TRUE) # 範囲を intersect"},{"path":"geometry-operations.html","id":"部分集合と切り取り-clip","chapter":"5 ジオメトリ演算","heading":"5.2.6 部分集合と切り取り (clip)","text":"\nオブジェクトの切り取りは、その形状を変更することができるが、オブジェクトを部分集合を作成することもでき、切り取り/部分集合オブジェクトと交差する (または部分的に交差する) フィーチャのみを返す。\nこの点を説明するために、を Figure 5.8 の円 x と y の境界ボックスをカバーする点の部分集合を作成しよう。\nある点は1つの円の中に入り、ある点は両方の円の中に入り、ある点はどちらの円の中にも入らない。\nst_sample() は、円 x と y の範囲内にある点を 単純なランダム化をして生成し、Figure 5.9 に示すような出力を得るために、以下のように使用する。ここで一つ問題がある。円 x と y 両方と交差する点の部分集合はどのように得られるだろうか？\nFIGURE 5.9: 円x、yを囲むバウンディングボックス内にランダムに分布する点。オブジェクトx、yの両方と交差する点がハイライトされる。\n以下のコードは、同じ結果を得るための3つの方法を示している。\n以下のコードチャンクの最初の行に示すように、x と y の交差 (前のコードチャンクでは x_and_y で表される) を直接部分集合オブジェクトとして使用することができる。\nまた、以下のコードチャンクの 2 行目で示すように、p で表される入力点と部分集合/クリッピングオブジェクト x_and_y との交点を求めることもできる。\nこの第二のアプローチは、x_and_y と部分的に交差するフィーチャを返すが、部分集合・オブジェクトの境界を越える空間的に広範囲なフィーチャについては、形状を修正したものを返すことになる。\n3 つ目のアプローチは、前の章で紹介した二項の空間述語 st_intersects() を使って部分集合オブジェクトを作成することである。\n結果は (属性名の表面的な違いを除いて) 同じだが、実装は大きく異なる。上記の例は、応用というより教育的な目的で作成されたものであり、R で地理的ベクタオブジェクトを扱うための理解を深めるために結果を再現することを勧める。しかし、これは、どの実装を使うべきかという重要な問題を提起している。\n一般に、上記の最初のアプローチのような簡潔な実装が好まれる。\nChapter 11 では、同じ技術やアルゴリズムの異なる実装を選択する問題に戻る。","code":"\nbb = st_bbox(st_union(x, y))\nbox = st_as_sfc(bb)\nset.seed(2024)\np = st_sample(x = box, size = 10)\nx_and_y = st_intersection(x, y)\n# way #1\np_xy1 = p[x_and_y]\n# way #2\np_xy2 = st_intersection(p, x_and_y)\n# way #3\nsel_p_xy = st_intersects(p, x, sparse = FALSE)[, 1] & \n  st_intersects(p, y, sparse = FALSE)[, 1]\np_xy3 = p[sel_p_xy]"},{"path":"geometry-operations.html","id":"geometry-unions","chapter":"5 ジオメトリ演算","heading":"5.2.7 ジオメトリ結合","text":"\nSection 3.2.3 で見たように、空間的な集約は、同グループの接しているポリゴンのジオメトリを静かにディゾルブ (dissolve) させることができる。\n以下のコードでは、Base と dplyr 関数を使って 48 の米国の州と District Columbia (us_states) を 4 つの地域に集約している (結果は Figure 5.10 を参照)。\nFIGURE 5.10: 連続したポリゴンに対する空間的な集計。アメリカの州の人口を地域に集計し、人口を色で表した。この操作により、州間の境界が自動的に解消されることに注意。\nジオメトリ的にはどうなっているのだろうか？\n裏側では、aggregate() と summarize() の両方がジオメトリを結合し、st_union() を使ってジオメトリ間の境界を解消している。\nこれは、アメリカ西部の連合体を作成する以下のコードチャンクで実証されている。この関数は 2 つの形状を受け取り、結合することができる。以下のコードでは、Texas を組み込んだ西側のブロックを結合している (課題: 結果を再現してプロットしなさい)。","code":"\nregions = aggregate(x = us_states[, \"total_pop_15\"], by = list(us_states$REGION),\n                    FUN = sum, na.rm = TRUE)\nregions2 = us_states |> \n  group_by(REGION) |>\n  summarize(pop = sum(total_pop_15, na.rm = TRUE))\nus_west = us_states[us_states$REGION == \"West\", ]\nus_west_union = st_union(us_west)\ntexas = us_states[us_states$NAME == \"Texas\", ]\ntexas_union = st_union(us_west_union, texas)"},{"path":"geometry-operations.html","id":"type-trans","chapter":"5 ジオメトリ演算","heading":"5.2.8 ジオメトリ型の変換","text":"\nジオメトリキャストは、ジオメトリ型の変換を可能にする強力な操作である。\nこれは、sf パッケージの st_cast() 関数で実装されている。\n重要なことは、st_cast() は、単一のシンプルフィーチャ (sfg) オブジェクト、シンプルフィーチャ列 (sfc)、およびシンプルフィーチャオブジェクトで異なる動作をすることである。ここでは、複合点を作成して、シンプルフィーチャ (sfg) オブジェクトに対するジオメトリ型のキャストの動作を説明する。この場合、st_cast() は新しいオブジェクトを線やポリゴン (Figure 5.11) に変換するのに便利である。\nFIGURE 5.11: 多点ジオメトリからキャストされた線とポリゴンの例。\n複合点から線への変換は、GPS 測定やジオタグ付きメディアなど、順序付けられたポイント観測から線オブジェクトを作成する一般的な操作である。\nこれにより、移動した経路の長さを計算するなどの空間演算を行うことができる。\n複合点や線からポリゴンへの変換は、例えば、湖の周囲で取得した GPS 測定値のセットや建物の敷地の角から面積を計算するためによく使われる。また、st_cast() では、逆の変換をすることも可能である。シンプルフィーチャのジオメトリ列 (sfc) とシンプルフィーチャオブジェクトのジオメトリキャストは、ほとんどの場合、sfg の場合と同じように動作する。\n重要な違いの 1 つは、複合タイプから非複合タイプへの変換である。\nこの処理の結果、sfc または sf の複合オブジェクトは、多数の非複合オブジェクトに分割される。以下のような sf オブジェクトを持っている。POI - 点 (定義上、一つの点)MPOI - 4 点からなる複合点LIN - 5 点からなる線MLIN - 二つの線の複合線 (5 つの点と 2 つの点)POL - ポリゴン (5 つの点)MPOL - 二つのポリゴンからなる複合ポリゴン (どちらも 5 つの点)GC - GEOMETRYCOLLECTION で複合点 (4 つの点) と線 (5 つの点)Table 5.1 は、シンプルフィーチャに対して可能なジオメトリタイプの変換を示したものである。\n単一のシンプルフィーチャ (表の最初の列で表される) は、Table 5.1 の列で表される複数のジオメトリタイプに変換することができる。\n例えば、1 つの点から複数行の文字列やポリゴンに変換することはできない。[1, 4:5] のセルに NA が含まれている理由を説明する。\n一部の変換では、単一のフィーチャ入力を複数のサブフィーチャに分割し、sf オブジェクトを「拡張」(重複する属性値を持つ新しい行を追加) している。\n例えば、5 組の座標からなる多点ジオメトリを「POINT」ジオメトリに変換すると、出力には 5 つのフィーチャが含まれる。TABLE 5.1: シンプルフィーチャジオメトリ (Section 2.1 参照) に対する 入力型は行、出力型は列で指定。Note:\n注: 括弧内の値はフィーチャ数を表し、NA は操作が不可能なことを表す。例として、新しいオブジェクトである multilinestring_sf にジオメトリタイプの変換を適用してみよう (Figure 5.12 の左側)。道路や河川のネットワークをイメージしていただきたい。\n新しいオブジェクトは、すべての線を定義する 1 つの行だけを持っている。\nこのため、各線分に名前を付けたり、1 本の線の長さを計算したりすることができないなど、実行できる操作に制限がある。\nこのような場合、1 つの複合線を 3 つの線に分離する st_cast() 関数を使用することができる。\nFIGURE 5.12: 複合線 (左) と線 (右) 間の型キャストの例。\n新しく作成されたオブジェクトでは、属性の作成 (詳しくは Section 3.2.5 を参照) や長さの測定が可能である。","code":"\nmultipoint = st_multipoint(matrix(c(1, 3, 5, 1, 3, 1), ncol = 2))\nlinestring = st_cast(multipoint, \"LINESTRING\")\npolyg = st_cast(multipoint, \"POLYGON\")\nmultipoint_2 = st_cast(linestring, \"MULTIPOINT\")\nmultipoint_3 = st_cast(polyg, \"MULTIPOINT\")\nall.equal(multipoint, multipoint_2)\n#> [1] TRUE\nall.equal(multipoint, multipoint_3)\n#> [1] TRUE\nmultilinestring_list = list(matrix(c(1, 4, 5, 3), ncol = 2), \n                            matrix(c(4, 4, 4, 1), ncol = 2),\n                            matrix(c(2, 4, 2, 2), ncol = 2))\nmultilinestring = st_multilinestring((multilinestring_list))\nmultilinestring_sf = st_sf(geom = st_sfc(multilinestring))\nmultilinestring_sf\n#> Simple feature collection with 1 feature and 0 fields\n#> Geometry type: MULTILINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#> CRS:           NA\n#>                             geom\n#> 1 MULTILINESTRING ((1 5, 4 3)...\nlinestring_sf2 = st_cast(multilinestring_sf, \"LINESTRING\")\nlinestring_sf2\n#> Simple feature collection with 3 features and 0 fields\n#> Geometry type: LINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#> CRS:           NA\n#>                    geom\n#> 1 LINESTRING (1 5, 4 3)\n#> 2 LINESTRING (4 4, 4 1)\n#> 3 LINESTRING (2 2, 4 2)\nlinestring_sf2$name = c(\"Riddle Rd\", \"Marshall Ave\", \"Foulke St\")\nlinestring_sf2$length = st_length(linestring_sf2)\nlinestring_sf2\n#> Simple feature collection with 3 features and 2 fields\n#> Geometry type: LINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#> CRS:           NA\n#>                    geom         name length\n#> 1 LINESTRING (1 5, 4 3)    Riddle Rd   3.61\n#> 2 LINESTRING (4 4, 4 1) Marshall Ave   3.00\n#> 3 LINESTRING (2 2, 4 2)    Foulke St   2.00"},{"path":"geometry-operations.html","id":"geo-ras","chapter":"5 ジオメトリ演算","heading":"5.3 ラスタデータに対するジオメトリ操作","text":"\n幾何学的なラスタ操作には、画像の平行移動 (shift)、反転 (flip)、ミラー (mirror)、拡大縮小 (scale)、回転 (rotate)、幾何補正 (warp) などがある。\nこれらの操作は、ジオリファレンスなど様々な用途で必要とされ、既知の CRS を持つ正確な地図に画像を重ね合わせるために使用される (Liu Mason 2009)。\nジオリファレンスには、さまざまな手法が存在する。既知の地上基準点に基づく地理補正 (georectification)局所的な地形も考慮したオルソ補正 (orthorectification)画像位置合わせ (image registration) は、異なるセンサーから撮影された同じものの画像を、ある画像と別の画像に (座標系や解像度の点で) 位置合わせして結合するために使用する。最初の 2 つについては、手作業が必要になることが多いため、R はむしろ不向きで、通常は専用の GIS ソフトウェアの助けを借りて行われる (Chapter 10 も参照)。\n一方、R では複数の画像の位置合わせが可能であり、本節ではその方法を中心に紹介する。\nこれには、画像の範囲、解像度、原点を変更することがよく含まれる。\nもちろん、投影法の一致も必要であるが、それはすでに Section 7.8 で説明した。いずれにせよ、1 枚のラスタ画像に対してジオメトリ演算を行う理由は他にもある。\n例えば、Chapter 14 で、ドイツの大都市圏を人口 50 万人以上の 20 km2 のピクセルと定義している。\nしかし、元の住民ラスタの解像度は 1 km2 であるため、解像度を 20 分の 1 に下げる (集約する) (Section 14.5 を参照)。\nラスタを集約するもう一つの理由は、単純に実行時間の短縮やディスクスペースの節約である。\nもちろん、この方法はラスタデータの粗い解像度が可能なタスクの場合にのみ推奨される。","code":""},{"path":"geometry-operations.html","id":"geometric-intersections","chapter":"5 ジオメトリ演算","heading":"5.3.1 ジオメトリ交差","text":"\nSection 4.3.1 では、他の空間オブジェクトが重なったラスタから値を抽出する方法を示した。\n空間出力を取り出すには、ほぼ同じ部分集合構文を使うことができる。\n唯一の違いは、drop の引数を FALSE にすることで、行列構造を維持したいことを明確にしなければならないことである。\nこれは、中点が clip と重なるセルを含むラスタオブジェクトを返すものである。同じ操作で、intersect() と crop() のコマンドも使用できる。","code":"\nelev = rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nclip = rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            resolution = 0.3, vals = rep(1, 9))\nelev[clip, drop = FALSE]"},{"path":"geometry-operations.html","id":"extent-and-origin","chapter":"5 ジオメトリ演算","heading":"5.3.2 範囲と原点","text":"\nラスタをマージしたり、マップ代数演算を実行する場合、解像度、投影、原点、範囲が一致する必要がある。そうでない場合、分解能が 0.2 度のラスタの値を、分解能が 1 度の 2 番目のラスタにどのように追加すればよいのだろうか。\nまた、投影方法や解像度の異なるセンサーの衛星画像を合成したい場合も、同じような問題が発生する。\nこのような不一致は、ラスタの位置合わせをすることで対処することができる。最も単純なケースとして、2 つの画像はその範囲 (extent) だけが異なる。\n以下のコードでは、ラスタの各辺に 1 行と 2 列を追加し、新しい値をすべて NA (Figure 5.13) に設定する。\nFIGURE 5.13: 元のラスタ (左) と同じラスタ (右) を上下 1 行ずつ、左右 2 列ずつに拡張したもの。\nR の terra パッケージで、範囲が異なる2つのオブジェクトに対して代数演算を実行するとエラーが発生する。しかし、2 つのラスタの範囲を extend() で揃えることができる。\n追加する行や列の数を (以前のように) 関数に指示するのではなく、別のラスタオブジェクトを使って計算させるようにしている。\nここでは、elev のオブジェクトを、elev_2 の範囲に拡張している。\n新しく追加された行と列は、NA を受け取る。\nラスタの原点 (origin) は、座標 (0, 0) に最も近いセルコーナーである。\norigin() 関数は、原点の座標を返す。\n以下の例では、座標 (0, 0) のセルコーナーが存在するが、必ずしもそうとは限らない。2 つのラスタが異なる原点を持つ場合、それらのセルは完全に重ならず、マップ代数は不可能となる。\n原点を変更する場合 origin() を使用する。24\nFigure 5.14 は、このように原点を変更した場合の効果を明らかにしたものである。\nFIGURE 5.14: 同じ値を持つが、原点が異なるラスタ。\nなお、解像度を変更すると (次節)、原点も頻繁に変更される。","code":"\nelev = rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_2 = extend(elev, c(1, 2))\nelev_3 = elev + elev_2\n#> Error: [+] extents do not match\nelev_4 = extend(elev, elev_2)\norigin(elev_4)\n#> [1] 0 0\n# 原点を変更\norigin(elev_4) = c(0.25, 0.25)"},{"path":"geometry-operations.html","id":"aggregation-and-disaggregation","chapter":"5 ジオメトリ演算","heading":"5.3.3 低解像度化と高解像度化","text":"\nラスタデータセットは、その解像度に関しても異なる場合がある。\n解像度を合わせるには、1　つのラスタの解像度を下げる (低解像度化、aggregate()) か上げる (高解像度化、disagg()) かのどちらかを選択する。25例として、dem (spDataLarge パッケージにある) の空間解像度を 5 倍に変更する (Figure 5.15)。\nさらに、出力セルの値は入力セルの平均に対応する (median()、sum() など、他の関数も使用可能)。\nFIGURE 5.15: オリジナル (左) と 集計後 (右)。\nTable 5.2 は、オリジナルと集計後の比較である。\naggregate() で解像度を「下げた」ことにより、\\((30.85, 30.85)\\) から \\((154.25, 154.25)\\) に上がった。\n列数 (nrow) と行数 (ncol) を減らすことで行った (Section 2.3 参照)。\n範囲は、新しいグリッドに合うように微調整された。TABLE 5.2: Properties original aggregated raster.\ndisagg() 関数は、ラスタオブジェクトの解像度を上げ、新しく作成されたセルに値を割り当てるための 2 つの方法を提供する。\n新たに生成されたセルの値を計算する方法は 2 つある。デフォルトの方法 (method = \"near\") は、すべての出力セルに入力セルの値を与えるだけなので、値が重複し、「ブロック状の」出力となる。\nbilinear 法は、入力画像の 4 つの最近接画素中心 (Figure 5.16 のオレンジ色の点) を用いて、距離で重み付けした平均値を計算する (Figure 5.16 の矢印)。\n出力セルの値は、左上の四角で表現される (Figure 5.16)。\nFIGURE 5.16: 双一次補間法で分解する場合は、最も近い 4 つの入力セルの距離加重平均で出力を決定。\ndem と dem_disagg の値を比較すると、両者が同一ではないことがわかる (compareGeom() や .equal() を使うこともできる)。\n分解は単純な補間技術であるため、これは予想外の結果である。\n分解することで解像度が上がるが、対応する値は解像度の低いソースと同程度の精度しかないことを念頭に置くことが重要である。","code":"\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndem_agg = aggregate(dem, fact = 5, fun = mean)#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\ndem_disagg = disagg(dem_agg, fact = 5, method = \"bilinear\")\nidentical(dem, dem_disagg)\n#> [1] FALSE"},{"path":"geometry-operations.html","id":"resampling","chapter":"5 ジオメトリ演算","heading":"5.3.4 リサンプリング","text":"\n上記の低解像度化・高解像度化の方法は、低解像度化・高解像度化係数によってラスタの解像度を変えたい場合にのみ適している。\nしかし、解像度や原点の異なるラスタが 2 つ以上ある場合はどうしたらよいのだろうか。\nこれがリサンプリングの役割で、新しい画素の位置に対して値を計算する処理である。\nつまり、この処理では、元のラスタの値を受け取り、カスタム解像度と原点を持つターゲットラスタの値を新たに計算し直す (Figure 5.17)。\nFIGURE 5.17: オリジナルからカスタムの解像度と原点のターゲットラスタへのリサンプリング\n\nFigure 5.18 に示すように、解像度/原点の異なるラスタの値を推定する方法はいくつかある。\n主なリサンプリング方法には、以下のようなものがある。最近傍 (Nearest neighbor): 元のラスタの最も近いセルの値を、ターゲットのセルに割り当てる。これは高速でシンプルな手法で、通常、カテゴリラスタのリサンプリングに適している。双一次補間 (Bilinear interpolation): 元のラスタから 4 つの最近接セルを加重平均して、ターゲットのセルに割り当てる (Figure 5.16)。これは、連続したラスタに適した最も高速な方法である。三次補間 (Cubic interpolation): 3 次多項式関数を適用し、出力セルの値を決定するために、元のラスタの16個の最近接セルの値を使用する。連続したラスタに使用され、双一次補間の結果より滑らかなサーフェスになるが、より計算量が多くなる。三次スプライン補間 (Cubic spline interpolation): 出力セルを決定するために元のラスタの 16 個の最近接セルの値を使用するが、結果を導き出すために三次スプライン補間 (区分的 3 次多項式関数) を適用する。連続したラスタに使用される。ランチョスリサンプリング (Lanczos windowed sinc resampling): 出力セルの値を決定するために、元のラスタの 36 個の最近接セルの値を使用する。連続的なラスタに使用される。26上記の説明では、最近傍リサンプリングのみがカテゴリラスタに適しており、連続ラスタにはすべてのメソッドが (異なるアウトカムで) 使用できることが強調されている。\nさらに、連続した方式を採用するごとに、処理時間が長くなる。\nさらに、リサンプリングは関連するセルの統計値 (最小値や最頻値など) を用いることもできる、リサンプリングを適用するために、terra パッケージは、resample() 関数を提供する。\n入力ラスタ (x)、目的の空間特性を持つラスタ (y)、リサンプリング方法 (method) を受け付ける。resample() 関数の動作を確認するために、対象となる空間特性を持つラスタが必要である。\nこの例では、target_rast を作成するが、すでに存在するラスタオブジェクトを使用することが多いだろう。次に、2 つのラスタオブジェクトを最初の 2 つの引数として与え、上で説明したリサンプリングメソッドのうちの 1 つを提供する必要がある。Figure 5.18 は、dem オブジェクトについて、異なるリサンプリング手法を比較したものである。\nFIGURE 5.18: ラスタのオリジナルと 5 種類のリサンプリング手法の視覚的比較。\nresample() 関数には、さらに、sum、min、q1、med、q3、max、average、mode、rms などのリサンプリング手法も用意されている。\nこれらはすべて、NA でないすべてのグリッドセルの値に基づいて所定の統計量を計算するものである。\n例えば、sum は、各ラスタセルが空間的に広範な変数 (例えば人数) を表す場合に有効である。\nsum を使用した効果として、リサンプルされたラスタは元のラスタと同じサンプル総数を持つはずである。\nSection 7.8 で見るように、ラスタの再投影はリサンプリングの特殊なケースで、対象ラスタが元のラスタと異なる CRS を持つ場合に行われる。terra のジオメトリ操作はユーザーフレンドリーで、かなり高速であり、大きなラスタオブジェクトでも動作する。\nしかし、広範なラスタや多くのラスタファイルに対して、terra が最も効率的でない場合もあり、代替手段を検討する必要がある。最も確立された選択肢は、GDAL ライブラリである。\nこれは、以下のようないくつかのユーティリティ関数を含んでる。gdalinfo - ラスタファイルの解像度、CRS、バウンディングボックスなど、さまざまな情報を一覧表示gdal_translate - ラスタデータを異なるファイル形式間で変換gdal_rasterize - ベクタデータからラスタファイルへの変換gdalwarp - ラスタのモザイク処理、リサンプリング、切り取り、再投影が可能","code":"\ntarget_rast = rast(xmin = 794650, xmax = 798250, \n                   ymin = 8931750, ymax = 8935350,\n                   resolution = 300, crs = \"EPSG:32717\")\ndem_resampl = resample(dem, y = target_rast, method = \"bilinear\")#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily"},{"path":"geometry-operations.html","id":"演習-3","chapter":"5 ジオメトリ演算","heading":"5.4 演習","text":"E1. nz データセットの簡略版を生成してプロットしなさい。\nms_simplify() の keep (0.5 から 0.00005 の範囲) と st_simplify() の dTolerance (100 から 100,000) の値を変えて実験しなさい。各メソッドで結果の形が崩れ始め、New Zealand を認識できなくなるのはどの値からか？発展: st_simplify() の結果のジオメトリ型は、ms_simplify() のジオメトリ型と何が違うか？また、どのように解決できるか？E2. 空間データ操作の章の最初の演習で、Canterbury 地方には New Zealand の 101 の高地のうち 70 地点があることがわかった。\nst_buffer()を使用して、Canterbury から 100 km 以内にある nz_height の点はいくつあるか？E3. New Zealand の地理的重心を求めなさい。\nCanterburyの地理的重心からの距離は？E4. ほとんどの世界地図は北を向いている。\nオブジェクト world のジオメトリの反射 (この章では触れないアフィン変換の 1 つ) によって、南を上にしたワールドマップを作ることができる。\nそのコードを書きなさい。\nヒント: この変換には、本章の rotation() 関数を使うことができる。\nボーナス: あなたの国の逆さ地図を作ってみなさい。E5. Section 5.2.6 のコードを実行しなさい。そのセクションで作成したオブジェクトを参照して、x と y に含まれる p の点の部分集合を作成しなさい。基本サブセット演算子を使用する。st_intersection()で作成した中間オブジェクトを使用する。E6. アメリカの州の境界線の長さをメートル単位で計算しなさい。\nどの州の境界線が最も長く、どの州の境界線が最も短いか。\nヒント: st_length 関数は LINESTRING または MULTILINESTRING 形状の長さを計算する。E7. srtm.tif ファイルを R で読み込みなさい (srtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\")))。\nこのラスタの解像度は 0.00083 * 0.00083度。\nterra パッケージで利用可能なすべてのメソッドを使用して、その解像度を 0.01 * 0.01度に変更しなさい。\n結果を視覚化しなさい。\nリサンプリング方法の結果の違いは何があるか?","code":""},{"path":"raster-vector.html","id":"raster-vector","chapter":"6 ラスタとベクタの相互作用","heading":"6 ラスタとベクタの相互作用","text":"","code":""},{"path":"raster-vector.html","id":"prerequisites-06","chapter":"6 ラスタとベクタの相互作用","heading":"必須パッケージ","text":"この章では、以下のパッケージが必要である。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)"},{"path":"raster-vector.html","id":"introduction-06","chapter":"6 ラスタとベクタの相互作用","heading":"6.1 イントロダクション","text":"\nこの章では、Chapter 2 で紹介したラスタとベクタの地理データモデル間の相互作用に焦点を当てる。\n主要な技法をいくつか紹介する。\n最初は、ベクタオブジェクトを使用したラスタの切り落とし (crop) とマスク (mask) から始める (Section 6.2)。\n次に、さまざまな種類のベクタデータを使ってラスタ値を抽出する (Section 6.3)。\n最後は、ラスタベクタ変換である (Section 6.4 と Section 6.5)。\n以上の概念を、実世界でどのように応用できるかを理解するため、これまでの章で使用したデータを用いて実際に操作していく。","code":""},{"path":"raster-vector.html","id":"raster-cropping","chapter":"6 ラスタとベクタの相互作用","heading":"6.2 ラスタの切り落とし (crop)","text":"\n多くの地理データプロジェクトでは、リモートセンシング画像 (ラスタ) や行政境界線 (ベクタ) など、さまざまなソースからのデータを統合している。\n入力されたラスタデータセットの範囲は、対象地域よりも大きいことがよくある。\nこの場合、入力データの空間的な広がりを統一するために、ラスタの切り落とし (crop) やマスク (mask) が有効である。\nこの 2 つの処理により、オブジェクトのメモリ使用量と、その後の解析に必要な計算資源を削減できる。ラスタデータを含む魅力的な地図を作成する際に必要な前処理工程となる場合がある。ここでは、2 つのオブジェクトを使ってラスタ切り落としを説明する。SpatRaster のオブジェクト srtm は、米国 Utah 州南西部の標高 (海抜メートル) を表す。ベクタ (sf) オブジェクト zion は、Zion 国立公園 (Zion National Park) を表す。ターゲットと切り取りオブジェクトの両方が同じ投影である必要がある。\nしたがって、以下のコードは Chapter 2 でインストールされた spDataLarge パッケージからデータセットを読み込むだけでなく、zion を「再投影」している (この話題は Chapter 7 で取り上げている)。srtm のラスタを切り出すために terra パッケージの crop() を使用する。\nこの関数は、第 1 引数に渡されたオブジェクトの矩形範囲を、第 2 引数に渡されたオブジェクトの範囲に縮小する。\nつまり、以下のコマンドで Figure 6.1 (B) を生成する。\ncrop() に関連するものとして、terra 関数 mask() がある。これは第 2 引数に渡されたオブジェクトの境界外の値を NA に設定するものである。\nしたがって、次のコマンドは、Zion 国立公園の境界の外側のすべてのセルをマスクする (Figure 6.1 (C))。crop() と mask() は、一緒に使うことが多い。\n() ラスタの範囲を目的の領域に限定し、(b) 領域外の値をすべて NA に置き換える。27mask() の設定を変更すると、異なる結果が得られる。\ninverse = TRUE を設定すると、公園の境界の内側をすべてマスクする (詳細は ?mask を参照) (Figure 6.1 (D))。また、updatevalue = 0 を設定すると、国立公園外のすべてのピクセルが 0 に設定される。\nFIGURE 6.1: ラスタクロップ、ラスタマスク。\n","code":"\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, st_crs(srtm))\nsrtm_cropped = crop(srtm, zion)\nsrtm_masked = mask(srtm, zion)\nsrtm_cropped = crop(srtm, zion)\nsrtm_final = mask(srtm_cropped, zion)\nsrtm_inv_masked = mask(srtm, zion, inverse = TRUE)"},{"path":"raster-vector.html","id":"raster-extraction","chapter":"6 ラスタとベクタの相互作用","heading":"6.3 ラスタ抽出","text":"\nラスタ抽出は、地理的 (通常はベクタ) な「範囲選択」オブジェクトに基づいて、特定の位置の「ターゲット」ラスタに関連する値を識別して返す処理である。\n結果は、使用する範囲選択の種類 (点、線、ポリゴン) と、terra::extract() 関数に渡される引数に依存する。\nラスタ抽出の逆、つまりベクタオブジェクトに基づいてラスタセル値を割り当てるのがラスタ化で、Section 6.4 で説明する。\n基本的な例として、ラスタセルの特定の点の値を抽出してみよう。\nそのために、Zion 国立公園内の 30 カ所のサンプルを収録した zion_points を使用する (Figure 6.2)。\n次のコマンドは、srtm から標高値を抽出し、各ポイントの ID (ベクタの行ごとに 1 つの値) と関連する srtm の値を含むデータフレームを作成する。\nさて、出来上がったオブジェクトを cbind() 関数で zion_points データセットに追加してみよう。\nFIGURE 6.2: ラスタ抽出に使用した点の位置。\n\nラスタ抽出は、線範囲選択でも機能する。\nそして、線に接するラスタセルごとに 1 つの値を抽出する。\nしかし、抽出されたラスタ値の各ペア間の距離を正しく取得することが難しいため、線の断片に沿った値を得るための線抽出アプローチは推奨されていない。この場合、線を多くの点に分割し、その点の値を抽出するのが良い方法である。\nこれを示すために、以下のコードでは、Figure 6.3 () に示した Zion 国立公園の北西から南東に向かう直線、zion_transect を作成する (ベクタデータモデルについての復習は Section 2.2 を参照)。線の範囲選択から高さを抽出することの有用性は、ハイキングの計画を立てることを想像してみるとよくわかる。\n以下に示す方法は、ルートの「標高プロファイル」を提供し (線は直線である必要はない)、長い上り坂による所要時間を見積もるのに便利である。まず、各断片に固有の id を追加する。\n次に、st_segmentize() 関数を使って、与えられた密度 (dfMaxLength) で線に沿って点を追加し、st_cast() で点に変換することができる。これで大きな点の集合ができたので、断片の最初の点と、それ以降の各点との距離を導き出してみたい。\nこのケースでは、1 つの断片しかないが、原理的には、このコードはいくつの断片でも動作するはずである。最後に、断片の各ポイントの標高値を抽出し、この情報をメインオブジェクトに結合する。その結果、zion_transect、Figure 6.3 (B) に示すように、標高プロファイルを作成することができる。\nFIGURE 6.3: ラスタ抽出に使用した線の位置 () と、その線に沿った標高 (B)。\n\nラスタ抽出のための地理ベクタオブジェクトの最後のタイプは、ポリゴンである。\n線と同様に、ポリゴンも 1 ポリゴンあたり多くのラスタ値を返す傾向がある。\nこれは以下のコマンドで示され、ID (ポリゴンの行番号) と srtm (関連する標高値) の列名を持つデータフレームが生成される。このような結果を利用して、ポリゴンごとのラスタ値の要約統計量を生成することで、例えば、単一の地域を特徴付けることや、多くの地域を比較することができる。\nこれは以下のコードで示されている。このコードは、Zion 国立公園の標高値の要約統計を含むオブジェクト zion_srtm_df を作成する (Figure 6.4 () を参照)。上のコードチャンクは、Chapter 3 で説明されているように、ポリゴン ID ごとのセル値の要約統計を提供するために dplyr を使用した。\nその結果、例えば公園の最高標高が海抜約 2,661 m であることなど、有用な要約が得られる (標準偏差など、他の要約統計もこの方法で計算できる)。\nこの例ではポリゴンが 1 つしかないので 1 行のデータフレームだが、複数の範囲選択ポリゴンが使用されている場合にも動作する。同様のアプローチは、ポリゴン内のカテゴリ的なラスタ値の出現をカウントする場合にも有効である。\nこれは、Figure 6.4 (B) の spDataLarge パッケージの土地被覆データセット (nlcd) を使って説明され、以下のコードで実証されている。\nFIGURE 6.4: () 連続 と (B) カテゴリのラスタ抽出に使用した範囲。\n\nterra パッケージはポリゴン内のラスタ値を高速に抽出するが、extract() は大きなポリゴンデータセットを処理する際のボトルネックになることがある。\nexactextractr パッケージは、exact_extract() 関数を通してピクセル値を抽出するための 高速の代替手段を提供する。\nまた、exact_extract() 関数は、デフォルトで、ポリゴンによってオーバーラップされた各ラスタセルの割合を計算し、より正確である (詳細については、以下の注を参照)。","code":"\ndata(\"zion_points\", package = \"spDataLarge\")\nelevation = terra::extract(srtm, zion_points)\nzion_points = cbind(zion_points, elevation)\nzion_transect = cbind(c(-113.2, -112.9), c(37.45, 37.2)) |>\n  st_linestring() |> \n  st_sfc(crs = crs(srtm)) |>\n  st_sf(geometry = _)\nzion_transect$id = 1:nrow(zion_transect)\nzion_transect = st_segmentize(zion_transect, dfMaxLength = 250)\nzion_transect = st_cast(zion_transect, \"POINT\")\nzion_transect = zion_transect |> \n  group_by(id) |> \n  mutate(dist = st_distance(geometry)[, 1]) \nzion_elev = terra::extract(srtm, zion_transect)\nzion_transect = cbind(zion_transect, zion_elev)\nzion_srtm_values = terra::extract(x = srtm, y = zion)\ngroup_by(zion_srtm_values, ID) |> \n  summarize(across(srtm, list(min = min, mean = mean, max = max)))\n#> # A tibble: 1 × 4\n#>      ID srtm_min srtm_mean srtm_max\n#>   <dbl>    <int>     <dbl>    <int>\n#> 1     1     1122     1818.     2661\nnlcd = rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\nzion2 = st_transform(zion, st_crs(nlcd))\nzion_nlcd = terra::extract(nlcd, zion2)\nzion_nlcd |> \n  group_by(ID, levels) |>\n  count()\n#> # A tibble: 7 × 3\n#> # Groups:   ID, levels [7]\n#>      ID levels         n\n#>   <dbl> <fct>      <int>\n#> 1     1 Developed   4205\n#> 2     1 Barren     98285\n#> 3     1 Forest    298299\n#> 4     1 Shrubland 203701\n#> # ℹ 3 more rows"},{"path":"raster-vector.html","id":"rasterization","chapter":"6 ラスタとベクタの相互作用","heading":"6.4 ラスタ化","text":"\nラスタ化とは、ベクタオブジェクトをラスタオブジェクトに変換して表現することである。\n通常、出力されたラスタは定量的な解析 (地形の解析など) やモデリングに利用される。\nChapter 2 で見たように、手法によってはラスタデータモデルの方が適していることがある。\nさらに、ラスタ化は地理的なデータ集計の一種と考えることができ、結果として得られる値はすべて同じ空間分解能を持つため、データセットを簡素化することができる。terra パッケージには、この作業を行うための関数 rasterize() が含まれている。\nその最初の 2 つの引数は、ラスタ化されるベクタオブジェクト x とテンプレートラスタ yである。後者は、出力の範囲、解像度、CRS を定義するラスタである。\n入力ラスタの地理的解像度が低すぎる (セルサイズが大きすぎる) と、ベクタデータの地理的変動を完全に見逃す可能性があり、高すぎる場合は計算時間がかかりすぎる可能性がある。\n適切な地理的解像度を決定する際に従うべき単純なルールはなく、結果の使用目的によって大きく左右される。\n例えば、ラスタ化の出力を他の既存ラスタに合わせる必要がある場合など、ターゲット解像度がユーザーに課されることがよくある。\nラスタ化を実演するために、入力ベクタデータ cycle_hire_osm_projected (ロンドンの自転車レンタルポイントに関するデータセットを Figure 6.5 () に図示) と同じ範囲と CRS、空間解像度 1000 メートルのテンプレートラスタを使用することにする。ラスタ化は非常に柔軟な操作で、結果はテンプレートとなるラスタの性質だけでなく、入力ベクタの種類 (点、ポリゴンなど) や、rasterize() 関数が取るさまざまな引数に依存する。この柔軟性を説明するために、3 つの異なるアプローチでラスタ化を試みる。\nまず、レンタルサイクルの有無を表すラスタ (有無ラスタと呼ぶ) を作成する。\nこの場合、rasterize() は、x と y (前述のベクタとラスタのオブジェクト) のみを要求する (図示の結果 Figure 6.5 (B))。fun 引数は、近接した複数の観測値をラスタオブジェクトの関連セルに変換するために使用される要約統計量を指定する。\nデフォルトでは、fun = \"last\" が使用されるが、fun = \"length\" などの他のオプションも使用できる。この場合、各グリッドセル内のレンタルサイクルのステーション数をカウントする (この操作の結果は、Figure 6.5 (C) に示す)。新しい出力である ch_raster2 は、各グリッドセル内のレンタルサイクルのステーション数を示している。\nレンタルサイクルのステーションは、capacity 変数で記述される自転車の数が異なるため、各グリッドセルの収容台数 (capacity) はどの程度なのかという疑問が生じる。\nこれを計算するためには、フィールド (\"capacity\") を sum することが必要で、その結果、Figure 6.5 (D) のような出力が得られる。以下のコマンドで計算する (mean など他の要約関数も使用できる)。\nFIGURE 6.5: 点のラスタ化例。\n\nまた、カリフォルニア州のポリゴンと境界線をベースにしたデータセット (下記作成) は、線のラスタ化を表している。\nポリゴンオブジェクトを複合線にキャストした後、0.5 度の分解能を持つテンプレートラスタを作成する。線またはポリゴンのラスタ化を考慮する場合、有用な追加引数の 1 つは touches である。\nデフォルトでは FALSE であるが、TRUE に変更すると、線またはポリゴンの境界で接触しているすべてのセルが値を得る。\ntouches = TRUE による線のラスタ化を以下のコードで実行する (Figure 6.6 ())。ポリゴンのラスタ化と比較すると、touches = FALSE がデフォルトで、Figure 6.6 (B) に示すように、範囲選択ポリゴン内に中心点があるラスタセルのみが選択されることになる。\nFIGURE 6.6: 線とポリゴンのラスタ化の例。\n","code":"\ncycle_hire_osm = spData::cycle_hire_osm\ncycle_hire_osm_projected = st_transform(cycle_hire_osm, \"EPSG:27700\")\nraster_template = rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                       crs = crs(cycle_hire_osm_projected))\nch_raster1 = rasterize(cycle_hire_osm_projected, raster_template)\nch_raster2 = rasterize(cycle_hire_osm_projected, raster_template, \n                       fun = \"length\")\nch_raster3 = rasterize(vect(cycle_hire_osm_projected), raster_template, \n                       field = \"capacity\", fun = sum, na.rm = TRUE)\ncalifornia = dplyr::filter(us_states, NAME == \"California\")\ncalifornia_borders = st_cast(california, \"MULTILINESTRING\")\nraster_template2 = rast(ext(california), resolution = 0.5,\n                        crs = st_crs(california)$wkt)\ncalifornia_raster1 = rasterize(california_borders, raster_template2,\n                               touches = TRUE)\ncalifornia_raster2 = rasterize(california, raster_template2) "},{"path":"raster-vector.html","id":"spatial-vectorization","chapter":"6 ラスタとベクタの相互作用","heading":"6.5 空間ベクタ化","text":"\n空間ベクタ化は、ラスタ化 (Section 6.4) と対で、方向が逆になる。\n空間的に連続したラスタデータを、点、線、ポリゴンなどの空間的に離散したベクタデータに変換する。\n最も単純なベクタ化は、ラスタセルの中心を点に変換することである。\n.points() は、NA 以外のすべてのラスタグリッドセルに対して実行する (Figure 6.7)。\nなお、ここでは、st_as_sf() を使って、結果のオブジェクトを sf クラスに変換することもしている。\nFIGURE 6.7: elev オブジェクトのラスタ表現と点表現。\n\n空間ベクタ化のもう一つの一般的なタイプは、例えば連続した高さや温度の線 (等温線) を表す等高線の作成である。\nここでは、実世界のデジタル標高モデル (DEM) を使用する。というのも、人工のラスタ elev は平行線を生成するためである (読者への課題: これを検証し、なぜこうなるのかを説明しなさい)。\n等高線は terra 関数 .contour() で作成することができる。この関数は、R に元々ある filled.contour() のラッパーである (図示せず)。contour() や rasterVis::contourplot() などの関数で、既存のプロットに等高線を追加することもできる。\n\nFigure 6.8 に示すように、等値線 (isoline) にはラベルを付けることができる。\nFIGURE 6.8: モンゴル山南麓の等高線を重ねた陰影付きデジタル標高モデル。\n\nベクタ化の最後のタイプとして、ラスタをポリゴンに変換しよう。\nこれには、terra::.polygons() を使う。各ラスタセルを 5 つの座標からなるポリゴンに変換し、そのすべてがメモリに保存される (ラスタがベクタと比較して高速であることが多い理由が分かる!)。以下では、grain オブジェクトをポリゴンに変換し、その後、同じ属性値を持つポリゴン間の境界を解消することで説明している (.polygons() の dissolve の引数も参照)。\nFIGURE 6.9: ラスタ ()のベクタ化 (B) ポリゴン (dissolve = FALSE; 中央) と融合ポリゴン (dissolve = TRUE)。\ngrain データセットのポリゴンは、長方形のピクセルをつなぐことで定義される直方体の境界を持つ。\nポリゴンを滑らか (smooth) にするために、Chapter 5 のパッケージ smoothr を使用することができる。\n平滑化処理はポリゴン境界の鋭いエッジを除去するため、平滑化ポリゴンは元のピクセルとは空間的範囲が正確に同じにはならない。\nそのため、平滑化されたポリゴンをさらに解析に使用する場合は注意が必要となる。","code":"\nelev = rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_point = as.points(elev) |> \n  st_as_sf()\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem) |> \n  st_as_sf()\nplot(dem, axes = FALSE)\nplot(cl, add = TRUE)\ngrain = rast(system.file(\"raster/grain.tif\", package = \"spData\"))\ngrain_poly = as.polygons(grain) |> \n  st_as_sf()"},{"path":"raster-vector.html","id":"演習-4","chapter":"6 ラスタとベクタの相互作用","heading":"6.6 演習","text":"以下の演習では、spDataLarge パッケージのベクタデータ (zion_points) とラスタデータ (srtm) を使うことがある。\nまず、ベクタデータ (ch) から、ポリゴンの「凸多面体」で領域を示す。E1. srtm ラスタを、 (1) zion_pointsと (2) ch のデータセットを使い切り落とし (crop) なさい。\n作成した地図に違いはあるか？\n次に、同じデータセットを使い srtm をマスクしなさい。\n何か違いはあるか?\nその違いを説明できるか?E2. まず、zion_points で表される点の srtm から値を抽出しなさい。\n次に、zion_points の各点を 90 個のバッファで囲んで srtm の平均値を抽出し、この 2 つの値を比較しなさい。\nバッファによる値の抽出は、点のみによる抽出よりもどのような場合に適しているだろうか?ボーナス: exactextractrパッケージを使用して抽出を実行し、結果を比較しなさい。E3. New Zealand の標高 3100 m 以上のポイント (nz_heightオブジェクト) の部分集合を作成し、新しい点データセットの範囲に対して解像度 3 km のテンプレートラスタを作成しなさい。\nこれら 2 つの新しいオブジェクトを使い、各グリッドセルで最も標高の高い点の数を数えなさい。各グリッドセル内の最大標高を求めなさい。E4. New Zealand の高地の位置を数えるラスタ (前のエクササイズで作成) を集約し、その地理的解像度を半分に下げ (セルが 6 * 6 km になるように)、結果をプロットしなさい。低解像度のラスタを元の解像度 3 km に再サンプルしなさい。結果はどう変わったか?ラスタの解像度を下げることの利点と欠点を 2 つ挙げなさい。E5. grain データセットをポリゴンにして、土を表している正方形をフィルタしなさい。ベクタデータがラスタデータよりも良い点と悪い点を挙げなさい。ラスタをベクタに変換すると良いのはどのような時か？","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nzion_points_path = system.file(\"vector/zion_points.gpkg\", package = \"spDataLarge\")\nzion_points = read_sf(zion_points_path)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nch = st_combine(zion_points) |>\n  st_convex_hull() |> \n  st_as_sf()"},{"path":"reproj-geo-data.html","id":"reproj-geo-data","chapter":"7 地理データの再投影","heading":"7 地理データの再投影","text":"","code":""},{"path":"reproj-geo-data.html","id":"prerequisites-07","chapter":"7 地理データの再投影","heading":"必須パッケージ","text":"この章では、以下のパッケージが必要である。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)"},{"path":"reproj-geo-data.html","id":"reproj-intro","chapter":"7 地理データの再投影","heading":"7.1 イントロダクション","text":"Section 2.4 では、座標参照系 (CRS) を紹介し、地理座標系 (‘lon/lat’、単位は経度と緯度) と投影座標系 (通常は基準点からのメートル) という 2 つの主要なタイプに焦点を当てた。\nこの章では、その知識をもとに、さらに踏み込んだ内容になっている。\n具体的には、ある CRS から別の CRS に地理データを設定し、変換する方法を説明する。特にデータが緯度経度座標の場合、CRS を無視することによって発生し得る問題があるので、これを明らかにして注意を促したい。\n多くのプロジェクトでは、CRS について心配する必要はないし、変換について考える必要もない。\nしかし、データが投影座標参照系なのか地理座標参照系なのか、そしてそれがジオメトリ操作に与える影響を知ることは重要である。\nこの情報を知っていれば、CRS は裏でただうまく機能してくれる。しかし、うまくいかないことがあったとき、CRS に原因があるかもしれず、突然 CRS について学ぶ必要が出てくるのである。\nすべての投影データが入る CRS を明確に定義し、異なる CRS をどのように、そしてなぜ使うのかを理解することで、物事がうまくいくことを確実にすることができる。\nさらに、座標系について学ぶことで、地理データセットとその効果的な使用方法についての知識を深めることができる。この章では、CRS の基本を学び、異なる CRS を使用した場合の結果 (何が問題になるかを含む) を示し、ある座標系から別の座標系にデータセットを「再投影」する方法について説明する。\n次のセクションでは R における CRS を紹介し、続いて Section 7.3 で空間オブジェクトに関連する CRS の取得と設定方法を示す。\nSection 7.4 は、バッファを作成する作業例を参照しながら、データがどの CRS にあるのかを知ることの重要性を示している。\nSection 7.5 と Section 7.6 において、それぞれ、いつ再投影するべきか、どの CRS を使うかという問題に取り組んでいる。\n最後に、ベクタとラスタの再投影については、Section 7.7 と Section 7.8 で、地図投影の修正については Section 7.9 で説明する。","code":""},{"path":"reproj-geo-data.html","id":"crs-in-r","chapter":"7 地理データの再投影","heading":"7.2 座標参照系","text":"\nR-spatial のコアパッケージや QGIS などのデスクトップ GIS ソフトウェアなど、CRS 変換を必要とする最新の地理ツールのほとんどは、「ある座標参照系 (CRS) から別の座標に変換する」オープンソース C++ ライブラリ PROJ とつながっている。\nCRS には、以下のような様々な表現方法がある。単純だが曖昧になる可能性のある記述、例えば「lon/lat座標で表示される」+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs のような、形式化された、しかし今では時代遅れの「proj4 文字列」(または proj-string)EPSG:4326 のような識別用の ‘authority:code’ テキスト文字列上の例はそれぞれ、全地球測位システム (Global Positioning System, GPS) 座標やその他多くのデータセットの基礎となる「WGS84」座標系という同じものを指している。(訳注: WGS84 は、世界で一般的に用いられている緯度経度座標系だが、日本の場合は、JGD2011 が用いられることが多い。そのコードは EPSG:6668 である。なお、JGD2011 は東日本大震災の影響を考慮しており、震災以前は JGD2000 が用いられていた。そのコードは EPSG:4612 である。データが作成された年によって両者を使い分けるとよい。詳細については、空間情報クラブなどを参照。)\nしかし、どれが正しいのだろうか？\n一言でいうと、第三の CRS 識別方法が正しい。EPSG:4326 は、本書で取り上げる sf と terra (さらに stars) パッケージ、及び QGIS や PROJ など地理データを扱う多くのソフトウェアプロジェクトが理解できる。\nEPSG:4326 は将来を見据えたものである。\nさらに、「EPSG:4326」は、機械可読でありながら、短く覚えやすく、オンラインで非常に「見つけやすい」 (例えば、EPSG:4326 を検索すると、ウェブサイト epsg.io の専用ページが表示される)。\nより簡潔に 4326 だけでも sf によって理解されるが、 曖昧さを防ぎ、文脈を提供するために、より明示的な AUTHORITY:CODE という形式を推奨する。\nより長く答えると、3 つの記述のどれも十分ではなく、CRS の処理と変換を明確にするためには、より詳細な情報が必要になる。\nこのため、Open Geospatial Consortium (OGC、sf パッケージが実装するシンプルフィーチャの仕様も整備している団体) が、WKT (Well-Known Text) と呼ばれる CRS 記述形式をオープンスタンダードで開発した。\nこれは、「ISO 19111:2019 に記述された座標参照系の抽象モデルのテキスト文字列実装の構造と内容を定義する」100ページを超えるドキュメントに詳細が記述されている。 (Open Geospatial Consortium 2019)。\nWGS84 CRS、識別子 EPSG:4326 の WKT 表現は以下の通りである。\nコマンドの出力は、CRS 識別子 (空間参照識別子または Spatial Reference Identifier SRID とも呼ばれる) がどのように機能するかを示している。これは単にルックアップであり、CRS のより完全な WKT 表現に関連する一意の識別子を提供するものである。\nこのことは、識別子と CRS の長い WKT 表現との間にミスマッチがある場合はどうなるのか、という問題を提起している。\nこの点、Open Geospatial Consortium (2019) は明確で、冗長な WKT 表現が識別子より優先される。引用された識別子の属性や値が、WKT 記述で明示的に与えられた属性や値と矛盾する場合、 WKT の値が優先されるものとする。\nCRS の識別子を AUTHORITY:CODE という形式で参照する慣習は、他の言語で書かれた地理ソフトウェアでも使われており、正式に定義された広範囲の座標系を参照することができる。28\nCRS の識別子で最もよく使われる機関は、CRS の標準化リストを発表した欧州石油調査グループ (European Petroleum Survey Group) の頭文字をとった EPSG である (EPSG は2005年に石油・ガス団体の Geomatics Committee International Association Oil & Gas Producers により引き継ぎされた)。\nCRS の識別子には、他の機関を使用することもできる。\n例えば、ESRI:54030 は、ESRI の Robinson 投影の実装で、以下の WKT 文字列 (最初の8行のみ表示) を持っていることを指している。\nWKT 文字列は網羅的で詳細かつ正確であるため、CRS の格納や変換を曖昧にすることなく行うことができる。\n測地系 (datum)、楕円体 (ellipsoid)、本初子午線、投影法 (projection)、単位 (unit) など、任意の CRS に関するすべての関連情報が含まれている。29\n最近 (ver 6 以降) の PROJ のバージョンでは、座標操作を定義するために proj-string を使用することができるが、いくつかの proj-string キー (+nadgrids , +towgs84 , +k , +init=epsg:) はもうサポートされていないか、推奨されないものである。\nまた、proj-string に直接設定できる測地基準系 (datum) は 3 つ (WGS84、NAD83、NAD27) だけである。\nCRS の定義の進化と PROJ ライブラリの長い説明は、Bivand (2021)、Pebesma Bivand (2023b) の第 2 章、Floris Vanderhaeghe によるブログ (https://inbo.github.io/tutorials/tutorials/spatial_crs_coding/) に記載されている。\nPROJ documentation にも概説されているように、WKT CRS 形式には WKT1 と 2 種類の WKT2 があり、後者 (WKT2, 2018 仕様) は ISO 19111:2019 に対応するものである (Open Geospatial Consortium 2019)。","code":"\nst_crs(\"EPSG:4326\")\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#>         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]],\n#>         ENSEMBLEACCURACY[2.0]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"World.\"],\n#>         BBOX[-90,-180,90,180]],\n#>     ID[\"EPSG\",4326]]\nst_crs(\"ESRI:54030\")\n#> Coordinate Reference System:\n#>   User input: ESRI:54030 \n#>   wkt:\n#> PROJCRS[\"World_Robinson\",\n#>     BASEGEOGCRS[\"WGS 84\",\n#>         DATUM[\"World Geodetic System 1984\",\n#>             ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>                 LENGTHUNIT[\"metre\",1]]],\n...."},{"path":"reproj-geo-data.html","id":"crs-setting","chapter":"7 地理データの再投影","heading":"7.3 座標系の照会と設定","text":"\nR の空間オブジェクトに CRS がどのように格納され、どのようにクエリや設定ができるのかを見ていこう。\nまず、ベクタの地理データオブジェクトの CRS の取得と設定について、次の例から見ていく。新しいオブジェクトである new_vector は、世界の国々を表すクラス sf のデータフレームである (詳しくはヘルプページ ?spData::world を参照)。\nCRS は sf 関数 st_crs() で取得することができる。\n出力は主に2つの要素を含むリストである。User input (この場合、WGS 84、入力ファイルから取得した EPSG:4326 の同義語)、前述の CRS 識別子に対応する。wkt CRS に関するすべての関連情報を含む完全な WKT 文字列を含む。input 要素は柔軟性があり、入力ファイルやユーザー入力に応じて、AUTHORITY:CODE 表現 (例: EPSG:4326)、CRS の名前 (例: WGS 84)、あるいは proj-string 定義を含めることができる。\nwkt 要素には WKT 表現が格納され、オブジェクトをファイルに保存したり、座標演算を行う際に使用される。\n上記で、new_vector のオブジェクトは、WGS84 楕円体を持ち、グリニッジ本初子午線を使用し、緯度・経度軸の順番になっていることがわかる。\nこの場合、この CRS の使用に適したエリアを説明する USAGE や、CRS の識別子を示す ID (EPSG:4326) などの追加要素もある。\nまた、st_crs 関数には、使用中の CRS に関する追加情報を取得することができる、という便利な特徴もある。\n例えば、以下を実行してみよう。st_crs(new_vector)$IsGeographic: CRS が地理的かどうかを確認st_crs(new_vector)$units_gdal: CRS 単位を調べるst_crs(new_vector)$srid: その「SRID」識別子を抽出 (存在する場合)st_crs(new_vector)$proj4string: proj-string 表現を抽出CRS がない場合や間違った CRS が設定されている場合は、st_set_crs() 関数を使用することができる (この場合、ファイル読み込み時にすでに CRS が正しく設定されているので、WKT 文字列は変更されずに残る)。\nラスタ地理データオブジェクトの CRS の取得と設定は、同様の方法で行われる。\nterra パッケージの crs() 関数は SpatRaster オブジェクトから CRS 情報にアクセスする (読みやすく表示するために cat() 関数を使用していることに注意)。出力は CRS の WKT 表現である。\n同じ関数、crs() を使って、ラスタオブジェクトに CRS を設定することもできる。ここでは、識別子 (ほとんどの場合こちらを推奨) または完全な WKT 表現のいずれかを使用することができる。\ncrs を設定する代替方法としては、proj-string 文字列または crs() を持つ他の既存のオブジェクトから抽出された CRS があるが、これらのアプローチは将来対応されない可能性がある。重要な点として、st_crs() と crs() 関数は、座標の値や形状を変更しない。\nその役割は、オブジェクト CRS のメタデータ情報を設定することのみである。Section 2.2 で紹介したロンドンの例を基に、以下のコードで作成した london データセットのように、地理的オブジェクトの CRS が不明な場合がある。この出力 NA は、sf が CRS が何であるかを知らず、推測するつもりがないことを示している (NA は、“Applicable/Available” の略語で、文字通り「利用できない」という意味)。\nCRS を手動で指定するか、CRS メタデータを持つソースから読み込まれない限り、sf は座標系について「わからない」と言う以外の明示的な仮定をしない。\nこの動作は、利用可能な CRS の多様性を考えると理にかなっているが、GeoJSON ファイル形式仕様のような、すべての座標が lon/lat CRS (EPSG:4326) を持つという単純化した仮定をするいくつかのアプローチとは異なる。\nすべての地理座標には座標参照系があり、ソフトウェアがプロットやジオメトリの操作を正しく判断できるのは、扱う CRS の種類がわかっている場合のみである。\nしたがって、データセットの CRS を常に確認し、不足している場合は設定することが重要である。","code":"\nvector_filepath = system.file(\"shapes/world.gpkg\", package = \"spData\")\nnew_vector = read_sf(vector_filepath)\nst_crs(new_vector) # get CRS\n#> Coordinate Reference System:\n#>   User input: WGS 84 \n#>   wkt:\n#>   ...\nnew_vector = st_set_crs(new_vector, \"EPSG:4326\") # CRS を設定\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast = rast(raster_filepath)\ncat(crs(my_rast)) # CRS を取得\n#> GEOGCRS[\"WGS 84\",\n#>     DATUM[\"World Geodetic System 1984\",\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n....\ncrs(my_rast) = \"EPSG:26912\" # CRS を設定\nlondon = data.frame(lon = -0.1, lat = 51.5) |> \n  st_as_sf(coords = c(\"lon\", \"lat\"))\nst_is_longlat(london)\n#> [1] NA\nlondon_geo = st_set_crs(london, \"EPSG:4326\")\nst_is_longlat(london_geo)\n#> [1] TRUE"},{"path":"reproj-geo-data.html","id":"geom-proj","chapter":"7 地理データの再投影","heading":"7.4 投影データおよび非投影データに対する幾何学操作","text":"sf version 1.0.0 より、R は緯度経度 CRS を持って入るベクタデータセットに対する機能が大幅に強化された。この機能強化は、Section 2.2.9 で取り上げた S2 球面ジオメトリエンジンによるものである。\nFigure 7.1 で示すように、sf は、CRS 種別に応じて GEOS または S2 を使い分ける (デフォルトは S2)。30\n座標投影系のデータと CRS がないデータの場合、常に GEOS が使われる。地理データではデフォルトで S2 が使われるが、無効化したい場合は sf::sf_use_s2(FALSE) とする。\nFIGURE 7.1: 入力データの CRS に依存する sf パッケージのジオメトリ操作の動作。\nCRS の重要性を示すために、このセクションでは、前セクションで作成した london オブジェクトの周りに 100 km のバッファを作成する。\nまた、100 km にほぼ相当する 1 度 (赤道では 1 度は約 111 km) の「距離」を持つ意図的に欠陥のあるバッファを作成する。\nコードに入る前に、Figure 7.2 を見て、これからコードチャンクで再現するはずの出力を視覚的に把握するのもよいだろう。最初の段階として、上記で作成した london と london_geo のオブジェクトの周りに、ロンドン中心部から1度と 100 km (または 10万 m、科学的表記法では 1e5 と表現できる) の境界距離の3つのバッファを作成する。上の1行目では、sf は入力が投影されていると仮定して、度数単位のバッファを持つ結果を生成しているが、後述するようにこれは問題である。\n2 行目の sf では、Chapter 2 で導入された球面幾何エンジン S2 明示せずに使用し、max_cells = 1000 のデフォルト値 (3 行目で 100 に設定) を使用してバッファの範囲を計算しているが、その結果はすぐに明らかになるだろう。\n非投影 (地理) 座標系に対する sf の S2 幾何エンジン使用の影響を強調するために、以下のコードチャンクで、コマンド sf_use_s2() (デフォルトではオン、TRUE) で一時的にそれを無効にしてみよう。\nlondon_buff_no_crs と同様、新しい london_geo オブジェクトは、単位が度でほとんどの場合意味を持たない。上記の警告メッセージは、lon/lat データに対して平面ジオメトリ演算を実行する際の問題を示唆している。\n球形幾何演算をコマンド (sf::sf_use_s2(FALSE)) でオフにすると、バッファ (およびその他のジオメトリ演算) は緯度と経度の単位を使用するため、メートルなどの適切な距離単位の代わりにならないため、価値のない出力となる場合がある。地理的 (緯度経度) CRS に関する警告は「CRS を設定してはいけない」という解釈をするべきではない。むしろ、ほとんど常に設定すべきである。\nこれは、投影された CRS にデータを再投影することを提案したと理解するのがよいだろう。\nただし、この提案に常に耳を傾ける必要もない。空間演算と幾何演算を実行しても、ほとんど違いがない場合もある (例えば、空間的な部分集合作成など)。\nしかし、バッファ作成など距離を伴う操作では、(球面幾何エンジンを使わずに) 良い結果を得るには、データの投影コピーを作成し、それに対して操作を実行するしかない。\nこれは、以下のコードチャンクで行われる。結果は、london と同じであるが、適切な CRS (この場合、EPSG コードが 27700 の British National Grid) で、単位がメートルである新しいオブジェクトが作成される。\nCRS が変化したことは、st_crs() を使って次のように確認できる (出力の一部は ... で置き換えている)。この CRS の記述で注目すべきは、EPSG コード (EPSG: 27700) と詳細な wkt の文字列 (最初の5行のみ表示) であろう。31\nLENGTHUNIT フィールドに記述された CRS の単位が (度ではなく) メートルであることから、これが投影型 CRS であることがわかる。 st_is_longlat(london_proj) は現在 FALSE を返し、london_proj に対する幾何演算は警告なしで機能する。\nlondon_proj のバッファ操作は GEOS を使用し、結果は適切な距離の単位で返される。\n次のコードは、ちょうど 100 km の投影データの周りにバッファを作成するものである。先に作成した CRS を持つ 3 つの london_buff* オブジェクト (london_buff_s2、london_buff_lonlat、london_buff_projected) の形状を Figure 7.2 に示す。\nFIGURE 7.2: London 周辺のバッファで、S2 球面幾何エンジンを用いて作成した緯度経度データ (左)、投影データ (中)、球面幾何を用いない緯度経度データ (右) の結果を示している。左のプロットは、投影されていないデータを sf でバッファ作成した結果を示しており、デフォルトで Google の S2 spherical geometry engine を max_cells を 1000 に設定して呼び出している (細線)。太い「ブロック状の」線は、max_cells を 100 に設定して同じ操作を行った結果を示している。\ns2 と適切に予測された CRS に基づくバッファは、バッファの境界のすべての部分が London から等距離にあることを意味し、「つぶされた」ものではないことは、Figure 7.2 から明らかである。\n入力に CRS がないか、sf_use_s2() がオフになっているため、s2 が使われていないときに緯度経度 CRS から生成される結果は、南北軸に細長く歪んでいる。緯度経度に投影データを仮定する (GEOS による) アルゴリズムを使うことが危険であることが明らかである。\nしかし、S2 で生成された結果も、劇的な変化はないものの、歪んでいる。\nFigure 7.2 (左) のバッファ境界はどちらもギザギザしているが、これは s2 の引数 max_cells を 100 に設定して作成したバッファを表す太い境界の場合のみ、明らかまたは関連性があると考えられる。\nS2 経由の緯度経度データから得られる結果は、投影データから得られる結果とは異なるという教訓である。\n投影データにおける S2 由来のバッファと GEOS 由来のバッファの差は、max_cells の値が大きくなるほど小さくなる。この引数の「正しい」値は多くの要因に依存すると思われるが、デフォルト値 1000 は妥当なデフォルト値だと思われる。\nmax_cells の値を選択する時、計算の速度と結果の解像度のバランスをとる必要がある。\n滑らかな曲線の境界が有利な場合、バッファ作成 (または他のジオメトリ操作) の前に投影型 CRS に変換することが適切な場合がある。CRS の重要性 (投影座標か地理座標か) と、緯度経度に対してバッファ作成する際に S2 を使用するという sf のデフォルト設定がデータに与える影響は、上記の例から明らかである。\nこの後のセクションでは、投影 CRS が必要な場合にどの CRS を使用するか、ベクタおよびラスタオブジェクトの再投影の詳細について、より深く掘り下げて説明する。","code":"\nlondon_buff_no_crs = st_buffer(london, dist = 1)  # 正しくない: CRS がない\nlondon_buff_s2 = st_buffer(london_geo, dist = 100000) # 暗黙で s2 を使用\nlondon_buff_s2_100_cells = st_buffer(london_geo, dist = 100000, max_cells = 100) \nsf::sf_use_s2(FALSE)\n#> Spherical geometry (s2) switched off\nlondon_buff_lonlat = st_buffer(london_geo, dist = 1) # 正しくない結果\n#> Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\n#> endCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n#> dist is assumed to be in decimal degrees (arc_degrees).\nsf::sf_use_s2(TRUE)\n#> Spherical geometry (s2) switched on\nlondon_proj = data.frame(x = 530000, y = 180000) |> \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:27700\")\nst_crs(london_proj)\n#> Coordinate Reference System:\n#>   User input: EPSG:27700 \n#>   wkt:\n#> PROJCRS[\"OSGB36 / British National Grid\",\n#>     BASEGEOGCRS[\"OSGB36\",\n#>         DATUM[\"Ordnance Survey of Great Britain 1936\",\n#>             ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n#>                 LENGTHUNIT[\"metre\",1]]],\n....\nlondon_buff_projected = st_buffer(london_proj, 100000)"},{"path":"reproj-geo-data.html","id":"whenproject","chapter":"7 地理データの再投影","heading":"7.5 いつ再投影するべきか？","text":"\n前節では、CRS を手動で設定する方法として、st_set_crs(london, \"EPSG:4326\") を紹介した。\nしかし、現実のアプリケーションでは、データの読み込み時に自動的に CRS が設定されるのが一般的である。\n多くのプロジェクトで、CRS 関連の主なタスクは、ある CRS から別の CRS に、オブジェクトを変換することである。\nしかし、どのような場合にデータを変換する必要があるのだろうか。\nそして、どの CRS に？\nこういった質問に対する明確な答えはなく、CRS の選択には常にメリットだけでなくデメリットもある (Maling 1992)。\nこのセクションでは、決定する際に役立つ一般原則を紹介しよう。まず最初に、いつ変換するべきかを考える。\nleaflet パッケージでデータをオンライン公開する場合など、地理的 CRS が必要になる。\nまた、異なる CRS を持つ2つのオブジェクトの距離を求める場合のように、異なる CRS を持つ 2 つの sf オブジェクトを比較したり、組み合わせたりする必要がある場合もある。london と london_proj のオブジェクトを地理的に比較できるようにするには、一方を他方の CRS に変換する必要がある。\nしかし、どの CRS を使えばいいのか？\n特にウェブマッピングを含む多くのプロジェクトでは、EPSG:4326 での出力が必要であるが、その場合、投影オブジェクトを変換する価値がある。\nしかし、球面幾何演算エンジンではなく平面ジオメトリ演算が必要なプロジェクト (例えば、滑らかなエッジを持つバッファを作成する) の場合、地理的 CRS のデータを英国ナショナルグリッド (EPSG:27700) などの投影 CRS で同等のオブジェクトに変換する価値があるだろう。\nそれが、Section 7.7 のテーマである。","code":"\nst_distance(london_geo, london_proj)\n# > Error: st_crs(x) == st_crs(y) is not TRUE"},{"path":"reproj-geo-data.html","id":"which-crs","chapter":"7 地理データの再投影","heading":"7.6 どの CRS を使うべきか？","text":"\nどの CRS を使うかというのは難しい問題で、「正しい」答えがあるわけではない。\n「万能の投影は存在せず、すべて指定したフレームの中心から離れると歪みが発生する」 (Bivand, Pebesma, Gómez-Rubio 2013) 。\nさらに言えば、すべてのタスクで 1 つの投影法だけに執着するべきではない。\nある投影法を解析の一部に使い、別の投影法を別の部分に使い、さらに別の投影法を可視化することも可能である。\n常に自分の目標に最も適した CRS を選ぶように心がけよう。地理的 CRS を選択する場合、WGS84 (日本語 版) となることが多い。(訳注: 上で既に述べたが、WGS84 に対応する日本の CRS は、東日本大震災以降の JGD2011 と、それ以前の JGD2000 である。大事なことなので繰り返す。)\nウェブマッピングだけでなく、GPS データセットや何千ものラスタ、ベクタデータセットがこの CRS でデフォルトで提供されているため、利用されている。\nWGS84 は世界で最も一般的な CRS なので、その EPSG コード 4326 を知っておくとよいだろう。(訳注: JGD2011は 6668、JGD2000 は 4612 である。)32\nこの「マジックナンバー」は、投影された CRS が異常なオブジェクトを、広く理解されるものに変換するために使用することができる。投影 CRS が必要な場合はどうだろうか？\n自由に決められないこともある。\n「多くの場合、投影の選択は公的な地図作成機関によって行われる」 (Bivand, Pebesma, Gómez-Rubio 2013) 。\nつまり、現地のデータソースで作業する場合、公式の CRS が最も正確ではないとしても、互換性を確保するために、データが提供された CRS で作業することが望ましいと思われる。\nLondon の例は、() 英国ナショナルグリッド (関連する EPSG コード27700) がよく知られており、(b) 元のデータセット (london) がすでにその CRS を持っていたので、簡単に答えることができたのである。\n一般的に使われているデフォルトは Universal Transverse Mercator (UTM 日本語) で、地球を縦 60 個のくさびと横 20 個の緯度に分割した CRS のセットである。\n地球上のほとんどの場所には UTM コードがあり、例えば「60H」 は R が発明された New Zealand 北部を指している。\nUTM EPSG コードは、北半球では 32601 から 32660 まで、南半球では 32701 から 32760 まで順次表示される。その仕組みを説明するために、 ここにある通り、地球上の任意の地点に関連する EPSG コードを計算する関数 lonlat2UTM() を作ってみよう。次のコマンドは、この機能を利用して、Auckland と London の UTM ゾーンと関連する EPSG コードを特定する。UTM CRS で使用されている横メルカトル図法は等角 (conformal) であるが、UTM ゾーンの中心から離れるにつれて面積や距離の歪みがひどくなる。\nそのため、GIS ソフトウェア Manifold のドキュメントでは、UTM ゾーンを使用するプロジェクトの縦断範囲を中心子午線から 6 度までに制限することを提案している (manifold.net)。\nそのため、UTM は比較的狭い範囲での角度保存を重視する場合にのみ使用することを勧める。現在、適切な CRS を選択するためのツールも用意されており、これには crssuggest パッケージ が含まれている。\nこのパッケージのメイン関数である suggest_crs() は、地理的な CRS を持つ空間オブジェクトを受け取り、与えられた領域に使用可能な投影 CRS のリストを返す。33\nもう一つの便利なツールは、選択した場所とタイプに基づいて CRS をリストアップするウェブページ https://jjimenezshaw.github.io/crs-explorer/ である。\n重要な注意点: これらのツールは多くの場面で役立つが、適用する前に推奨される CRS の特性を知っておく必要がある。\n適切な CRS がすぐにわからない場合、CRS の選択は、その後の地図や分析において保存することが最も重要である特性によって決められるべきだろう。\nCRS は、等面積、等距離、等角 (形状はそのまま)、またはそれらの妥協点の組み合わせである (Section 2.4.2 参照)。\nローカルパラメータを持つカスタム CRS を対象地域に合わせて作成し、単一の CRS がすべてのタスクに適合しないプロジェクトでは、複数の CRS を使用することができる。\n「測地線計算」は、CRS が適切でない場合の代替手段を提供することができる (proj.org/geodesic.html を参照)。\nどの投影 CRS を使っても、数百キロメートルに及ぶジオメトリでは、結果が正確でない可能性がある。\nカスタム CRS を決定する際には、以下を勧める。34\nカスタムローカル投影 (原点の緯度・経度を調査地域の中心に設定) のランベルト正積方位図法 (Lambert azimuthal equal-area, LAEA 日本語版)、これはすべての場所で等面積投影だが数千キロメートル以上では形状が歪んでしまう。ある地点とローカル投影の中心点との直線距離を具体的に正確に表す正距方位図法 (Azimuthal equidistant, AEQD 日本語版) 投影図数千 km に及ぶ地域のランベルト正角円錐図法 (Lambert conformal conic, LCC 日本語版) 投影。円錐は、距離と面積の特性がセカント線間で妥当となるように設定されている。極域のステレオ投影 (Stereographic, STERE 日本語版) 投影。ただし、中心から数千キロメートル離れた面積や距離の計算に頼らないように注意すること。地域のデータセットに特化した投影 CRS を自動的に選択する方法として、調査地域の中心点に対して正距方位図法投影を作成することが考えられる。\nこれは、データセットの中心点に基づくメートル単位でカスタム CRS (EPSG コードなし) を作成するものである。\n他のデータセットが作成したカスタム CRS と互換性がなく、数百キロメートルに及ぶ広範なデータセットに使用すると、結果が正確でなくなる可能性がある。このセクションで説明する原則は、ベクタデータセットとラスタデータセットに等しく適用される。\nしかし、CRS 変換には、それぞれの地理データモデルに特有の特徴がある。\nベクタデータ変換の特殊性は Section 7.7 で、ラスタ変換の特殊性は Section 7.8 で説明する。\n次に、Section 7.9 で、カスタム地図投影を作成する方法を紹介する。","code":"\nlonlat2UTM = function(lonlat) {\n  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1\n  if (lonlat[2] > 0) {\n    utm + 32600\n  } else{\n    utm + 32700\n  }\n}\nlonlat2UTM(c(174.7, -36.9))\n#> [1] 32760\nlonlat2UTM(st_coordinates(london))\n#> [1] 32630"},{"path":"reproj-geo-data.html","id":"reproj-vec-geom","chapter":"7 地理データの再投影","heading":"7.7 ベクタジオメトリの再投影","text":"\nChapter 2 は、ベクタ幾何学がいかに点で構成されているか、そしていかに点が直線や多角形などのより複雑なオブジェクトの基礎を形成しているかを示した。\nつまり、ベクタの再投影は、直線や多角形の頂点となるこれらの点の座標を変換することになる。Section 7.5 は、2 つのオブジェクト間の距離を計算するために、少なくとも1つの sf オブジェクトを異なる CRS を持つ同等のオブジェクトに変換する必要がある例を含んでいる。london の変換版ができたので、sf 関数 st_transform() を使って、2 つのロンドン表現間の距離を求めることができる。35\nlondon と london2 が 2 km 以上も離れているのは意外かもしれない。36CRS の確認と再投影のための関数を、cycle_hire_osm を参照して以下に示す。これは spData の sf オブジェクトで、ロンドンで自転車をレンタルできる「ドッキングステーション」を表しているものである。\nSection 7.1 で学んだように、sfオブジェクトの CRS は、関数 st_crs() を使って問い合わせることができる。\n出力は、以下で示すように座標系に関する情報を複数行のテキストとして表示する。Section 7.3 で見たように、主な CRS コンポーネントである User input と wkt は単一の実体として出力される。st_crs() の出力は、実際には、次のコードチャンクの出力に示すように、input と wkt という単一の文字列という 2 つの要素を持つクラス crs の名前付きリストになっている。Name、proj4string、epsg を含む追加の要素を $ 演算子で取り出すことができる (詳しくは ?st_crs) と GDAL website の CRS tranformation tutorial を参照)。Section 7.2 で述べたように、crs_lnd オブジェクトの $wkt 要素に格納された WKT 表現は、究極の真理の源である。\nこれは、前のコードチャンクの出力が、オブジェクトとその CRS の固有の属性ではなく、PROJ によって提供される wkt 表現からのクエリであることを意味する。オブジェクトの CRS が変換されると、CRS の wkt と User Input の両方の要素が変更される。\n以下のコードチャンクでは、CRS を投影した新しいバージョンの cycle_hire_osm を作成する (簡潔にするため、CRS 出力の最初の 4 行のみを表示する)。この結果、オブジェクトは EPSG コード 27700 の新しい CRS を持つことになる。\nしかし、この EPSG コードや他のコードの詳細を調べるにはどうしたらよいだろうか？\nネットで検索するのも一つの方法であるが、CRS オブジェクトのプロパティを見ても良い。その結果、EPSG コード 27700 が英国ナショナルグリッドを表しており、「 EPSG 27700」とネット検索すれば出てくるだろう。","code":"\nlondon2 = st_transform(london_geo, \"EPSG:27700\")\nst_distance(london2, london_proj)\n#> Units: [m]\n#>      [,1]\n#> [1,] 2018\nst_crs(cycle_hire_osm)\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCS[\"WGS 84\",\n#>     DATUM[\"WGS_1984\",\n#>         SPHEROID[\"WGS 84\",6378137,298.257223563,\n....\ncrs_lnd = st_crs(london_geo)\nclass(crs_lnd)\n#> [1] \"crs\"\nnames(crs_lnd)\n#> [1] \"input\" \"wkt\"\ncrs_lnd$Name\n#> [1] \"WGS 84\"\ncrs_lnd$proj4string\n#> [1] \"+proj=longlat +datum=WGS84 +no_defs\"\ncrs_lnd$epsg\n#> [1] 4326\ncycle_hire_osm_projected = st_transform(cycle_hire_osm, \"EPSG:27700\")\nst_crs(cycle_hire_osm_projected)\n#> Coordinate Reference System:\n#>   User input: EPSG:27700 \n#>   wkt:\n#> PROJCRS[\"OSGB36 / British National Grid\",\n#> ...crs_lnd_new = st_crs(\"EPSG:27700\")\ncrs_lnd_new$Name\n#> [1] \"OSGB36 / British National Grid\"\ncrs_lnd_new$proj4string\n#> [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000\n+y_0=-100000 +ellps=airy +units=m +no_defs\"\ncrs_lnd_new$epsg\n#> [1] 27700"},{"path":"reproj-geo-data.html","id":"reproj-ras","chapter":"7 地理データの再投影","heading":"7.8 ラスタジオメトリの再投影","text":"\n前節で説明した投影の概念は、ラスタにも適用できる。\nしかし、ベクタとラスタの再投影には重要な違いがある。\nベクタオブジェクトの変換は、すべての頂点の座標を変更することになるが、ラスタデータには当てはまらない。\nラスタは同じ大きさの矩形セル (度やメートルなどの地図単位で表現される) で構成されているため、通常、ピクセルの座標を個別に変換することは不可能である。\nよってラスタの再投影では、多くの場合、元のオブジェクトとは異なる列と行の数で、新しいラスタオブジェクトを作成する。\nその後、新しいピクセルを適切な値で「埋める」ことができるように、属性を再推定する必要がある。\nつまり、ラスタの再投影は、ラスタ範囲を別の CRS にベクタ再投影 (Section 7.7)、リサンプリングによる新しいピクセル値の計算 (Section 5.3.4) という 2 つの別々の空間処理として考えることができる。\nしたがって、ラスタデータとベクタデータの両方を使用する場合は、ラスタの再投影を避け、ベクタの再投影を行う方が良い場合がほとんどである。ラスタの再投影処理は、terra パッケージの project() を使用する。\n前節で紹介した st_transform() 関数と同様、project() は空間オブジェクト (この場合はラスタデータセット) と何らかの CRS 表現を第 2 引数として受け取る。\n第 2 引数には、異なる CRS を持つ既存のラスタオブジェクトを指定することもできる。ラスタ変換の例として、カテゴリデータと連続データを使ったものを見てみよう。\n土地被覆データは、通常、カテゴリ化された地図で表現される。\nnlcd.tif ファイルは、以下のコードチャンクの出力に示すように、NAD83 / UTM ゾーン 12N CRS の National Land Cover Database 2011 から取得した米国ユタ州の小領域の情報を提供する (出力の最初の行のみ示す)。この地域では、8 つの土地被覆クラスが区別された (NLCD2011 の土地被覆クラスの全リストは、 mrlc.govで見ることができる)。カテゴリ別ラスタを再投影する場合、推定値は元のラスタと同じでなければならない。\nこれは、各新規セルの値を入力ラスタの最も近いセル (中心) の値に設定する最近傍補間法 (near) を使って行うことができる。\n例えば、ウェブマッピングに適した地理的 CRS である WGS84 に cat_raster を再投影している。\nまず、この CRS の 定義を取得する。\n次のステップは、project() 関数でラスタを再投影することである。カテゴリデータの場合は、最近傍補間法 (near) を使用する。新しいオブジェクトの多くのプロパティは、Table 7.1 に示すように、列と行の数 (したがってセルの数)、解像度 (メートルから度に変換)、範囲など、以前のオブジェクトとは異なる (新しいカテゴリが作成されたのではなく、NA 値が追加されたため、カテゴリ数が 8 から 9 に増加していることに注意。土地被覆クラスは維持されている)。TABLE 7.1: オリジナル (cat_raster_wgs84) と 投影 (cat_raster_wgs84) のカテゴリラスタ データセットの主要な属性。数値ラスタ (numeric またはこの場合は integer の値) の再投影もほぼ同じ手順で行う。\nこれは、Shuttle Radar Topography Mission (SRTM) の spDataLarge にある srtm.tif で実証されており、WGS84 CRS による海抜メートル (標高) の高さを表している。これから、このデータセットを投影型 CRS に再投影するが、カテゴリデータに適した最近傍法ではない。\nその代わりに、元のラスタの 4 つの最近接セルに基づいて出力セルの値を計算する双一次補間法を使用する。37\n投影されたデータセットの値は、これら 4 つのセルの値の距離加重平均である。\n入力セルが出力セルの中心に近いほど、その重みは大きくなる。\n以下のコマンドは、WGS 84 / UTM zone 12N を表すテキストストリングを作成し、この CRS にラスタを双一次補間法 (bilinear) で再投影するものである (出力は示していない)。数値変数のラスタ再投影は、セル数、解像度、範囲などの値や空間特性にも変化をもたらす。\nこれらの変化は、Table 7.2 で実証されている。38TABLE 7.2: オリジナル (‘con_raster’) と投影 (‘con_raster_ea’) 連続ラスタデータセット の主要な属性。","code":"\ncat_raster = rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\ncrs(cat_raster)\n#> PROJCRS[\"NAD83 / UTM zone 12N\",\n#> ...\nunique(cat_raster)\n#>       levels\n#> 1      Water\n#> 2  Developed\n#> 3     Barren\n#> 4     Forest\n#> 5  Shrubland\n#> 6 Herbaceous\n#> 7 Cultivated\n#> 8   Wetlands\ncat_raster_wgs84 = project(cat_raster, \"EPSG:4326\", method = \"near\")\ncon_raster = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\ncat(crs(con_raster))\n#> GEOGCRS[\"WGS 84\",\n#>     DATUM[\"World Geodetic System 1984\",\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n....\ncon_raster_ea = project(con_raster, \"EPSG:32612\", method = \"bilinear\")\ncat(crs(con_raster_ea))"},{"path":"reproj-geo-data.html","id":"mapproj","chapter":"7 地理データの再投影","heading":"7.9 カスタム地図投影法","text":"\nEPSG:4326 のような AUTHORITY:CODE の識別子で捕捉される確立された CRS は、多くのアプリケーションに適している。\nしかし、場合によっては、代替的な予測を使用したり、カスタム CRS を作成したりすることが望ましい。\nSection 7.6 は、カスタム CRS を使用する理由を述べ、いくつかの可能なアプローチを提示した。\nここでは、これらのアイデアを R で応用する方法を紹介する。一つは、既存の CRS の WKT の定義を取り、その要素の一部を変更し、新しい定義を再投影に使用する方法である。\nこれは、空間ベクタでは st_crs() と st_transform() で、空間ラスタでは crs() と project() で行うことができる。次の例では、zion オブジェクトをカスタム正距方位図法 (AEQD) CRS に変換する例を示している。カスタム AEQD CRS を使用するには、データセットの中心点の座標を度数で知る必要がある (地理的 CRS)。\n私たちの場合、zion 領域の重心を計算し、WGS84 に変換することでこの情報を抽出することができる。次に、新たに得られた値を用いて、以下に示す正距方位図法 CRS の WKT 定義を更新することができる。\n\"Central_Meridian\" は重心の経度、\"Latitude_Of_Origin\" は緯度であることに注意しておこう。この方法の最後のステップは、元のオブジェクト (zion) を新しいカスタム CRS (zion_aeqd) に変換することである。カスタム投影法は、例えば Projection Wizard のウェブアプリケーションを使って、対話的に行うことも可能である (Šavrič, Jenny, Jenny 2016)。\nこのサイトでは、データの空間的範囲と歪みのプロパティを選択すると、可能な投影のリストが返される。\nまた、このリストには投影の WKT 定義が含まれており、コピーして再投影に使用することができる。\nWKT 文字列を用いたカスタム CRS 定義の作成については、Open Geospatial Consortium (2019) を参照。\nPROJ 文字列は、Section 7.2 で述べた、投影、特に大きな地理的領域をカバーする幾何学に固有の制限を受け入れて、カスタム投影を作成するために使用することもできる。\n多くの投影法が開発され、PROJ 文字列の +proj= の要素で設定することができる。 PROJ website だけでも数十の投影法が詳細に記述されている。面積の関係を維持したまま世界を地図化する場合、Figure 7.3 に示されるモルワイデ図法 (Mollweide) が、一般的で賢明な選択となる (Jenny et al. 2017)。\nこの投影法を使用するには、st_transform 関数の proj-string 要素 \"+proj=moll\" を使って指定する必要がある。\nFIGURE 7.3: 世界のモルワイデ図法。\n世界地図を作成する際、すべての空間特性 (面積、方向、距離) に対して歪みを最小化することが望まれることが多い。\nこれを実現するための代表的な投影法として、Figure 7.4 に示される ヴィンケル図法 (第 3 図法) (Winkel Tripel Projections) がある。39\n結果は、以下のコマンドで作成された。\nFIGURE 7.4: 世界のヴィンケル第 3 図法。\nさらに、proj-string パラメータはほとんどの CRS 定義で変更可能であり、例えば +lon_0 と +lat_0 パラメータを使用して投影の中心を調整することができる。\n以下のコードは、ニューヨークの経度と緯度を中心としたランベルト正積方位図法 (Lambert azimuthal equal-area projection) に座標を変換するものである (Figure 7.5)。\nFIGURE 7.5: ニューヨークを中心とした世界のランベルト正積方位図法。\nCRS の変更に関する詳しい情報は、Using PROJ のドキュメントに記載されている。","code":"\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion_centr = st_centroid(zion)\nzion_centr_wgs84 = st_transform(zion_centr, \"EPSG:4326\")\nst_as_text(st_geometry(zion_centr_wgs84))\n#> [1] \"POINT (-113 37.3)\"\nmy_wkt = 'PROJCS[\"Custom_AEQD\",\n GEOGCS[\"GCS_WGS_1984\",\n  DATUM[\"WGS_1984\",\n   SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],\n  PRIMEM[\"Greenwich\",0.0],\n  UNIT[\"Degree\",0.0174532925199433]],\n PROJECTION[\"Azimuthal_Equidistant\"],\n PARAMETER[\"Central_Meridian\",-113.0263],\n PARAMETER[\"Latitude_Of_Origin\",37.29818],\n UNIT[\"Meter\",1.0]]'\nzion_aeqd = st_transform(zion, my_wkt)\nworld_mollweide = st_transform(world, crs = \"+proj=moll\")\nworld_wintri = st_transform(world, crs = \"+proj=wintri\")\nworld_laea2 = st_transform(world,\n                           crs = \"+proj=laea +x_0=0 +y_0=0 +lon_0=-74 +lat_0=40\")"},{"path":"reproj-geo-data.html","id":"演習-5","chapter":"7 地理データの再投影","heading":"7.10 演習","text":"E1. オブジェクト nz を WGS84 CRS に変換した nz_wgs というオブジェクトを作成しなさい。クラス crs のオブジェクトを作成し、CRS を調べなさい。オブジェクトの範囲への参照について、CRS によってどの単位を使っているか？nz_wgs から CRS を削除してプロットしなさい。New Zealand の地図のどこがおかしいか？その理由は？E2. データセット world をユニバーサル横メルカトル図法に変換し (\"+proj=tmerc\")、っ結果をプロットしなさい。\n何が変わったか? その理由は?\nWGS 84 に戻してプロットしなさい。\nなぜ、このオブジェクトはオリジナルと異なるのか?E3. 連続色ラスタ (con_raster) を、最近傍補間法で NAD83 / UTM zone 12N に変換しなさい。\n何が変わったか?\nそれは、結果にどのように影響するか?E4. 影鳥ラスタ (cat_raster) を、双一次補間法 (biulinear) で WGS 84 に変換しなさい。\n何が変わったか?\nそれは、結果にどのように影響するか?","code":""},{"path":"read-write.html","id":"read-write","chapter":"8 地理データI/O","heading":"8 地理データI/O","text":"","code":""},{"path":"read-write.html","id":"prerequisites","chapter":"8 地理データI/O","heading":"必須パッケージ","text":"この章では、以下のパッケージが必要である。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)"},{"path":"read-write.html","id":"introduction-08","chapter":"8 地理データI/O","heading":"8.1 イントロダクション","text":"この章では、地理データの読み書きの方法について説明する。\n地理データ入力 (Input) はジオコンピューテーションに不可欠である。 実世界のアプリケーションはデータなしには不可能である。\nデータ出力 (Output) も重要で、研究の結果得られた価値ある新しいデータセットや改良されたデータセットを他の人が利用できるようにすることができる。\nこれらの入力/出力の処理をまとめて、データ /O と呼ぶことができる。\n地理データの入出力は、プロジェクトの最初と最後に簡単に行われることが多い。\nしかし、データの入出力はプロジェクト成功の基礎である。プロジェクトの初期に犯したミス (例えば、古いデータや何らかの欠陥のあるデータセットを使用すること) は、大きな問題につながる可能性がある。地理ファイル形式はたくさんあり、それぞれに長所と短所があるので、Section 8.2 で説明する。\nこれらのファイルの入力と出力は、それぞれ Section 8.3 と Section 8.4 で説明する。\nSection 8.5 では、様々なジオポータルとその使用方法について説明する。\nデータへのアクセスを容易にするため地理データをダウンロードするためのパッケージについては、Section 8.6 で説明する。\n作成したデータをウェブ上などに公開したい場合、地理メタデータが重要となるので、Section ?? で説明する。\n空間データは、ウェブサービスとして取得することもできる。これは Section 8.8 で説明する。\n最後の Section 8.9 では、ビジュアライゼーションに関する Chapter 9 に備えて、ビジュアル出力 (地図) を保存するための方法を紹介する。","code":""},{"path":"read-write.html","id":"file-formats","chapter":"8 地理データI/O","heading":"8.2 ファイル形式","text":"\n地理データセットは通常、ファイルまたは空間データベースとして保存される。\nファイル形式はベクタデータとラスタデータのどちらかを保存できるが、 PostGIS のような空間データベースは両方を保存できる (Section 10.7 も参照)。\n今日、ファイル形式の多様性は困惑するほどある。しかし、1960年代には、ハーバード大学で空間解析のための最初の広く配布されたプログラム (SYMAP) などの初期の GIS ソフトウェアが開発された。それ以降、多くの統合と標準化が行われてきた (Coppock Rhind 1991)。\nGDAL (Geospatial Data Abstraction Library、「グードル」と発音する。「グー」の goo の oo 部分は、Object Oeirneted を表す) は、2000年のリリース以来、地理ファイル形式間の非互換性に関連する多くの問題を解決した。\nGDAL は、多くのラスタおよびベクタデータフォーマットの読み書きのための統一された高性能なインタフェースを提供する。40\nGRASS、ArcGIS、QGIS など、多くのオープンおよびプロプライエタリな GIS プログラムは、GUI の背後に GDAL を使用して、地理データを取り込み、適切な形式で出力するという足回りの作業を行なっている。GDAL は、200 以上のベクタおよびラスタデータフォーマットへのアクセスを提供する。\nTable 8.1 では、よく使われる空間ファイル形式についての基本情報を紹介している。\nTABLE 8.1: TABLE 8.2: 代表的な空間ファイル形式。\n\nファイル形式の標準化とオープンソースを保証する重要な発展は、1994年の Open Geospatial Consortium (OGC) の設立であった。\nOGC は、シンプルフィーチャというデータモデル (Section 2.2.1 参照) を定義するだけでなく、例えば GML、KML や GeoPackage などのファイル形式で使用されているようなオープンスタンダードの開発も調整している。\nOGC が推奨するオープンなファイル形式は、プロプライエタリなフォーマットと比較して、いくつかの利点がある。標準が公開され、透明性が確保され、ユーザーがファイル形式をさらに開発し、特定のニーズに合わせて調整する可能性が開かれることである。ESRI Shapefile は最も一般的なベクタデータ交換フォーマットであるが、オープンフォーマットではない (仕様はオープン)。\n1990 年代初頭に開発されたもので、多くの制約がある。\nまず、少なくとも 3 つのファイルで構成されるマルチファイル形式であること。\n255 列までしかサポートしておらず、列名は 10 文字まで、ファイルサイズは 2 GB までと制限されている。\nさらに、ESRI Shapefile は、ポリゴンと複合ポリゴンの区別ができないなど、可能なすべてのジオメトリタイプをサポートしていない。41\nこのような制約があるにもかかわらず、長い間、有力な代替手段が見つかっていなかった。\n最近では、GeoPackage が登場し、ESRI Shapefile に取って代わろうとしている。\nGeopackage は、地理空間情報を交換するためのフォーマットで、OGC 規格の一つである。\nGeoPackage 規格は、地理空間情報を SQLite の小さなコンテナに格納する方法についての規則を記述している。\nしたがって、GeoPackage は軽量な空間データベースコンテナであり、ベクタおよびラスタデータだけでなく、非空間データおよび拡張機能も格納することができる。\nGeoPackage 以外にも、調べる価値のある地理空間データ交換フォーマットがある (Table 8.1)。\nラスタデータの形式としては、GeoTIFF 形式が主流である。\nTIFF ファイル内に CRS などの空間情報を埋め込むことができる。\nESRI Shapefile と同様に1990年代に開発されたフォーマットで、オープンなフォーマットである。\nさらに、GeoTIFF は現在も拡張・改良が続けられている。\nGeoTIFF フォーマットに最近追加された最も重要なものの1つが、COG (Cloud Optimized GeoTIFF) と呼ばれるバージョンである。\nCOG として保存されたラスタオブジェクトは、HTTP サーバーでホストすることで、他の人がファイル全体をダウンロードすることなく、ファイルの一部だけを読むことができる (Section 8.3.2 と Section 8.4.2 を参照)。この他にも、Table 8.1 で触れていない地理ファイル形式が多数存在し、また新しい空間データフォーマットが開発されている。\n最近開発されているものには、GeoArrow や Zarr) がある。\nGDAL ドキュメントは、ベクタやラスタドライバに関して学ぶ際に優れたリソースである。\nさらに、Section 2.2.1 で紹介するように、空間データフォーマットの中には、ベクタやラスタ以外のデータモデル (タイプ) を格納できるものもある。\nLiDAR 点群を格納するための LAS、LAZ 形式、多次元配列を格納するための NetCDF、HDF 形式が含まれる。また、空間データは、CSV ファイルや Excel スプレッドシートなど、表形式 (非空間) のテキスト形式で保存されることも多い。\n例えば、GIS ツールを使わない人と空間サンプルを共有したり、空間データ形式を受け付けない他のソフトウェアとデータを交換したりする際に便利である。\nしかし、この方法は、点よりも複雑な形状を保存するにはかなり困難であり、CRS など重要な空間メタ情報を直接保存できないなど、いくつかの欠点がある。","code":""},{"path":"read-write.html","id":"data-input","chapter":"8 地理データI/O","heading":"8.3 データ入力 (I)","text":"sf::read_sf() (ベクタデータの読み込みに使うメイン関数) や terra::rast() (ラスタデータの読み込みに使うメイン関数) などのコマンドを実行すると、ファイルからデータを読み込むイベントの連鎖が無言で開始される。\nさらに、多くの R パッケージは、サンプルデータを提供していたり (たとえば、これまでにも使ってきた spData::world)、あるいはさまざまなデータソースからデータを取得する関数を提供している。\nこれらはすべて、R にデータをロードするか、より正確には、ワークスペースにオブジェクトを割り当てる。\nすなわち、オブジェクトが R にインポートされると、これは RAM に保存され42、ls() で一覧を表示することができ (あるいは開発環境の ‘Environment’ に表示され)、R セッション中の .GlobalEnv からアクセスできる。","code":""},{"path":"read-write.html","id":"iovec","chapter":"8 地理データI/O","heading":"8.3.1 ベクタデータ","text":"\n空間ベクタデータは、さまざまなファイル形式で提供されている。\n.geojson や .gpkg ファイルなど、よく使われる表現のほとんどは、裏で GDAL のベクタドライバ を使う sf 関数 read_sf() (または同等の st_read()) で直接 R に取り込むことができる。\nst_drivers() は、最初の 2 列に name と long_name を含むデータフレームを返す。続く列に、Table 8.3 の主要ファイル形式について図示しているように、データの書き込みやラスタデータの保存など GDAL (従って sf) で利用できる各ドライバの機能を返す。\nTABLE 8.3: TABLE 8.4: ベクタデータを読み書きするための一般的な ドライバ/フォーマット。\n以下のコマンドは、コンピュータにインストールされた GDAL の最初の 3 つのドライバを報告し (結果はインストールされた GDAL のバージョンによって異なる場合がある)、それらのフィーチャの要約を表示する。\nなお、大半のドライバはデータの書き込みが可能であるが (87 種類中 51 種類)、ベクタデータに加えてラスタデータを効率的に表現できるフォーマットは 16 種類しかない (詳しくは ?st_drivers())。read_sf() の第一引数は dsn で、これはテキスト文字列または単一のテキスト文字列を含むオブジェクトであるべきである。\nテキスト文字列の内容は、ドライバによって異なる可能性がある。\n多くの場合、ESRI Shapefile (.shp) や GeoPackage 形式 (.gpkg) と同様に、dsn はファイル名となる。\nread_sf() は、ファイルの拡張子からドライバを推測する (下の例は、.gpkg の場合)。 (訳注: 日本語データを含むファイルは、文字エンコーディングを指定しないと文字化けを起こす。ほとんどの場合、CP932 を使用しているので、read_sf() に引数 options = \"ENCODING=CP932\" を設定するとよい。なお、国土数値情報の近年のファイルは UTF-8 を採用していることもある。)ドライバによっては、dsn は、フォルダ名、データベースのアクセス認証情報、または GeoJSON 文字列表現として提供されることがある (詳細は、read_sf() のヘルプページの例を参照)。ベクタドライバのフォーマットには、複数のデータレイヤを格納できるものがある。\nデフォルトでは、read_sf() は dsn で指定されたファイルの最初のレイヤを自動的に読み込む。しかし、layer 引数を使用すると、他のレイヤを指定することができる。\nまた、read_sf() 関数には、ファイルの一部だけを RAM に読み込む方法が 2 つある。\n1 つ目は、query の引数に関連して、OGR SQL query text でデータのどの部分を読み取るかを指定できるようにしたものである。\n以下の例では、タンザニアのみのデータを抽出している (Figure 8.1A)。\nこれは、\"world\" のレイヤから name_long が \"Tanzania\" と等しいすべての列 (SELECT *) を取得すると指定することで実現する。利用可能な列の名前がわからない場合、'SELECT * world FID = 1' でデータの 1 行だけを読み込むのが良い方法である。\nFID はフィーチャ IDを表し、多くの場合行番号であるが、その値は使用するファイル形式に依存する。\n例えば、FID は、ESRI Shapefile では 0 から始まり、他のファイル形式では 1 または任意の番号から始まる。2 つ目の仕組みは、wkt_filter の引数を使用する。\nこの引数は、データを抽出したい研究領域を表すよく知られたテキストを想定している。\nタンザニアの国境線 50,000 m と交差するポリゴンをファイルから読み込む。\nそのためには、() バッファを作成する (Section 5.2.3)、(b) st_geometry() で sf バッファオブジェクトを sfc ジオメトリオブジェクトに変換する、(c) st_as_text() でジオメトリを WKT に変換する、のいずれかの方法で「フィルタ」を準備する必要がある。では、この「フィルタ」を wkt_filter の引数で適用してみよう。Figure 8.1 :B に示すように、この結果はタンザニアとその 50 km バッファ内のすべての国を含めている。\nFIGURE 8.1: () クエリと (B) wkt フィルタを用いて、ベクタデータの部分集合を読み込む。\n当然ながら、一部のオプションは特定のドライバに固有のものである。43\n例えば、表計算ソフトのフォーマット (.csv) に保存された座標を考えてみよう。\nこのようなファイルを空間オブジェクトとして読み込むには、当然、座標を表す列の名前 (以下の例では、X と Y) を指定しなければならない。\nこれは、options パラメータを設定することで行うことができる。\n可能なオプションについては、対応する GDAL ドライバの説明の「オープンオプション」セクションを参照。\nカンマ区切り値 (csv) 形式は、https://gdal.org/drv_csv.html。「XY」座標を記述する代わりに、1 つの列でジオメトリ情報を記述することも可能である。\nWell-known text (WKT)、well-known binary (WKB)、GeoJSON 形式がその例である。\n例えば、world_wkt.csv のファイルには、世界の国々のポリゴンを表す WKT という列がある。\nこのことを示すために、今回も options パラメータを使用する。\n最後の例として、read_sf() が KML ファイルも読み込むことを紹介する。\nKML ファイルは、地理情報を XML 形式で格納している。これは、アプリケーションに依存しない方法で Web ページを作成し、データを転送するためのデータ形式である (Nolan Lang 2014)。\nここでは、ウェブから KML ファイルにアクセスする。\nこのファイルには、複数のレイヤが含まれている。\nst_layers() は、利用可能なすべてのレイヤを表示する。\nread_sf() の layer パラメータの助けを借りて最初のレイヤ Placemarks を選択する。このセクションで紹介した例はすべて、地理データのインポートに sf パッケージを使用したものである。\n高速で柔軟性があるが、特定のファイル形式については他のパッケージも見てみる価値があるだろう。duckdb は、DuckDB というデータベースへの R インターフェースで、空間拡張もある。","code":"\nsf_drivers = st_drivers()\nhead(sf_drivers, n = 3)\nsummary(sf_drivers[-c(1:2)])\nf = system.file(\"shapes/world.gpkg\", package = \"spData\")\nworld = read_sf(f)\ntanzania = read_sf(f, query = 'SELECT * FROM world WHERE name_long = \"Tanzania\"')\ntanzania_buf = st_buffer(tanzania, 50000)\ntanzania_buf_geom = st_geometry(tanzania_buf)\ntanzania_buf_wkt = st_as_text(tanzania_buf_geom)\ntanzania_neigh = read_sf(f, wkt_filter = tanzania_buf_wkt)\ncycle_hire_txt = system.file(\"misc/cycle_hire_xy.csv\", package = \"spData\")\ncycle_hire_xy = read_sf(cycle_hire_txt,\n  options = c(\"X_POSSIBLE_NAMES=X\", \"Y_POSSIBLE_NAMES=Y\"))\nworld_txt = system.file(\"misc/world_wkt.csv\", package = \"spData\")\nworld_wkt = read_sf(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\")\nu = \"https://developers.google.com/kml/documentation/KML_Samples.kml\"\ndownload.file(u, \"KML_Samples.kml\")\nst_layers(\"KML_Samples.kml\")\n#> Driver: LIBKML \n#> Available layers:\n#>               layer_name geometry_type features fields crs_name\n#> 1             Placemarks                      3     11   WGS 84\n#> 2      Styles and Markup                      1     11   WGS 84\n#> 3       Highlighted Icon                      1     11   WGS 84\n....\nkml = read_sf(\"KML_Samples.kml\", layer = \"Placemarks\")"},{"path":"read-write.html","id":"raster-data-read","chapter":"8 地理データI/O","heading":"8.3.2 ラスタデータ","text":"\nラスタデータは、ベクタデータと同様に多くのファイル形式があり、中にはマルチレイヤファイルをサポートするものもある。\nterra の rast() コマンドは、レイヤが 1 つだけのファイルが提供された場合、1 つのレイヤで読み込む。また、マルチレイヤファイルを読み込む場合にも有効である。\nこれまでの例はすべて、ハードディスクに保存されているファイルから空間情報を読み取るものであった。\nしかし、GDAL は HTTP/HTTPS/FTP の Web リソースなど、オンラインのリソースから直接データを読み込むことも可能である。\nあとは、ファイルへのパスの前に /vsicurl/ というプレフィックスを付けるだけである。\n試しに、2000年から2012年までの 500 m 解像度での全球の月別積雪確率に接続してみよう。\n12月の積雪確率は、COG (Cloud Optimized GeoTIFF) ファイル (Section 8.2) として、zenodo.org に保存されている。\nオンラインファイルを読むには、そのURLと /vsicurl/ プレフィックスを指定するだけである。\n入力データが COG であるため、実際にはこのファイルを RAM に読み込むのではなく、値を取得せずに接続を作成している。\nその値は、何らかの値に基づく操作 (例えば、crop() や extract()) を適用した場合に読み取られる。\nこれにより、ファイル全体をダウンロードすることなく、データのごく一部だけを読み出すこともできるようになった。\n例えば、レイキャビクの座標を指定し、extract() 関数を適用すると、12 月の積雪確率 (70%) を求めることができる。この方法では、大きな GeoTIFF ファイル全体をダウンロードするのではなく、一つの値だけをダウンロードすることになる。\n上記の例は、単純な (しかし有用な) 1 つのケースを示しただけであるが、もっと探求すべきことがある。\nまた、/vsicurl/ のプレフィックスは、ラスタだけでなく、ベクタファイル形式にも有効である。\nベクタファイルのURLの前に接頭辞を付けるだけで、read_sf()、オンラインストレージから直接ベクタを読み込むことができるようになる。重要なのは、GDAL が提供する接頭辞は /vsicurl/ だけではないことである。ZIP アーカイブから空間ファイルを解凍せずに読み込むための /vsizip/ や、AWS S3 バケットにあるファイルをオンザフライで読み込むための /vsis3/ など、他にも多くの接頭辞が存在するのである。\n詳しくは、https://gdal.org/user/virtual_file_systems.html。ベクタデータと同様、ラスタデータも PostGIS などの空間データベースからも読み込むことができる。\n詳細は、Section 10.7 を参照。","code":"\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nsingle_layer = rast(raster_filepath)\nmultilayer_filepath = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nmultilayer_rast = rast(multilayer_filepath)\nmyurl = paste0(\"/vsicurl/https://zenodo.org/record/5774954/files/\",\n               \"clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif\")\nsnow = rast(myurl)\nsnow\n#> class       : SpatRaster \n#> size        : 35849, 86400, 1  (nrow, ncol, nlyr)\n#> resolution  : 0.00417, 0.00417  (x, y)\n#> extent      : -180, 180, -62, 87.4  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#> source      : clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif \n#> name        : clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0\nrey = data.frame(lon = -21.94, lat = 64.15)\nsnow_rey = extract(snow, rey)\nsnow_rey\n#>   ID clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0\n#> 1  1                                                         70"},{"path":"read-write.html","id":"data-output","chapter":"8 地理データI/O","heading":"8.4 データ出力 (O)","text":"地理データの書き込みでは、あるフォーマットから別のフォーマットへの変換や、新しく作成したオブジェクトの保存が可能である。\nデータの種類 (ベクタまたはラスタ)、オブジェクトのクラス (例: sf または SpatRaster)、保存される情報の種類と量 (オブジェクトのサイズ、値の範囲など) に応じて、空間ファイルを最も効率的に保存する方法を知ることが重要である。\n次の 2 つのセクションでは、その方法を説明する。","code":""},{"path":"read-write.html","id":"ベクタデータ","chapter":"8 地理データI/O","heading":"8.4.1 ベクタデータ","text":"read_sf() と対になるのは write_sf() である。\n.geojson、.shp、.gpkg などの最も一般的なものを含む、広範囲の地理ベクタファイル形式に sf オブジェクトを書き込むことができる。\nファイル名から、write_sf() が自動的に使用するドライバを決定する。\nまた、書き込み速度はドライバに依存する。注意: 同じデータソースに再度書き込もうとすると、この機能はファイルを上書きしてしまう。ファイルを上書きする代わりに、引数 layer でファイルに新しいレイヤを追加することができる。\nこれは、GeoPackage を含むいくつかの空間フォーマットでサポートされている。また、write_sf() と同等である、st_write() を使用することもできる。\nただし、デフォルトの挙動は異なる。異なる点は、ファイルを上書きしない (上書きしようとするとエラーを返す)、書き込まれたファイル形式とオブジェクトの短い要約を表示する、などである。layer_options 引数はまた、さまざまな目的で使用することができる。\nそのひとつが、空間データをテキストファイルに書き出すことである。\nこれは、layer_options の中に GEOMETRY を指定することで可能である。\n単純な点データセットの場合は AS_XY (座標のための新しい列を2つ作成する)、より複雑な空間データの場合は AS_WKT (空間オブジェクトのよく知られたテキスト表現を含む新しい列を1つ作成する) のいずれかになる。","code":"\nwrite_sf(obj = world, dsn = \"world.gpkg\")\nwrite_sf(obj = world, dsn = \"world.gpkg\")\nwrite_sf(obj = world, dsn = \"world_many_layers.gpkg\", layer = \"second_layer\")\nst_write(obj = world, dsn = \"world2.gpkg\")\n#> Writing layer `world2' to data source `world2.gpkg' using driver `GPKG'\n#> Writing 177 features with 10 fields and geometry type Multi Polygon.\nwrite_sf(cycle_hire_xy, \"cycle_hire_xy.csv\", layer_options = \"GEOMETRY=AS_XY\")\nwrite_sf(world_wkt, \"world_wkt.csv\", layer_options = \"GEOMETRY=AS_WKT\")"},{"path":"read-write.html","id":"raster-data-write","chapter":"8 地理データI/O","heading":"8.4.2 ラスタデータ","text":"\nwriteRaster() 機能は、SpatRaster のオブジェクトをディスク上のファイルに保存する。\nこの関数は、出力データ型とファイル形式に関する入力を期待するが、選択されたファイル形式に固有の GDAL オプションも受け付ける (詳しくは ?writeRaster を参照)。\nラスタを保存する際、terra パッケージは以下の 7 つのデータ形式を提供する: INT1U、INT2S、INT2U、INT4S、INT4U、FLT4S、FLT8S。44 データ型は、ディスクに書き込まれるラスタオブジェクトのビット表現を決定する (Table 8.5)。\nどのデータ型を使用するかは、ラスタオブジェクトの値の範囲による。\nデータ型が表現できる値が多いほど、ディスク上のファイルサイズは大きくなる。\n符号なし整数 (INT1U、INT2U、INT4U) はカテゴリデータに適しており、浮動小数点数 (FLT4S、FLT8S) は通常連続データを表す。\nwriteRaster() は FLT4S をデフォルトとして使用する。\nこれはほとんどの場合において有効であるが、二値やカテゴリデータを保存する場合、出力ファイルのサイズは不必要に大きくなる。\nしたがって、最小限の記憶容量を必要とし、なおかつすべての値を表現できるデータ型を使用することを勧める (summary() 関数で値の範囲を確認する)。TABLE 8.5: terra パッケージが対応しているデータ型。デフォルトでは、出力ファイル形式はファイル名から導かれる。\nファイル名を *.tif とすると、以下のように GeoTIFF ファイルが作成される。ラスタファイル形式によっては、追加オプションがあり、writeRaster() の options 引数に GDAL parameters を与えることで設定できる。\nGeoTIFF ファイルは、デフォルトで terra で記述され、LZW 圧縮が施されている gdal = c(\"COMPRESS=LZW\")。\n圧縮を変更したり無効にしたりするには、この引数を変更する必要がある。\nさらに、filetype = \"COG\" のオプションでラスタオブジェクトを COG (Cloud Optimized GeoTIFF, Section 8.2 ) として保存することができる。GeoTIFF 形式の圧縮については、Paul Ramsey の GeoTIFF 圧縮についてのブログ に包括的に書かれている。","code":"\nwriteRaster(single_layer, filename = \"my_raster.tif\", datatype = \"INT2U\")\nwriteRaster(x = single_layer, filename = \"my_raster.tif\",\n            gdal = c(\"COMPRESS=NONE\"), overwrite = TRUE)\nwriteRaster(x = single_layer, filename = \"my_raster.tif\",\n            filetype = \"COG\", overwrite = TRUE)"},{"path":"read-write.html","id":"retrieving-data","chapter":"8 地理データI/O","heading":"8.5 オープンデータの取得","text":"\nインターネット上には地理データが膨大かつ増え続け、その多くは無料でアクセス・利用することができる (ただし、提供者のクレジットを適切に表示することが必要)。45\n同じデータセットにアクセスする場所が複数あるという意味で、ある意味、データは多すぎるくらいにある。\n一部のデータセットは品質が低い。\nそこで、最初に最も重要な情報源をいくつか紹介する。\n様々な「ジオポータル」 (地理空間データセットを提供するウェブサービス、Data.gov など) は、幅広いデータを提供しているが、特定の場所についてのみ提供している場合が多い (この話題については、最新の Wikipedia page で説明されている)。\nグローバルなジオポータルの中には、この問題を克服しているものもある。\n例えば、GEOSS portal や Copernicus Data Space Ecosystem には、全世界をカバーするラスタデータセットを多数含んでいる。\nまた、米国航空宇宙局 (NASA) が運営するポータルサイト SEDAC や欧州連合の INSPIRE geoportal から、豊富なベクタデータセットにアクセスすることができ、世界や地域を網羅したデータを入手することができる。ジオポータルは、ほとんどの場合空間的および時間的範囲などの特性に基づいてデータセットを照会できるグラフィカルなインターフェースを提供している。米国地質調査所の EarthExplorer はその代表例である。\nブラウザ上でインタラクティブにデータセットを探索することは、利用可能なレイヤを理解する上で効果的な方法である。\nしかし、データのダウンロードは、再現性と効率性の観点から、コードで行うのがベストである。\nダウンロードは、主に URL や API を経由して、様々な手法でコマンドラインから開始することができる (例: Copernicus APIs を参照)。46\n静的 URL にホストされているファイルは、download.file() でダウンロードすることができる。以下のコードは、pangaea.de から米国の国立公園のデータにアクセスする例である。","code":"\ndownload.file(url = \"https://irma.nps.gov/DataStore/DownloadFile/673366\",\n              destfile = \"nps_boundary.zip\",\n              mode = \"wb\")\nunzip(zipfile = \"nps_boundary.zip\")\nusa_parks = read_sf(dsn = \"nps_boundary.shp\")"},{"path":"read-write.html","id":"geographic-data-packages","chapter":"8 地理データI/O","heading":"8.6 地理データパッケージ","text":"\n地理データにアクセスするための R パッケージが多数開発されており、その一部を Table 8.6 で紹介している。\nこれらのパッケージは、1 つまたは複数の空間ライブラリやジオポータルへのインターフェースを提供し、コマンドラインからのデータアクセスをさらに高速化することを目的としている。\nTABLE 8.6: TABLE 8.7: 地理データ取得パッケージの一部。\nTable 8.6 は、利用可能な地理データパッケージのごく一部に過ぎないことを強調しておく。\nこの他、tidycensus、tigris (USA)、cancensus (Canada)、eurostat、giscoR (European Union) あるいは idbr (international databases) など、様々な社会人口統計を取得する R パッケージが大量に存在している。Analyzing US Census Data (Walker 2022) には、こうしたデータを分析する方法がいくつか例示されている。\n同様に、bcdata (Province British Columbia)、geobr (Brazil)、RCzechia (Czech Republic)、rgugik (Poland) など、様々な地域や国の空間データにアクセスできる R パッケージが存在する。各データパッケージは、データにアクセスするためのコードの書き方がそれぞれ異なる。\nTable 8.6 の 3 つのパッケージについて、違いを確認できるようにデータを取得するコードチャンクを示す。47\nまず、国の境界線はよく使うので、rnaturalearth パッケージ (Massicotte South 2023) の ne_countries() 関数を用いて、以下のようにアクセスしてみよう。国境データは、geodata、giscoR、rgeoboundaries などでも得られる。2 つ目の例は、geodata パッケージを使用して、10 分の空間分解能 (赤道では約 18.5 km) で全球の月別降水量の合計を含む一連のラスタをダウンロードしてみよう (Hijmans 2023a)。\nその結果、SpatRaster クラスのマルチレイヤオブジェクトが生成される。3 つ目の例では、osmdata パッケージ (Padgham et al. 2023) を使って、OpenStreetMap (OSM) データベースから公園を検索してみよう。\n以下のコードチャンクに示すように、クエリは関数 opq() (OpenStreetMap query の略) で始まり、最初の引数は bounding box、またはつまり境界線を表すテキスト文字列 (この場合はリーズ市) である。\nその結果は、どの OSM 要素 (この場合は公園) に興味があるかを選択する関数に渡され、key-value ペアで表される。\n次に、これらのデータは関数 osmdata_sf() に渡され、データのダウンロードと sf オブジェクトのリストへの変換が行われる (詳しくは vignette('osmdata') を参照)。osmdata パッケージの制限は「容量制限」があり、大きな OSM データセット (例えば、大きな都市のすべての OSM データ) をダウンロードすることができない。\nこの制限を克服するために、osmextract パッケージが開発された。これは、あらかじめ定義された地域の OSM データベースの圧縮バージョンを含むバイナリ .pbf ファイルをダウンロードし、インポートすることができる。OpenStreetMap は、クラウドソースによる膨大なグローバルデータベースであり、日々成長を続けている。また、OSM クエリの迅速な開発とテストを行うためのウェブサービス Overpass turbo から PostGIS データベースへのデータ取り込みを行うための osm2pgsql まで、データに容易にアクセスできるツールのエコシステムが充実している。\nOSM から得られるデータセットの質は様々だが、データソースと OSM のエコシステムには多くの利点がある。データセットが世界中で利用でき、無料で、ボランティアの軍隊のおかげで常に改善されている。\nOSM の利用は、「市民科学」とデジタルコモンズへの還元を促すものである (www.openstreetmap.org から、よく知る世界の一部を表すデータの編集を始めることができる)。\nOSM データの活用例については、Chapter 10、Chapter 13、Chapter 14 を参照。パッケージにデータセットが組み込まれていることがある。\nこの場合、アクセス方法は 4 つある。パッケージをアタッチする方法 (パッケージが spData のように ‘lazy loading’ を使用している場合) は data(dataset, package = mypackage)、データセットを参照する方法は mypackage::dataset、生のデータファイルを参照する方法は system.file(filepath, package = mypackage) とする。\n次のコードは、world データセット (親パッケージを library(spData) にアタッチしてロード済み) を使って、後者の2つのオプションを説明している。48最後の例、system.file(\"shapes/world.gpkg\", package = \"spData\") は、spData パッケージの \"shapes/\" フォルダ内に格納されている world.gpkg ファイルへのパスを返す。\n空間情報を得るもう一つの方法は、ジオコーディング (住所などの位置情報を座標に変換すること) である。\nこれは通常、オンラインサービスに問い合わせを行い、その結果として位置情報を取得するものである。\nこのようなサービスは数多く存在するが、使用するジオコーディングの方法、使用制限、コスト、Application Programming Interface (API) キーの要件などが異なっている。\nR にはジオコーディングのためのパッケージがいくつかあるが、tidygeocoder は一貫したインタフェースで最も多くのジオコーディングサービスに接続することができるようである。\ntidygeocoder のメイン関数は geocode で、アドレスを持つデータフレームを受け取り、\"lat\" と \"long\" として座標を追加する。\nまた、この関数は method の引数でジオコーディングサービスを選択することができ、多くの追加パラメータを持つ。このパッケージを使って、London の Soho 地区のビルにある John Snow (訳注: 疫学的手法を導入しコレラの原因、感染経路を初めて特定した医師) の青い銘板の座標を検索してみよう。得られたデータフレームは、st_as_sf() を用いて sf オブジェクトに変換することができる。また、tidygeocoder は、一組の座標に基づいて一連の情報 (名前、住所など) を取得するために使用される逆ジオコーディングと呼ばれる逆の処理を実行することもできる。\nChapter 10 で紹介するように、地理データは地理ソフトのブリッジから R にインポートすることもできる。","code":"\nlibrary(rnaturalearth)\nusa_sf = ne_countries(country = \"United States of America\", returnclass = \"sf\")\nlibrary(geodata)\nworldclim_prec = worldclim_global(\"prec\", res = 10, path = tempdir())\nclass(worldclim_prec)\nlibrary(osmdata)\nparks = opq(bbox = \"leeds uk\") |> \n  add_osm_feature(key = \"leisure\", value = \"park\") |> \n  osmdata_sf()\nworld2 = spData::world\nworld3 = read_sf(system.file(\"shapes/world.gpkg\", package = \"spData\"))\nlibrary(tidygeocoder)\ngeo_df = data.frame(address = \"54 Frith St, London W1D 4SJ, UK\")\ngeo_df = geocode(geo_df, address, method = \"osm\")\ngeo_df\ngeo_sf = st_as_sf(geo_df, coords = c(\"long\", \"lat\"), crs = \"EPSG:4326\")"},{"path":"read-write.html","id":"地理メタデータ","chapter":"8 地理データI/O","heading":"8.7 地理メタデータ","text":"地理メタデータは地理情報管理の要であり、データセット、データ構造、サービスを記述するために使用される。{ちりめたでーた@地理メタデータ}\nメタデータはデータセットを FAIR (Findable, Accessible, Interoperable, Reusable) にするためのもので、ISO/OGC 標準、特に ISO 19115 標準とその基礎となるスキーマによって定義されている。\nこれらの標準は、メタデータカタログを通じて扱われる空間データインフラで広く使用されている。地理メタデータは geometa で管理することができる。geometa は ISO/OGC 標準に従って地理メタデータの書き込み、読み込み、検証ができるパッケージである。\ngeometa は、ISO 19110 (Feature catalogue)、ISO 19115-1 および 19115-2 (Geographic metadata vector gridded/imagery datasets)、ISO 19119 (geographic metadata service)、ISO 19136 (Geographic Markup Language) など、地理メタデータ情報に関するさまざまな国際標準をすでにサポートしており、ISO/TS 19139 (XML) と ISO 19115-3 技術仕様を使って R から地理メタデータを読み込んだり、検証したり、書き込んだりする方法を提供している。\n\n地理メタデータは、geometa で以下のように作成することができ、メタデータファイルを作成して保存する。このパッケージにはさらに多くの例が付属しており、メタデータの管理を容易にし自動化するために、geoflow などのパッケージによって拡張されている。標準的な地理情報管理の分野では、データとメタデータの区別はあまり明確ではない。\n例えば、Geography Markup Language (GML) 標準とファイル形式は、データとメタデータの両方をカバーしている。\ngeometa パッケージでは、sf でモデリングされたジオメトリオブジェクトから GML (ISO 19136) オブジェクトをエクスポートすることができる。\nこのような機能により、地理メタデータの使用 (例えば、単純なバウンディングボックスではなく、詳細な地理的範囲や時間的範囲に関するメタデータを含めることが可能) や、GML 標準を拡張するサービス (例 Open Geospatial Consortium Web Coverage Service, OGC-WCS) の提供が可能になる。","code":"\nlibrary(geometa)\n# メタデータを作成\nmd = ISOMetadata$new()\n#... fill the metadata 'md' object\n# メタデータを検証\nmd$validate()\n# ISOMetadata の XML での表現\nxml = md$encode()\n# save metadata\nmd$save(\"my_metadata.xml\")\n# XML からメタデータを読む\nmd = readISO19139(\"my_metadata.xml\")"},{"path":"read-write.html","id":"geographic-web-services","chapter":"8 地理データI/O","heading":"8.8 地理ウェブサービス","text":"\n空間データにアクセスするための Web API の標準化を目指して、Open Geospatial Consortium (OGC) は、Web サービス (OGC Web Services の略で OWS と総称) の標準仕様を多数策定している。\nこれらのサービスは、ISO/OGC Spatial Schema (ISO 19107:2019) や Simple Features (ISO 19125-1:2004) のような地理情報をモデル化し、Geographic Markup Language (GML)のようなデータをフォーマットするために開発されたコア標準を補完して使用する。\nこれらの仕様は、データとメタデータの一般的なアクセスサービスをカバーしています。\nベクトルデータは、Web Feature Service (WFS)でアクセスでき、グリッド/画像は、Web Coverage Service (WCS)でアクセスできる。\nWeb Feature Service (WFS) や Web Map Tile Service (WMTS) は、タイルのような地図画像にアクセスできる。\nメタデータは、Catalogue Service Web (CSW) によってもカバーされる。\n最後に、標準的な処理は、Web Processing Service (WPS) または Web Coverage Processing Service (WCPS) によって処理される。様々なオープンソースプロジェクトがこれらのプロトコルを採用している。例えば、データハンドリ ングのための GeoServer や MapServer、メタデータハンドリングのための GeoNetwork や PyCSW などがあり、クエリの標準化につながっている。\nGeoNode、GeOrchestra、 Examind のような空間データ基盤 (Spatial Data Infrastructures, SDI) のための統合ツールも、これらの標準ウェブサービスを直接、または前述のオープンソースツールを利用して採用している。他のウェブ API と同様に、OWS API はデータを要求するために ? に続く「ベース URL」と「エンドポイント」および「URL クエリ引数」を使う (httr パッケージ内の best-practices-api-packages vignette を参照)。OWS のサービスへのリクエスト方法はたくさんある。まず、httr パッケージを使った例で、ウェブサービスがどのように機能するかを理解しよう。\n最も基本的なものの 1 つが getCapabilities であり、以下の httr 関数 GET() と modify_url() で示されている。\n以下のコードチャンクは、API のクエリを作成しデータ取得する方法を示している。この場合、国連食糧農業機関 (Food Agriculture Organization, UN-FAO) が運営するサービスの機能を確認することができる。上記のコードチャンクは、API リクエストを GET() 関数でプログラム的に構築する方法を示している。この関数は、ベース URL とクエリパラメータのリストを受け取り、簡単に拡張することができる。\nリクエストの結果を、httr パッケージで定義されたクラス response のオブジェクト res に保存し、URL を含むリクエストの情報を含むリストとなる。\nbrowseURL(res$url) を実行するとわかるように、結果はブラウザで直接読むこともできる。\nリクエストの内容を抽出する一つの方法として、次のようなものがある。WFS サービスからデータをダウンロードするには、GetFeature リクエストと特定の typeName (以下のコードチャンクに示す) が必要である。利用できる名称は、アクセスする Web 機能サービスによって異なる。\nGetCapabilities ウェブ技術を使ってプログラムで抽出することもでき、 (Nolan Lang 2014)、ブラウザで出力された内容を手動でスクロールさせることもできる。データアクセスチェーンに沿ったジオメトリの妥当性を保つため、また、標準やオープンソースのサーバーソリューション (GeoServer など) はシンプルフィーチャアクセスに基づいて構築されているため、sf で導入された新しいデフォルトの動作を無効にし、データアクセス時に S2 ジオメトリモデルを使用しないようにすることが重要である。\nこれは上記のコード sf::sf_use_s2(FALSE) で行う。\nwrite_disk() を使って、結果をメモリにロードされるのではなく、ディスクに書き込むようにすることで、sf でインポートできるようにすることに注意しておこう。しかし、多くの日常的な作業には、より高レベルのインターフェースの方が適している場合があり、この目的のために多くのRパッケージやチュートリアルが開発されている。\nOWS サービスを利用する ows4R というパッケージが開発された。\nWFS、データ用の WCS、メタデータ用の CSW、処理用の WPS など、一般的なアクセスサービスへの安定したインタフェースを提供する。\nOGC のサービスカバレッジは github.com/eblondel/ows4R にあるパッケージの README に記載されており、新しい標準プロトコルは調査/開発中である。上記の例に基づいて、このパッケージで getCapabilities と getFeatures の操作を実行する方法を以下のコードに示す。\nows4R パッケージはクライアントの原理に依存している。\nOWSサービス (WFS など) と対話するために、以下のようにクライアントを作成する。例えば、getCapabilities や getFeatures などである。先に説明したように、OGC サービスでデータにアクセスする場合、sf 機能を扱うには、sf で導入された新しいデフォルトの動作を sf::sf_use_s2(FALSE) で無効にする必要がある。\nこれは ows4R でデフォルトで行われる。vignette にも例がある。たとえば、access raster data WCS や access metadata CSW を参照。","code":"\nlibrary(httr)\nbase_url = \"https://www.fao.org\"\nendpoint = \"/fishery/geoserver/wfs\"\nq = list(request = \"GetCapabilities\")\nres = GET(url = modify_url(base_url, path = endpoint), query = q)\nres$url\n#> [1] \"https://www.fao.org/fishery/geoserver/wfs?request=GetCapabilities\"\ntxt = content(res, \"text\")\nxml = xml2::read_xml(txt)\nxml\n#> {xml_document} ...\n#> [1] <ows:ServiceIdentification>\\n  <ows:Title>GeoServer WFS...\n#> [2] <ows:ServiceProvider>\\n  <ows:ProviderName>UN-FAO Fishe...\n#> ...\nlibrary(sf)\nsf::sf_use_s2(FALSE)\nqf = list(request = \"GetFeature\", typeName = \"fifao:FAO_MAJOR\")\nfile = tempfile(fileext = \".gml\")\nGET(url = base_url, path = endpoint, query = qf, write_disk(file))\nfao_areas = read_sf(file)\nlibrary(ows4R)\nWFS = WFSClient$new(\n  url = \"https://www.fao.org/fishery/geoserver/wfs\",\n  serviceVersion = \"1.0.0\",\n  logger = \"INFO\"\n)\nlibrary(ows4R)\ncaps = WFS$getCapabilities()\nfeatures = WFS$getFeatures(\"fifao:FAO_MAJOR\")"},{"path":"read-write.html","id":"visual-outputs","chapter":"8 地理データI/O","heading":"8.9 ビジュアル出力","text":"\nR は、多くの静的および対話的なグラフィックス形式をサポートしている。\n静的プロットを保存する最も一般的な方法は、例えばグラフィックデバイスを開き、プロットを作成し、それを閉じることである。この他の利用可能なグラフィックデバイスには、pdf()、bmp()、jpeg()、tiff() がある。\n出力プロットの幅、高さ、解像度など、いくつかのプロパティを指定することができる。\nさらに、いくつかのグラフィックパッケージは、グラフィック出力を保存するための独自の関数を提供している。\n例えば、tmap パッケージには、tmap_save() という関数がある。\nオブジェクト名と新規ファイルへのファイルパスを指定することで、tmap オブジェクトをさまざまなグラフィックフォーマットまたは HTML ファイルに保存することができる。一方、mapview パッケージで作成したインタラクティブ地図は、mapshot2() 関数を使用して HTML ファイルまたは画像として保存することができる。","code":"\npng(filename = \"lifeExp.png\", width = 500, height = 350)\nplot(world[\"lifeExp\"])\ndev.off()\nlibrary(tmap)\ntmap_obj = tm_shape(world) + tm_polygons(col = \"lifeExp\")\ntmap_save(tmap_obj, filename = \"lifeExp_tmap.png\")\nlibrary(mapview)\nmapview_obj = mapview(world, zcol = \"lifeExp\", legend = TRUE)\nmapshot2(mapview_obj, url = \"my_interactive_map.html\")"},{"path":"read-write.html","id":"演習-6","chapter":"8 地理データI/O","heading":"8.10 演習","text":"E1. ベクタ、ラスタ、地理データベースの形式を 3 つ挙げて説明しなさい。E2. sf 関数 read_sf() と st_read() の違いを 2 つ以上述べなさい。E3. パッケージ spData から cycle_hire_xy.csv ファイルを空間オブジェクトとして読みこみなさい (ヒント: misc フォルダにある)。\n読み込んだオブジェクトのジオメトリ型は何か?E4. rnaturalearth を使ってドイツの国境をダウンロードし、germany_borders というオブジェクトを作りなさい。\nこのオブジェクトを GeoPackage 形式のファイルに書き込みなさい。E5. geodata パッケージを用い、世界の月毎の最低気温を、空間解像度 5 分でダウンロードしなさい。\n6 月の値を抽出し、tmin_june.tif というファイルに保存しなさい (ヒント: terra::subset() を使う)。E6. ドイツの国境の性的地図を作成し、PNG ファイルとして保存しなさい。E7. cycle_hire_xy.csv ファイルのデータを使ってインタラクティブ地図を作りなさい。\nこの地図を cycle_hire.html に書き出しなさい。","code":""},{"path":"adv-map.html","id":"adv-map","chapter":"9 R で地図を作成","heading":"9 R で地図を作成","text":"","code":""},{"path":"adv-map.html","id":"prerequisites-09","chapter":"9 R で地図を作成","heading":"必須パッケージ","text":"この章では、すでに使用している以下のパッケージが必要である。本章での主要なパッケージは tmap である。\nCRAN よりも頻繁に更新されている r-universe 版をお勧めする。訳註: macOS では、tmap は文字化けすることがある。macOS で ragg を使用することで文字化けが解消する。以下の可視化に関するパッケージを使用する (動的な地図アプリを開発したい場合は、shiny もインストールしよう)。Section 4.3 については、以下の二つのデータセットを読み込む必要がある。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\ninstall.packages(\"tmap\", repos = c(\"https://r-tmap.r-universe.dev\",\n                                   \"https://cloud.r-project.org\"))\nlibrary(tmap)    # 静的地図と動的地図\nlibrary(leaflet) # 動的地図\nlibrary(ggplot2) # tidyverse データ可視化パッケージ\nnz_elev = rast(system.file(\"raster/nz_elev.tif\", package = \"spDataLarge\"))"},{"path":"adv-map.html","id":"introduction-09","chapter":"9 R で地図を作成","heading":"9.1 イントロダクション","text":"地理学的研究の満足度と重要性は、その結果を伝えることにある。\n地図作成は、コミュニケーションと細部への注意、そして創造力を必要とする古来の技術である。\nR における静的地図は、Section 2.2.3 で見たように、plot() 関数を使えば簡単にできる。\nR の基本メソッドを使って高度な地図を作成することも可能ではある (Murrell 2016)。\nしかし、この章の焦点は、専用の地図作成パッケージを使った地図作成にある。\n新しいスキルを身につけるには、1 つの分野の知識を深めてから手を広げていくことが大切である。\n地図の作成も例外ではない。そのため、この章では多くのパッケージを表面的にではなく、1 つのパッケージ (tmap) を深く掘り下げて説明する。地図作成は、楽しくてクリエイティブなだけでなく、実用的にも重要な役割を担っている。\n丁寧に作られた地図は、仕事の成果を伝えるのに最適な方法であるが、デザインの悪い地図は印象を悪くすることがある。\nよくあるデザイン上の問題点としては、Journal Maps のスタイルガイドで説明されているように、文字の配置やサイズ、読みにくさ、色の選び方の不注意などが挙げられる。\nさらに、地図作りが不十分だと、結果の伝達にも支障が生じることがある (Brewer 2015)。素人が作ったような地図では、情報の重要性が伝わらず、専門家によるデータ調査をうまく表現できないことがある。\n地図は数千年前からさまざまな用途に使われてきた。\n歴史的な例としては、3000年以上前の古バビロニア王朝の建物や土地所有の地図、約2000年前のプトレマイオスの代表作地理学の世界地図などがある (Talbert 2014)。地図は、歴史的にエリートが自分のために作るか、あるいはエリートのために誰かが作る行為であった。\nしかし、R パッケージの tmap や QGIS の印刷レイアウトのようなオープンソースの地図作成ソフトが登場し、誰でも高品質の地図を作ることができるようになり、「市民科学」が可能になったことで、状況は一変した。\nまた、ジオコンピュテーションの研究成果をわかりやすく紹介するためには、地図が最適な方法であることが多い。\nしたがって、地図作成はジオコンピュテーションの重要な一部であり、世界を記述するだけでなく、世界を変えることにも重点を置いているのである。この章では、さまざまな地図の作り方を紹介する。\n次のセクションでは、美観への配慮、ファセット、差し込み地図 (inset map) など、さまざまな静的地図について説明する。\nまた、Section 9.3 から Section 9.5 では、アニメーションやインタラクティブな地図 (Web 地図や地図アプリケーションを含む) を紹介している。\n最後に、Section 9.6 は、ggplot2 や cartogram など他の地図作成用パッケージを紹介する。","code":""},{"path":"adv-map.html","id":"static-maps","chapter":"9 R で地図を作成","heading":"9.2 静的地図","text":"\nジオコンピュテーションの視覚的な出力として最も一般的なのが静的地図であろう。\n標準的なフォーマットとしては、ラスタ出力用に .png、ベクタ出力用に .pdf がよく用いられる。\n当初、R が作成できる地図の種類は静的地図だけだった。\nsp (Pebesma Bivand 2005 参照) のリリースで状況が進展し、その後、地図作成のための多くの技術、関数、パッケージが開発された。\nしかし、インタラクティブ地図がどんどん発明されてきたにもかかわらず、10 年経っても R では依然として静的なプロットが地理データの可視化の重点となっていた (Cheshire Lovelace 2015)。ジェネリック関数の plot() 関数は、ベクタやラスタの空間オブジェクトから静的地図を作成する最速の方法であることが多い (Section 2.2.3 と Section 2.3.3 の項を参照)。\n特にプロジェクトの開発段階では、シンプルさとスピードが優先され、plot() はこの点で優れている。\nBase R のアプローチは拡張可能で、plot() は何十もの引数を提供している。\nまた、Murrell (2016) の Chapter 14 に示されているように、静的地図の低レベル制御を可能にする grid パッケージもアプローチの 1 つである。\nこの章では、tmap に焦点を当て、重要な美観とレイアウトのオプションに重点を置いている。\ntmap は強力で柔軟な地図作成パッケージで、賢明なデフォルトが設定されている。\n簡潔な構文で、ggplot2 のユーザには馴染みのある最小限のコードで魅力的な地図を作成することができる。\nまた、tmap_mode() を介して、同じコードで静的な地図とインタラクティブな地図を生成するユニークな機能を備えている。\n最後に、(sf オブジェクトと terra オブジェクトを含む) 空間クラスを受け入れることができる点では、ggplot2 などよりも優れている。","code":""},{"path":"adv-map.html","id":"tmap-basics","chapter":"9 R で地図を作成","heading":"9.2.1 tmap の基礎知識","text":"\nggplot2と同様に、tmap は「グラフィックの文法」という考えに基づいている (Wilkinson Wills 2005)。\n各入力データセットは、地図上の位置 (データの geometry で定義)、色、その他の視覚的変数など、さまざまな方法で「地図作成」することができる。\n基本的な構成要素は tm_shape() (入力データ、ベクタまたはラスタのオブジェクトを定義する) で、その後に tm_fill() や tm_dots() などの 1 つまたは複数のレイヤ要素が続く。\n以下のチャンクは、このようなレイヤ構成を示し、Figure 9.1 の地図を生成する。\nFIGURE 9.1: New Zealand の形状を tmap 関数で塗りつぶし (左)、境界 (中)、塗りつぶしと境界 (右) のレイヤを追加してプロット。\nこの場合、tm_shape() に渡されるオブジェクトは nz で、New Zealand の地域を表す sf オブジェクトである (sf オブジェクトについては Section 2.2.1 を参照)。\nnz を視覚的に表現するためにレイヤを追加し、tm_fill() と tm_borders() でそれぞれ Figure 9.1 の陰影部分 (左図) と枠線 (中図) を作成している。\nこれは、直感的な地図作りの手法である。\n新しいレイヤを追加する一般的なタスクは、追加演算子 + とそれに続く tm_*() によって引き受けられる。\nアスタリスク(*)は、以下のように名前から明らかなレイヤを指す。tm_fill(): (複合) ポリゴンの塗りつぶしtm_borders(): (複合) ポリゴンの境界線tm_polygons(): (複合) ポリゴンの塗りつぶしと境界線tm_lines(): (複合) 線の線tm_symbols(): (複合) 点、(複合) 線、(複合) ポリゴンのシンボルtm_raster(): ラスタデータの色付きのセル (３レイヤのあるラスタには tm_rgb() もある)tm_text(): (複合) 点、(複合) 線、(複合) ポリゴンのテキストFigure 9.1 の右側のパネルでは、塗りつぶし (fill) レイヤの上に境界 (borders) を重ねた結果を示す。","code":"\n# nz shape に塗りつぶしレイヤを追加\ntm_shape(nz) +\n  tm_fill() \n# nz shape に境界レイヤを追加\ntm_shape(nz) +\n  tm_borders() \n# nz shape に塗りつぶしと境界レイヤを追加\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders() "},{"path":"adv-map.html","id":"map-obj","chapter":"9 R で地図を作成","heading":"9.2.2 地図オブジェクト","text":"tmap の便利な点は、地図を表すオブジェクトを格納できることである。\n以下のコードは、Figure 9.1 の最後のプロットをクラス tmap のオブジェクトとして保存することでこれを示している (tm_fill() + tm_borders() を単一の関数に凝縮した tm_polygons() の使用に注意してみよう)。map_nz は後でプロットすることができる。例えば、レイヤを追加したり (下図参照)、コンソールで map_nz を実行するだけで、print(map_nz) と同じ意味になる。新しい shape は、+ tm_shape(new_obj) で追加することができる。\nこの場合、new_obj は、先行するレイヤの上にプロットされる新しい空間オブジェクトを表す。\nこのようにして新しい形状が追加されると、次の新しい形状が追加されるまで、それ以降のすべての美観機能はその形状を参照する。\nこの構文により、複数の形状やレイヤを持つ地図を作成することができる。次のコードでは、関数 tm_raster() を使ってラスタレイヤ (レイヤを半透明にするために col_alpha を設定している) を描画している様子を示している。先に作成した map_nz オブジェクトをベースに、新しい地図オブジェクト map_nz1 を作成する。このオブジェクトには、New Zealand 全土の平均標高を表す別の図形 (nz_elev) が含まれている (Figure 9.2 左図)。\nさらに図形やレイヤを追加することもできる。以下のコードでは、New Zealand の領海を表す nz_water を作成し、作成した線を既存の地図オブジェクトに追加している。tmap オブジェクトに追加できるレイヤやシェイプの数に制限はない。同じシェイプを複数回使用することも可能である。\nFigure 9.2 に示される最終的な地図は、先に作成された map_nz2 オブジェクトに tm_dots() で高ポイントを表すレイヤ (オブジェクト nz_height に格納) を追加して作成される (tmap のポイントプロット機能の詳細については ?tm_dots と ?tm_bubbles を参照)。\nその結果、4つのレイヤを持つ地図ができあがり、Figure 9.2 の右側のパネルに示されている。\n便利だがあまり知られていない tmap の機能として、tmap_arrange() がある。これは、複数の地図オブジェクトを一つの「メタプロット」に配置することができる。\nFigure 9.2は、map_nz1 から map_nz3 までをメタプロットしている例である。\nFIGURE 9.2: Figure 9.1 の最終地図にレイヤを追加した地図。\nまた、+ 演算子でさらに要素を追加することができる。\nただし、美観の設定は、レイヤ関数の引数で制御する。","code":"\nmap_nz = tm_shape(nz) + tm_polygons()\nclass(map_nz)\n#> [1] \"tmap\"\nmap_nz1 = map_nz +\n  tm_shape(nz_elev) + tm_raster(col_alpha = 0.7)\nnz_water = st_union(nz) |>\n  st_buffer(22200) |> \n  st_cast(to = \"LINESTRING\")\nmap_nz2 = map_nz1 +\n  tm_shape(nz_water) + tm_lines()\nmap_nz3 = map_nz2 +\n  tm_shape(nz_height) + tm_symbols()\ntmap_arrange(map_nz1, map_nz2, map_nz3)"},{"path":"adv-map.html","id":"visual-variables","chapter":"9 R で地図を作成","heading":"9.2.3 可視化の変数","text":"\n前節のプロットは、tmap のデフォルトの美観セッティングを示してきた。\ntm_fill() と tm_symbols() のレイヤには灰色の影を使用し、tm_lines() で作成した線を表現するために、連続した黒い線を使用する。\nもちろん、これらのデフォルト値やその他の美観はオーバーライドすることができる。\nこのセクションの目的は、その方法を示すことである。地図の美観には、大きく分けて「データによって変化するもの」と「一定であるもの」がある。\nヘルパー関数 aes() を使って変数の美観を表現する ggplot2 とは異なり、 tmap はレイヤの種別に応じた美観の引数を直接受け付ける。fill: ポリゴンの塗りつぶし色col: ポリゴン境界線、線、点、ラスタの色lwd: 線幅lty: 線種size: シンボルの大きさshape: シンボルの形さらに、塗りつぶしと線の色の透過率を fill_alpha col_alpha で指定することができる。美観に変数に応じて変化させるには、対応する引数に列名を渡す。美観を固定するには、希望の値を渡す。49\n固定値を設定した例を Figure 9.3 に示す。\nFIGURE 9.3: よく使われる塗りつぶしや枠線の美観を固定値に変更した場合の影響。\nBase R のプロットと同様に、美観を定義する引数もまた、様々な値を受け取ることができる。\n以下の Base R コード (Figure 9.4 の左のパネルを生成) とは異なり、tmap 美観引数は数値ベクタを受け付けない。代わりに、fill (および、ラインレイヤのための lwd、ポイントレイヤのための size など、異なることがある他の美観) は、プロットされるジオメトリに関連する属性を指定する文字列を必要とする。\nしたがって、次のように望ましい結果を得ることができる (Figure 9.4 右図)。\nFIGURE 9.4: 数値色フィールドの Base (左) と tmap (右) の処理方法の比較。\n視覚化の変数には、.scale、.legend、.free という文字列を後ろにつけた 3 つの追加引数がある。\n例えば、tm_fill() には fill、fill.scale、fill.legend、fill.free といった引数がある。\n.scale 引数は、地図と凡例での表示方法を指定し (Section 9.2.4)、.legend はタイトル、方向、位置を指定する (Section 9.2.5)。\n.free 引数は、多くのファセットをもつ地図で、ファセットによって縮尺や凡例が変わる場合などに使用する。","code":"\nma1 = tm_shape(nz) + tm_polygons(fill = \"red\")\nma2 = tm_shape(nz) + tm_polygons(fill = \"red\", alpha = 0.3)\nma3 = tm_shape(nz) + tm_polygons(col = \"blue\")\nma4 = tm_shape(nz) + tm_polygons(lwd = 3)\nma5 = tm_shape(nz) + tm_polygons(lty = 2)\nma6 = tm_shape(nz) + tm_polygons(fill = \"red\", fill_alpha = 0.3,\n                                 col = \"blue\", lwd = 3, lty = 2)\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\nplot(st_geometry(nz), col = nz$Land_area)  # 成功\ntm_shape(nz) + tm_fill(fill = nz$Land_area) # 失敗\n#> Error: palette should be a character value\ntm_shape(nz) + tm_fill(fill = \"Land_area\")"},{"path":"adv-map.html","id":"scales","chapter":"9 R で地図を作成","heading":"9.2.4 スケール (scale)","text":"\nここでいうスケールとは、縮尺のことではなく、地図と凡例で値がどのように表示されるかを制御することである。\n例えば、col という変数の場合、col.scale が空間オブジェクトの色が値にどう対応するかを制御する。size という変数の場合、size.scale は大きさが値にどう対応するかを制御する。\nデフォルトでは、tm_scale() が使われ、入力データ種別 (因子型、数値型、整数型) によって自動的に設定を選択する。\nポリゴンの塗りつぶしの設定によってスケールがどのように機能するか見てみよう。\nカラー設定は、地図デザインの重要な要素である。Figure 9.5 に示すように、空間変動の描き方に大きな影響を与える可能性がある。\nこれは、New Zealand の地域を中央値によって色分けする 4 つの方法を、左から右に示している (下のコードチャンクでも示している)。デフォルトの設定では、次の段落で説明する ‘pretty’ 区切りが使用されるbreaks では、手動で区切りを設定することができるn は、数値変数を分類するビンの数を設定するpalette は配色を定義するもので、例えば BuGn\nFIGURE 9.5: 色設定。結果は (左から) デフォルト設定、手動区切り、n 区切り、パレットを変更した場合の結果を示している。\n\ntm_scale_ から始まる関数ファミリーを使うことで縮尺をカスタマイズすることもできる。\n最も重要なものは、tm_scale_intervals()、tm_scale_continuous()、tm_scale_categorical() である。\ntm_scale_intervals() 関数は、入力データ値を間隔セットに分割する。\nbreaks を手動で設定する代わりに、style 引数で tmap に自動的にブレイクを作成するアルゴリズムを選択することもできる。\nデフォルトは tm_scale_intervals(style = \"pretty\") で、可能な限り整数に丸めて均等の間隔にする。\nその他のオプションは、Figure 9.6 に示している。style = \"equal\": 入力値を同じ範囲のビンに分割し、一様な分布を持つ変数に適している (結果の地図が色の多様性に乏しくなる可能性があるため、歪んだ分布を持つ変数には推奨されていない)。style = \"quantile\": 同じ数の観測が各カテゴリに入ることを保証する (ビンの範囲が広く変化する可能性があるというマイナス面を含む)。style = \"jenks\": データ中の類似した値のグループを識別し、カテゴリ間の差異を最大化する。style = \"log10_pretty\": 右に裾が広がっている分布の変数に使われる、pretty スタイルの対数版 (底は 10)\nFIGURE 9.6: tmap の style 引数で設定するビン方法の違い。\n\ntm_scale_continuous() 関数は、連続色フィールドの色を提示する。連続ラスタによく用いられる (Figure 9.7 左図)。\n分布が偏っている場合に対して、tm_scale_continuous_log() や tm_scale_continuous_log1p() といった派生もある。\n最後に、tm_scale_categorical() は、カテゴリ値を代表し、各カテゴリに固有の色が割り当てられる (Figure 9.7 右図)。\nFIGURE 9.7: tmap における連続スケールとカテゴリスケール\n\nパレットは、ビンに関連付けられ、前述の breaks、n、style 引数で決定される色域を定義する。\nこの引数には色ベクトルまたは新しい色パレット名を与えるが、tmaptools::palette_explorer() で対話的に選択することができる。\nプレフィックスとして - を付けると、パレットの順序を逆にすることができる。\n色パレットは大きく分けて、カテゴリ、連続、発散の三種類ある (Figure 9.8)。目的に応じてこの三種類を使い分ける。50\nカテゴリパレットは、区別しやすい色で構成されており、州名や土地被覆クラスなど、特定の順序を持たないカテゴリデータに最適である。\n色は直感的にわかるように、例えば川は青、牧草地は緑にする。\nカテゴリを増やしすぎると、大きな凡例や多くの色を使った地図は理解できないことがある。51次に、連続パレットをみてみよう。\n連続パレットは、例えば明るい色から暗い色への勾配に沿っており (明るい色は低い値を表す傾向がある)、連続的な (数値) 変数に適している。\n連続パレットは、以下のコードで示すように、単色 (例えば、greens は明るいから暗い緑へ) または多色/色相 (例えば、yl_gn_bu は明るい黄色から緑色を経て青色へのグラデーション) である (出力は示していない)。結果を見るために自分でコードを実行してみよう。3 番目のパレットである発散パレットは、通常 3 色の間 (紫-白-緑: Figure 9.8) で、通常 2 つの単色パレットの両端を濃い色で結合して作成される。\nその主な目的は、ある気温、世帯収入の中央値、干ばつイベントの平均確率など、重要な基準点からの差異を可視化することである。\n参照点の値は、midpoint の引数を用いて tmap で調整することができる。\nFIGURE 9.8: カテゴリ、連続色、発散のパレットの例。\n色を扱う際に考慮すべき重要な原則は、「知覚可能性」と「アクセシビリティ」の 2 つである。\nまず、地図の色は感覚と合っていなければならない。\n特定の色は、経験や文化的なレンズを通して見ることができる。\n例えば、緑は植物や低地を表し、青は水や涼しさを連想させる色である。\nまた、情報を効果的に伝えるために、色パレットは分かりやすいものが望ましい。\nどの数値が低く、どの数値が高いかが明確で、色も徐々に変化することが望ましい。\n第二に、色の変化は、多くの人がアクセスできるものでなければならない。\nそのため、色弱者用のパレットをできるだけ多く使うことが大切である。52","code":"\ntm_shape(nz) + tm_polygons(fill = \"Median_income\")\ntm_shape(nz) + tm_polygons(fill = \"Median_income\",\n                           fill.scale = tm_scale(breaks = c(0, 30000, 40000, 50000)))\ntm_shape(nz) + tm_polygons(fill = \"Median_income\",\n                           fill.scale = tm_scale(n = 10))\ntm_shape(nz) + tm_polygons(fill = \"Median_income\",\n                           fill.scale = tm_scale(values = \"BuGn\"))\ntm_shape(nz) + \n  tm_polygons(\"Median_income\", fill.scale = tm_scale(values = \"greens\"))\ntm_shape(nz) + \n  tm_polygons(\"Median_income\", fill.scale = tm_scale(values = \"yl_gn_bu\"))\ntm_shape(nz) + \n  tm_polygons(\"Median_income\",\n              fill.scale = tm_scale_continuous(values = \"pu_gn_div\", \n                                               midpoint = 28000))"},{"path":"adv-map.html","id":"legends","chapter":"9 R で地図を作成","heading":"9.2.5 凡例","text":"\n美観変数と設定を決定したのち、地図の凡例スタイルに注意を向けよう。\ntm_legend() 関数を使うと、タイトル、位置、方向を変えたり、あるいは非表示することができる。\n最も重要なのはタイトルで、凡例のタイトルとなる。\n一般に、タイトルには二つの情報を記述する。一つ目は内容で、二つ目は値の単位である。\n以下のコードは、変数名 Land_area よりも魅力的な名前を提供することで、この機能を示している (expression() は上付き文字を設定するために使用)。tmap の凡例方向はデフォルトでは縦長 \"portrait\" であるが、\"landscape\" で横長とすることもできる。\n凡例の位置は、position 引数で設定する。凡例の位置 (およびその他の tmap の地図要素の位置) は、関数でカスタマイズできる。\n最も重要なものを二つ紹介する。tm_pos_out(): これがデフォルトで、凡例を図郭の外に配置する。\n位置については、横方向 (\"left\"、\"center\"、\"right\") と縦方向 (\"bottom\"、\"center\"、\"top\") で指定する。tm_pos_in(): は、図郭内に配置する。\n最初の引数は \"left\"、\"center\"、\"right\" のいずれかで、2 番目の引数は \"bottom\"、\"center\"、\"top\" のいずれかである。または、2 つの値のベクトル (または 0 から 1 の 2 つの値) を与えても良い。この場合、凡例は図郭内に配置される。","code":"\nlegend_title = expression(\"Area (km\"^2*\")\")\ntm_shape(nz) +\n  tm_polygons(fill = \"Land_area\", fill.legend = tm_legend(title = legend_title))\ntm_shape(nz) +\n  tm_polygons(fill = \"Land_area\",\n              fill.legend = tm_legend(title = legend_title,\n                                      orientation = \"landscape\",\n                                      position = tm_pos_out(\"center\", \"bottom\")))"},{"path":"adv-map.html","id":"layouts","chapter":"9 R で地図を作成","heading":"9.2.6 レイアウト","text":"\n地図レイアウトとは、すべての地図要素を組み合わせて、まとまりのある地図にすることである。\n地図要素には、マップされるオブジェクト、地図グリッド (メッシュ)、縮尺バー、タイトル、マージンなどがあり、前のセクションで説明したカラー設定は、地図の見え方に影響を与えるパレットとブレークポイントに関連している。\nどちらも微妙な変化をもたらすだろうが、地図が残す印象には同じように大きな影響を与える。経緯線網、方位記号、スケールバー、タイトルなどの整飾には、それぞれ tm_graticules()、tm_compass()、tm_scalebar()、tm_title() といった関数がある (Figure 9.9)。53\nFIGURE 9.9: 方位記号とスケールバーを追加した地図。\nまた、tmap では、様々なレイアウト設定を変更することができる。その一部を、以下のコードで作成し、Figure 9.10 に図示している (全リストは、args(tm_layout) または ?tm_layout を参照)。\nFIGURE 9.10: レイアウトオプションは、(左から) scale、bg.color、frame の各引数で指定。\ntm_layout() の他の引数は、地図が配置されるキャンバスとの関係で、地図の多くの側面を制御する。\nここでは、便利なレイアウト設定をご紹介する (一部、Figure 9.11)。inner.margin と outer.margin はマージンを設定fontface で制御されるフォント設定と fontfamily凡例設定には、legend.show (凡例を表示すかどうか)、legend.(地図を省略するか)、legend.outside (凡例を地図の外に出すか) などの二値オプションや、legend.position ですべて設定図郭の幅 (frame.lwd) と二重線 (frame.double.line) を許可するオプションsepia.intensity (地図のセピア度合) と saturation (色・グレースケール) を制御する色設定\nFIGURE 9.11: 選択したレイアウトオプション。\n","code":"\nmap_nz + \n  tm_graticules() +\n  tm_compass(type = \"8star\", position = c(\"left\", \"top\")) +\n  tm_scalebar(breaks = c(0, 100, 200), text.size = 1, position = c(\"left\", \"top\")) +\n  tm_title(\"New Zealand \", fontfamily = \"HiraginoSans-W3\")\nmap_nz + tm_layout(scale = 4, fontfamily = \"HiraginoSans-W3\")\nmap_nz + tm_layout(bg.color = \"lightblue\")\nmap_nz + tm_layout(frame = FALSE)"},{"path":"adv-map.html","id":"faceted-maps","chapter":"9 R で地図を作成","heading":"9.2.7 ファセット地図","text":"\nファセット地図は「スモール・マルチプル」とも呼ばれ、多数の地図を横に並べ、時には縦に重ねて構成する (Meulemans et al. 2017)。\nファセットは、空間的な関係が時間などの別の変数に対してどのように変化するかを視覚化することができる。\n例えば、集落の人口の変化を、各パネルが特定の時点の人口を表すファセット地図で表現することができる。\n時間の次元は、色などの別の視覚化に関する変数で表現できる。\nしかし、これは複数のポイントが重なるため、地図が乱雑になる危険性がある (都市は移動しない！)。ファセット地図では、一つのジオメトリ・データの属性データのそれぞれの列に対して一つのファセットとなる (sf オブジェクトのデフォルトのプロット方法、Chapter 2 を参照)。\nまた、ファセットを用いて、点パターンの経時変化など、ジオメトリの変化を表現することもできる。\nこのファセット化されたプロットの使用例を Figure 9.12 に示す。\nFIGURE 9.12: 国連による人口予測に基づき、1970年から2030年までの都市集積の上位30位までを示したファセット地図。\nこのコードでは、tmap で作成されたファセット地図の主要な特徴を示している。ファセット変数を持たないシェイプは繰り返される (この場合、world の国々)変数によって変化する の引数 (この場合は \"year\")nrow / ncol ファセットが配置される行と列の数を指定あるいは、tm_facets_grid() 関数を使って 3 変数 rows、columns、pages からファセットを作る。ファセット地図は、変化する空間的関係を示すのに有用なだけでなく、地図アニメーションの基礎としても有用である (Section 9.3 参照)。","code":"\nurb_1970_2030 = urban_agglomerations |> \n  filter(year %in% c(1970, 1990, 2010, 2030))\n\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(fill = \"black\", col = \"white\", size = \"population_millions\") +\n  tm_facets_wrap(by = \"year\", nrow = 2)"},{"path":"adv-map.html","id":"差し込み地図","chapter":"9 R で地図を作成","heading":"9.2.8 差し込み地図","text":"\n差し込み地図とは、メイン地図の中や横に描画される小さな地図のことである。\nコンテキストを提供したり (Figure 9.13)、非連続な領域を接近させて比較を容易にしたり (Figure 9.14)、様々な目的を果たすことが可能である。\nまた、より小さなエリアに焦点を当てたり、地図と同じエリアを別のトピックでカバーしたりすることもできる。下の例では、New Zealand の南アルプスの中央部の地図を作成している。\n差し込み地図は、メイン地図が New Zealand 全体に対してどのような位置にあるかを示すものである。\n最初のステップは、関心のある領域を定義することである。これは、新しい空間オブジェクト nz_region を作成することで可能である。ステップ 2 では、New Zealand の南アルプス周辺を示すベース地図を作成する。\nここは、最も重要なメッセージが述べられている場所である。ステップ 3 で、差し込み地図を作成する。\n差し込み地図はコンテキストを示し、関心のある領域を特定するのに役立つ。\n差し込み地図には境界線を記載するなどして、メイン地図の位置を明確に示す必要がある。散布図など通常のグラフと地図の違いの一つとして、入力データは地図のアスペクト比を決定する。\nよって、この場合、nz_region と nz という二つのデータセットのアスペクト比を計算する必要がある。\nnorm_dim() 関数は、オブジェクトの幅 (\"w\") と高さ (\"h\") を正規化する (画像デバイスは \"snpc\" の単位を理解する)。アスペクト比を得て、viewport() 関数を使い、二つの地図 (主地図と差し込み地図) の大きさと位置を指定する。\nviewport とは、ある瞬間のグラフィック要素を描画するために使用するグラフィックデバイスの一部である。\n私たちのメイン地図の viewport は、ちょうどそのアスペクト比の表現である。差し込み地図の表示領域は、大きさと位置を指定する必要がある。\nここでは、メイン地図の半分の大きさにするために幅と高さに 0.5 をかけ、メイン地図フレームの右下から 0.5 cm の位置に配置する。最後に、新規にキャンバスを作り、メイン地図を表示し、メイン地図の領域内に差し込み地図を配置する。\nFIGURE 9.13: 差し込み地図で背景を説明 - New Zealand の南アルプスの中央部の位置。\n差し込み地図を保存するには、グラフィックデバイス (Section 8.9 参照) を使うか、tmap_save() 関数に引数 insets_tm および insets_vp を設定する方法がある。また、差し込み地図は、非連続なエリアを 1 つの地図にするために使用する。\nよく使われる例はアメリカ合衆国の地図で、アメリカ本土とハワイ、アラスカで構成されている。\nこのようなケースでは、個々の差し込みに最適なプロジェクションを見つけることが非常に重要である (詳しくは Chapter 7 を参照)。\ntm_shape() の引数 crs に US National Atlas Equal Area の EPSG コードを指定すれば、米国本土の地図に US National Atlas Equal Area を使用することができる。残りのオブジェクト hawaii と alaska は、すでに適切な投影を持っている。したがって、2 つの地図を別々に作成するだけでよい。これら 3 つの地図を組み合わせ、サイズを調整し配置することで、最終的な地図ができあがる。\nFIGURE 9.14: アメリカ合衆国の地図。\n上記で紹介したコードはコンパクトで、他の差し込み地図のベースとして使用することができる。ただし、Figure 9.14 ではハワイとアラスカの位置とサイズがうまく表現されていないことがわかる。\nより詳細なアプローチについては、geocompkg の us-map vignette を参照。","code":"\nnz_region = st_bbox(c(xmin = 1340000, xmax = 1450000,\n                      ymin = 5130000, ymax = 5210000),\n                    crs = st_crs(nz_height)) |> \n  st_as_sfc()\nnz_height_map = tm_shape(nz_elev, bbox = nz_region) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"YlGn\"),\n            col.legend = tm_legend(position = c(\"left\", \"top\"))) +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 1) +\n  tm_scalebar(position = c(\"left\", \"bottom\"))\nnz_map = tm_shape(nz) + tm_polygons() +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 0.1) + \n  tm_shape(nz_region) + tm_borders(lwd = 3) +\n  tm_layout(bg.color = \"lightblue\")\nlibrary(grid)\nnorm_dim = function(obj){\n    bbox = st_bbox(obj)\n    width = bbox[[\"xmax\"]] - bbox[[\"xmin\"]]\n    height = bbox[[\"ymax\"]] - bbox[[\"ymin\"]]\n    w = width / max(width, height)\n    h = height / max(width, height)\n    return(unit(c(w, h), \"snpc\"))\n}\nmain_dim = norm_dim(nz_region)\nins_dim = norm_dim(nz)\nmain_vp = viewport(width = main_dim[1], height = main_dim[2])\nins_vp = viewport(width = ins_dim[1] * 0.5, height = ins_dim[2] * 0.5,\n                  x = unit(1, \"npc\") - unit(0.5, \"cm\"), y = unit(0.5, \"cm\"),\n                  just = c(\"right\", \"bottom\"))\ngrid.newpage()\nprint(nz_height_map, vp = main_vp)\npushViewport(main_vp)\nprint(nz_map, vp = ins_vp)\nus_states_map = tm_shape(us_states, crs = \"EPSG:9311\") + \n  tm_polygons() + \n  tm_layout(frame = FALSE)\nhawaii_map = tm_shape(hawaii) +\n  tm_polygons() + \n  tm_title(\"Hawaii\") +\n  tm_layout(frame = FALSE, bg.color = NA, \n            title.position = c(\"LEFT\", \"BOTTOM\"))\nalaska_map = tm_shape(alaska) +\n  tm_polygons() + \n  tm_title(\"Alaska\") +\n  tm_layout(frame = FALSE, bg.color = NA)\nus_states_map\nprint(hawaii_map, vp = grid::viewport(0.35, 0.1, width = 0.2, height = 0.1))\nprint(alaska_map, vp = grid::viewport(0.15, 0.15, width = 0.3, height = 0.3))"},{"path":"adv-map.html","id":"animated-maps","chapter":"9 R で地図を作成","heading":"9.3 地図アニメーション","text":"\nSection 9.2.7 で紹介されているファセット地図は、変数の空間分布が (例えば時間経過とともに) どのように変化するかを示すことができるが、このアプローチには欠点がある。\nファセットは数が多いと小さくなる。\nさらに、画面やページ上で各ファセットが物理的に分離しているため、ファセット間の微妙な差異を検出することが難しい。地図アニメーションは、これらの問題を解決する。\nデジタル版でしか表示できないが、より多くのコンテンツがオンラインに移行するにつれて、これは問題ではなくなってきている。\n印刷された地図から地図アニメーション (またはインタラクティブ) バージョンを含むウェブページに読者をリンクすることで、地図を生き生きとさせることができる。\nR でアニメーションを生成する方法はいくつかあり、ggplot2 をベースにした ganimate のようなアニメーションパッケージもある (Section 9.6 を参照)。\nこのセクションでは、tmap を使った地図アニメーションの作成に焦点を当てる。構文はこれまでのセクションでも使っており、アプローチの柔軟性がある。Figure 9.15 は、地図アニメーションの簡単な例である。\nファセットプロットとは異なり、複数の地図を一画面に押し込むことはなく、世界で最も人口の多い集積地の空間分布が時間とともにどのように進化していくかを見ることができる (アニメーション版は同書のウェブサイトを参照)。\nFIGURE 9.15: 1950年から2030年までの、国連による人口予測に基づく都市集積の上位30位を示した地図アニメーション。アニメーション版は、geocompr.robinlovelace.net で見ることができる。\nFigure 9.15 に示した地図アニメーションは、Section 9.2.7 で示したファセット・地図を生成するのと同じ tmap 技術を使用して作成することができる。\nただし、tm_facets_wrap() の引数に関連して、2 つの違いがある。nrow = 1, ncol = 1 として、一つの時間を一つのレイヤとしているfree.coords = FALSE で、アニメーションのために地図の範囲を維持する追加した引数を、次のコードチャンクで示そう。54結果である urb_anim は、各年度の個別の地図のセットを表している。\n最終的には、tmap_animation() でこれらを合成して、.gif ファイルとして保存する。\n次のコマンドは、Figure 9.15 に示されたアニメーションを作成する。ただし、いくつかの要素が欠けているので、演習で追加する。地図アニメーションの威力を示すもう一つの例が、Figure 9.16 にある。\nこれは、アメリカにおける州の発達を示すもので、最初は東部で形成され、その後徐々に西部へ、最後は内陸部へと発展していった。\nこの地図を再現するためのコードは、本書の GitHub リポジトリのスクリプト code/09-usboundaries.R に記載されている。\nFIGURE 9.16: 米国における人口増加、州形成、境界線の変化を示す地図アニメーション (1790-2010年)。アニメーション版は r.geocompx.org でオンライン公開。\n","code":"\nurb_anim = tm_shape(world) + tm_polygons() + \n  tm_shape(urban_agglomerations) + tm_symbols(size = \"population_millions\") +\n  tm_facets_wrap(by = \"year\", nrow = 1, ncol = 1, free.coords = FALSE)\ntmap_animation(urb_anim, filename = \"urb_anim.gif\", delay = 25)"},{"path":"adv-map.html","id":"interactive-maps","chapter":"9 R で地図を作成","heading":"9.4 インタラクティブ地図","text":"\n静止画や地図アニメーションは、地理データセットを盛り上げることができるが、インタラクティブな地図は、それらを新しいレベルに引き上げることができる。\nインタラクティブ性には様々な形態があるが、最も一般的で有用なのは、地理データセットのどの部分でもパンしたりズームしたりして、「ウェブ地図」の上に重ねてコンテキストを表示す機能である。\nより高度なインタラクティブ性のレベルとしては、さまざまなフィーチャをクリックすると表示されるポップアップ、つまりインタラクティブラベルのようなものがある。\nより高度なインタラクティブ機能としては、下記の mapdeck の例で示したように、地図を傾けたり回転させたりする機能や、ユーザーがパンやズームをすると自動的に更新される「動的にリンクした」サブプロット (Pezanowski et al. 2018) を提供する機能などが挙げられる。しかし、インタラクティブ性の最も重要なタイプは、インタラクティブまたは「スリッピー」なウェブ地図上での地理データの表示である。\n2015年にリリースされた leaflet (leaflet JavaScript ライブラリを使用) パッケージは、R 内からインタラクティブな Web 地図の作成に革命をもたらし、多くのパッケージがこれらの基盤の上に新機能を追加し (例: leaflet.extras2)、Web 地図の作成を静的地図作成と同じくらいシンプルにしている (例: mapview や tmap など)。\nここでは、各アプローチを紹介した順と逆に説明する。\ntmap (すでに学習済みの構文)、mapview、mapdeckそして最後に leaflet (対話型地図の低レベル制御を提供) を使って、動く地図を作成する方法を探究する。Section 9.2 で述べた tmap は、同じコードを使って静的な地図とインタラクティブな地図を作ることができる。\ntmap_mode(\"view\") というコマンドでビューモードに切り替えることで、任意の時点でインタラクティブ表示に切り替えることができる。\n以下のコードは、tmap オブジェクト map_nz に基づいて New Zealand のインタラクティブ地図を作成し、Section 9.2.2 で作成し、Figure 9.17 で図示している。\nFIGURE 9.17: tmap のビューモードで作成されたNew Zealand のインタラクティブ地図。インタラクティブ版は r.geocompx.org からオンラインで入手可能。\nインタラクティブモードが「オン」になったので、tmap で作成したすべての地図が起動する (インタラクティブな地図を作成する別の方法として、tmap_leaflet 機能がある)。\nこのインタラクティブモードの特筆すべき点は、以下のデモのように tm_basemap() (または tmap_options()) でベース地図を指定できることである (結果は表示していない)。あまり知られていないが、tmap の表示モードは、ファセット・プロットにも対応している。\nこの場合、tm_facets_wrap() の引数 sync を使用すると、以下のコードで作成した Figure 9.18 のように、複数の地図を作成し、ズームとパンの設定を同期させることができる。\nFIGURE 9.18: 2016年と2017年の世界のコーヒー生産量を同期させたファセット化されたインタラクティブ地図で、tmapのビューモードの動作を実演。\n同じ機能で tmap をプロットモードに戻す。tmap を使いこなせない場合は、mapview を使ってインタラクティブ地図を作成するのが一番手っ取り早いだろう。\n以下の 1 行コードは、さまざまな地理データ形式をインタラクティブに探索するための信頼できる方法である。\nFIGURE 9.19: mapview の動作イメージ図。\nmapview は簡潔な構文でありながら、強力な機能を備えている。\nデフォルトでは、マウスの位置情報、(ポップアップによる) 属性問い合わせ、スケールバー、レイヤへのズームボタンなどの標準的な GIS 機能が提供されている。\nデータセットを複数のレイヤに「バースト」する機能や、+ の後に地理的オブジェクトの名前を付けて複数のレイヤを追加する機能など、高度な制御を提供する。\nさらに、属性の自動的な色付けも可能である (引数 zcol)。\n要するに、データドリブンの leaflet API と考えることができる (leaflet については後述する)。\nmapview は常に空間オブジェクト (sf と SpatRaster) を最初の引数として期待することから、パイプ式の末尾でうまく機能する。\n次の例では、sf を使って直線とポリゴンを交差させ、mapview (Figure 9.20) で可視化する場合を考えてみよう。\nFIGURE 9.20: sf ベースのパイプ式の末尾で mapview を使用。\n注意点としては、mapview のレイヤは + 演算子で追加する (ggplot2 や tmap に似ている)。\nデフォルトでは、mapview はユーザーフレンドリーで機能の多い leaflet JavaScript ライブラリを使い、地図を出力する。\nしかし、他のレンダリングライブラリの方がパフォーマンスが良い (巨大なデータでもスムーズ)。\nmapview は、レンダリングライブラリ (\"leafgl\" と \"mapdeck\") を、mapviewOptions() で設定することができる。55\nmapview の詳細については、パッケージのウェブサイトを参照。 r-spatial.github.io/mapview/ を参照。R でインタラクティブな地図を作成する方法は他にもある。\n例えば、googleway パッケージは、柔軟で拡張性の高いインタラクティブなマッピングインターフェースを提供する\n(詳細は googleway-vignette 参照)。\n同じ著者による別のアプローチとして、mapdeck があり、Uber の Deck.gl フレームワーク にアクセスできるようになっている。\nWebGL を使用することで、大規模なデータセット (最大数百万点) をインタラクティブに可視化することができる。\n本パッケージは、Mapbox access tokens を使用している。本パッケージを使用する前に、登録する必要がある。mapdeck のユニークな点は、Figure 9.21 で図示するようにインタラクティブな 2.5 次元パースペクティブを提供できる点にある。\nこれによって、地図をパン、ズーム、回転することができる上に、地図から「押し出した」データを見ることができるのである。\nFigure 9.21 は、英国における交通事故を可視化したもので、棒の高さは地域ごとの死傷者数を表している。\nFIGURE 9.21: mapdeck によって生成された、イギリス全土の道路交通事故死傷者数を表す地図。1 km のセルの高さは事故件数を表す。\nブラウザでは、ズームやドラッグのほか、Cmd / Ctrl を押すと、地図を回転させたり傾けたりすることができる。\nmapdeck vignette で示されているように、パイプ演算子で複数のレイヤを追加することができる。\nmapdeck は sf オブジェクトもサポートしている。先のコードチャンクの add_grid() 関数呼び出しを add_polygon(data = lnd, layer_id = \"polygon_layer\") に置き換えて、インタラクティブな傾いた地図にロンドンを表すポリゴンを追加してみるとわかる。最後に、leaflet は R で最も成熟し、広く使われている対話型の地図作成パッケージである。\nleaflet は、Leaflet JavaScript ライブラリへの比較的低レベルのインタフェースを提供し、その引数の多くは、オリジナルの JavaScript ライブラリのドキュメントを読めば理解できる ( leafletjs.com を参照)。Leaflet 地図は leaflet() で作成され、その結果は leaflet 地図オブジェクトとなり、他の leaflet 関数にパイプで渡すことができる。\nこれにより、Figure 9.22 を生成する以下のコードで示すように、複数の地図レイヤや制御設定をインタラクティブに追加することができる (詳しくは rstudio.github.io/leaflet/ を参照)。\nFIGURE 9.22: ロンドン市内の自転車レンタルポイントを紹介した leaflet パッケージの実例。インタラクティブ版はオンラインを参照。\n","code":"\ntmap_mode(\"view\")\nmap_nz\nmap_nz + tm_basemap(server = \"OpenTopoMap\")\nworld_coffee = left_join(world, coffee_data, by = \"name_long\")\nfacets = c(\"coffee_production_2016\", \"coffee_production_2017\")\ntm_shape(world_coffee) + tm_polygons(facets) + \n  tm_facets_wrap(nrow = 1, sync = TRUE)\ntmap_mode(\"plot\")\n#> ℹ tmap mode set to \"plot\".\nmapview::mapview(nz)\nlibrary(mapview)\noberfranken = subset(franconia, district == \"Oberfranken\")\ntrails |>\n  st_transform(st_crs(oberfranken)) |>\n  st_intersection(oberfranken) |>\n  st_collection_extract(\"LINESTRING\") |>\n  mapview(color = \"red\", lwd = 3, layer.name = \"trails\") +\n  mapview(franconia, zcol = \"district\") +\n  breweries\nlibrary(mapdeck)\nset_token(Sys.getenv(\"MAPBOX\"))\ncrash_data = read.csv(\"https://git.io/geocompr-mapdeck\")\ncrash_data = na.omit(crash_data)\nms = mapdeck_style(\"dark\")\nmapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) |>\n  add_grid(data = crash_data, lat = \"lat\", lon = \"lng\", cell_size = 1000,\n           elevation_scale = 50, colour_range = hcl.colors(6, \"plasma\"))\npal = colorNumeric(\"RdYlBu\", domain = cycle_hire$nbikes)\nleaflet(data = cycle_hire) |> \n  addProviderTiles(providers$CartoDB.Positron) |>\n  addCircles(col = ~pal(nbikes), opacity = 0.9) |> \n  addPolygons(data = lnd, fill = FALSE) |> \n  addLegend(pal = pal, values = ~nbikes) |> \n  setView(lng = -0.1, 51.5, zoom = 12) |> \n  addMiniMap()"},{"path":"adv-map.html","id":"mapping-applications","chapter":"9 R で地図を作成","heading":"9.5 地図アプリ","text":"\nSection 9.4 で示したインタラクティブなウェブ地図は、遠くまで行くことができる。\n表示するレイヤを慎重に選択し、ベース地図とポップアップを使用することで、ジオコンピュテーションを含む多くのプロジェクトの主な結果を伝えることができる。\nしかし、ウェブ地図というアプローチでインタラクティブ性を追求することには限界がある。地図はパン、ズーム、クリックといったインタラクティブな動きをするが、コードは静的で、ユーザーインターフェースは固定されている。ウェブ地図では、すべての地図コンテンツが一般的に静的であるため、ウェブ地図は大規模なデータセットを容易に扱うことができない。変数間の関係を示すグラフや「ダッシュボード」のようなインタラクティブなレイヤを追加することは、ウェブ地図のアプローチでは困難であるこれらの制約を克服するためには、静的なウェブ地図にとどまらず、地理空間系のフレームワークや地図サーバーを利用することが必要である。\nこの分野の製品には、GeoDjango (Django Web フレームワークを拡張したもので、Python)、MapServer (Web アプリケーション開発用のフレームワークで、大部分が C と C++ で書かれている) や GeoServer (Java で書かれた成熟した強力な地図サーバ) が含まれる。\nこれらはそれぞれ拡張性があり、毎日何千人もの人々に地図を提供することが可能である (あなたの地図に対する人々の関心が十分に高ければの話であるが)。\n欠点としては、このようなサーバーサイドのソリューションは、セットアップと保守に多くの熟練した開発者の時間を必要とし、地理空間データベース管理者 (DBA) などの役割を持つ人々を巻き込んでしまうこともよくある。R の場合は幸運なことに、shiny を使って、ウェブ地図アプリケーションを素早く作成できるようになった。\nオープンソース本 Mastering Shiny で説明されているように、 shiny は、R コードをインタラクティブなウェブアプリに変換する R パッケージでありフレームワークである (Wickham 2021)。\n leaflet::renderLeaflet() を使うことで、shiny アプリにインタラクティブ地図を追加することができる。\nこのセクションでは、ウェブ地図の観点から shiny の基本を学び、100 行未満のコードで全画面の地図アプリケーションを完成させることができる。shiny の仕組みは、shiny.posit.co に詳しく書かれているが、「フロントエンド」 (ユーザーが見る部分) と「バックエンド」コードという 2 つの構成要素がある。\nshiny アプリでは、これらの要素は通常、app フォルダ内にある app.R という R スクリプト内の ui と server というオブジェクトで作成される。\nこれにより、ウェブの地図アプリケーションを 1 つのファイルで表現することも可能で、例えば、本書の GitHub リポジトリにある CycleHireApp/app.R は単一のファイルで表現している。大規模なアプリを検討する前に、「lifeApp」と名付けた最小限の例を実際に見てみよう。56\n以下のコードでは、shinyApp() というコマンドで、lifeApp を定義して起動する。これは、平均寿命のレベルが低い国を表示させることができるインタラクティブなスライダーである (Figure 9.23 を参照)。\nFIGURE 9.23: shiny で作成したWeb地図アプリケーションの最小限の例を示す画面。\nlifeApp のユーザーインターフェース (ui) は fluidPage() で作成されている。\nこれには、入力と出力の「ウィジェット」、この場合、sliderInput() (他にも多くの *Input() 関数が利用できる) と leafletOutput() が含まれる。\nウィジェットはデフォルトで列方向に配置されており、Figure 9.23 でスライダーインターフェースが地図の真上に配置されている理由を説明している (列方向にコンテンツを追加する方法については ?column を参照)。サーバー側 (server) は、input と output を引数に持つ関数である。\noutput は、render*() 関数によって生成された要素を含むオブジェクトのリストである。この例では、renderLeaflet() が output$map を生成している。\nサーバーで参照される input$life などの入力要素は、上記のコードで ui — inputId = \"life\" によって定義される中に存在する要素に関連していなければならない。\n関数 shinyApp() は ui と server の両要素を結合し、その結果を新しい R プロセスで対話的に提供する。\nFigure 9.23 に表示されている地図のスライダーを動かすと、ユーザーインターフェースでは見えないようになっているが、実際には R のコードが再実行される。この基本的な例をもとに、どこにヘルプがあるか (?shiny 参照) を知っておけば、あとは読むのをやめてプログラミングを始めるのが一番だろうね。\n次のステップとして推奨されるのは、以前に紹介した CycleHireApp/app.R スクリプトを任意の integrated development environment (IDE) で開き、それを修正して繰り返し実行することである。\nこの例では、shiny で実装されたウェブ地図アプリケーションのコンポーネントの一部が含まれており、それらがどのように動作するかを「照らす」べきものである。CycleHireApp/app.R スクリプトには、単純な ‘lifeApp’ の例で示されたものを超える shiny 関数が含まれている。shiny.robinlovelace.net/CycleHireApp 参照。\nreactive() と observe() (ユーザーインターフェースに反応する出力を作成するため — ?reactive 参照) と leafletProxy() (すでに作成されている leaflet オブジェクトを変更するため) がある。\nこのような要素は、shiny で実装された Web 地図アプリケーションの作成を可能にする (Lovelace et al. 2017).。\nRStudio の leaflet ウェブサイト の shiny セクションで説明されているように、新しいレイヤの描画やデータのサブセットなどの高度な機能を含むさまざまな「イベント」をプログラムすることが可能である。CycleHireApp などのアプリを試すことで、R によるウェブ地図アプリケーションの知識だけでなく、実践的なスキルも身につけることができる。\n例えば、setView() の内容を変更すると、アプリが起動されたときにユーザーに表示される開始バウンディングボックスが変更される。\nこのような実験は、ランダムに行うよりも、関連する文書を参照し、?shiny を始めとして、演習で提起したように問題を解決する動機で行うべきである。このように shiny を使用することで、地図アプリケーションのプロトタイプ作成をこれまで以上に迅速かつ身近に行うことができる (shiny アプリケーション https://shiny.posit.co/deploy/ の実装は、この章の範囲を超えた別のトピック)。\n最終的にアプリケーションが異なる技術で展開されるとしても、shiny によって、ウェブ地図アプリケーションが比較的少ないコード行数で開発できることは間違いない (CycleHireApp の場合、86行)。\nしかし、shiny アプリは巨大になる傾向がある。\n例えば、pct.bike でホストされている Propensity Cycle Tool (PCT) は、英国運輸省の資金援助による全国規模の地図ツールである。\nPCT は毎日何十人もの人が利用しており、1,000行以上の コード (Lovelace et al. 2017) に基づいた複数のインタラクティブな要素を備えている。このようなアプリの開発には時間と労力がかかるが、shiny は再現性のあるプロトタイプ作成のためのフレームワークを提供し、開発プロセスを支援するはずである。\nshiny でプロトタイプを簡単に開発することの問題点として、地図アプリケーションの目的が詳細に想定されていない段階でプログラミングを開始する誘惑に駆られることが挙げられる。\nそのため、shiny を提案しながらも、インタラクティブ地図のプロジェクトの第一段階として、ペンと紙という古くからある技術から始めることを推奨している。\nこのように、プロトタイプのウェブアプリケーションは、技術的な考慮事項ではなく、開発者の動機と想像力によって制限されるべきものなのである。\nFIGURE 9.24: CycleHireApp は、住んでいる場所と必要な自転車に基づいて、最も近い自転車レンタルステーションを見つけるためのシンプルなウェブマッピングアプリケーション。インタラクティブ版は geocompr.robinlovelace.net で確認できる。\n","code":"\nlibrary(shiny)   # shiny\nlibrary(leaflet) # renderLeaflet 関数\nlibrary(spData)  # world データを読み込む \nui = fluidPage(\n  sliderInput(inputId = \"life\", \"Life expectancy\", 49, 84, value = 80),\n      leafletOutput(outputId = \"map\")\n  )\nserver = function(input, output) {\n  output$map = renderLeaflet({\n    leaflet() |> \n      # addProviderTiles(\"OpenStreetMap.BlackAndWhite\") |>\n      addPolygons(data = world[world$lifeExp < input$life, ])})\n}\nshinyApp(ui, server)"},{"path":"adv-map.html","id":"other-mapping-packages","chapter":"9 R で地図を作成","heading":"9.6 その他の地図作成パッケージ","text":"tmap は、さまざまな静的地図 (Section 9.2) を作成するための強力なインターフェースを提供し、インタラクティブな地図 (Section 9.4) もサポートしている。\nしかし、R で地図を作成するためのオプションは他にもたくさんある。\nこのセクションの目的は、これらの一部を紹介し、追加リソースのポインタを提供することである。地図作成は、R パッケージの開発において驚くほど活発な分野なので、ここでカバーしきれないほど多くのことを学ぶことができる。最も成熟した選択肢は、コアな空間パッケージである sf (Section 2.2.3) と terra (Section 2.3.3) が提供する plot() メソッドを使用することである。\nこれらのセクションで触れていないが、ベクタとラスタのオブジェクトのプロットメソッドは、組み合わせて同じプロットエリアに描画することができる (sf プロットのキーやマルチバンドのラスタなどの要素はこれを邪魔する)。\nこの動作は、Figure 9.25 を生成する次のコードチャンクで説明される。\nplot() には他にも多くのオプションがあり、?plot のヘルプページと sf 5 番目の vignette sf5 のリンクをたどって調べることができる (訳注: vignette 日本語版)。\nFIGURE 9.25: plot() で作成したNew Zealand の地図。右の凡例は標高 (海抜 1000 m) を示している。\ntidyverse のプロットパッケージ ggplot2 は sf オブジェクトを geom_sf() でサポートしている。\n構文は tmap で使用されているものと似ている。\n最初は ggplot() で、次に + geom_*() を追加釣ることで、レイヤを追加する。ここで * は、geom_sf() (sf オブジェクトの場合) や geom_points() (点の場合) などのレイヤタイプを表す。ggplot2 はデフォルトで経緯度図郭線を描画する。\n経緯度図郭線のデフォルト設定は、scale_x_continuous() , scale_y_continuous() または coord_sf(datum = NA) で上書きできる。\nその他の注目すべき特徴としては、aes() でカプセル化された引用符なしの変数名を使って、どの美観が異なるかを示したり、data 引数を使ってデータソースを切り替えたりしている。以下のコードチャンクでは Figure 9.26 を作成している。また、ggplot2 をベースにした地図の利点として、plotly パッケージ の関数 ggplotly() を使って表示すると、簡単にインタラクティブなレベルを与えることができることが挙げられる。\n例えば、plotly::ggplotly(g1) を試してみて、その結果を blog.cpsievert.で説明されている他の plotly 地図作成関数と比較してみてみよう。gplot2 の利点は、強力なユーザコミュニティと多くのアドオンパッケージを持っていることである。\n例えば、ggplot2 の地図機能を強化するために、北矢印 (annotation_north_arrow()) やスケールバー (annotation_scale()) あるいは背景タイル (annotation_map_tile()) を追加する ggspatial がある。\nまた、layer_spatial() は様々な空間データクラスを扱うことができる。\nこれによって、Figure 9.26 で示すように、terra の SpatRaster オブジェクトをプロットすることができる。\nFIGURE 9.26: ggplot2 のみ (左) と ggplot2 と ggspatial (右) で生成したNew Zealand 地図の比較。\n同時に、ggplot2 にはいくつかの欠点がある。\ngeom_sf() 関数は、空間データ から使用する希望の凡例を作成できない場合がある。\nオープンソースの ggplot2 book (Wickham 2016) や、ggrepel や tidygraph などの多数の ‘ggpackage’ の説明の中に、良い追加リソースがある。最初に sf、terra、ggplot2 パッケージを使った地図作成を取り上げたのは、これらのパッケージが非常に柔軟で、様々な静的地図を作成することが可能であることがある。\n特定の種類の地図作成パッケージ (次の段落) を取り上げる前に、すでに取り上げた汎用の地図作成パッケージの代替品 (Table 9.1) について考えてみる価値がある。\nTABLE 9.1: TABLE 9.2: 汎用の地図作成パッケージ\nTable 9.1 は、さまざまな地図作成パッケージが利用可能であることを示しており、この表に記載されていないものも多数ある。\n特に注目すべきは mapsf で、コロプレス図、比例シンボル地図、フロー地図など、さまざまな地理的視覚化を生成することができる。\nこれらは、mapsf vignette に記載されている。Table 9.3 に示すように、いくつかのパッケージは、特定の地図タイプに焦点を当てている。\n地理空間を歪めたカルトグラムの作成、ラインマップの作成、ポリゴンの正六角形グリッドへの変換、複雑なデータを地理的トポロジーを表すグリッド上に可視化し、３次元表現をするパッケージである。TABLE 9.3: 特定の目的のための地図作成パッケージとその関連する指標。しかし、前述のパッケージはいずれも、データ準備や地図作成のアプローチが異なっている。\n次の段落では、cartogram パッケージ (Jeworutzki 2023) にのみ焦点を当てる。\nそのため、geogrid、 geofacet、linemap、tanaka、rayshader のドキュメントを読んで、より詳しく知ることを勧める。カルトグラムとは、地図変数を表現するために一定の幾何学的な歪みを持たせた地図のことである。\nこのような地図の作成は、R では cartogram を用いることで、連続 (contiguous)・非連続 (non-contiguous) の面積カルトグラム (area cartogram) を作成することが可能である。\nこれ自体は地図作成パッケージではないが、汎用の地図作成パッケージを使用してプロットできるような歪んだ空間オブジェクトを構築することが可能である。cartogram_cont() 関数は、連続した面積カルトグラムを作成する。\n入力として、sf オブジェクトと変数名 (列) を受け取る。\nさらに、intermax 引数 (カルトグラム変換の最大反復回数) を変更することが可能である。\n例えば、New Zealand の地域の所得の中央値を連続カルトグラム (Figure 9.27 右図) で表すと、次のようになる。\nFIGURE 9.27: 標準地図 (左) と連続範囲 (右) の比較。\ncartogram では、cartogram_ncont() を使用して非連続面積カルトグラムを、cartogram_dorling() を使用して円面積カルトグラム (Dorling 法) を作成することもできる。\n非連続面積カルトグラムは、提供された重み付け変数に基づいて各面積を縮小することによって作成される。\n円面積カルトグラム (Dorling 法) は、重み付け変数に比例した面積を持つ円から構成されている。\n以下のコードは、米国各州の人口の非連続面積と円面積カルトグラム (Dorling 法) の作成例である (Figure 9.28)。\nFIGURE 9.28: 非連続領域カルトグラム (左) と円面積カルトグラム (右) の比較。\n","code":"\ng = st_graticule(nz, lon = c(170, 175), lat = c(-45, -40, -35))\nplot(nz_water, graticule = g, axes = TRUE, col = \"blue\")\nterra::plot(nz_elev / 1000, add = TRUE, axes = FALSE)\nplot(st_geometry(nz), add = TRUE)\nlibrary(ggplot2)\ng1 = ggplot() + geom_sf(data = nz, aes(fill = Median_income)) +\n  geom_sf(data = nz_height) +\n  scale_x_continuous(breaks = c(170, 175))\ng1\nlibrary(ggspatial)\nggplot() + \n  layer_spatial(nz_elev) +\n  geom_sf(data = nz, fill = NA) +\n  annotation_scale() +\n  scale_x_continuous(breaks = c(170, 175)) +\n  scale_fill_continuous(na.value = NA)#> Warning: One or more parsing issues, call `problems()` on your data frame for details,\n#> e.g.:\n#>   dat <- vroom(...)\n#>   problems(dat)\nlibrary(cartogram)\nnz_carto = cartogram_cont(nz, \"Median_income\", itermax = 5)\ntm_shape(nz_carto) + tm_polygons(\"Median_income\")\nus_states9311 = st_transform(us_states, \"EPSG:9311\")\nus_states9311_ncont = cartogram_ncont(us_states9311, \"total_pop_15\")\nus_states9311_dorling = cartogram_dorling(us_states9311, \"total_pop_15\")"},{"path":"adv-map.html","id":"演習-7","chapter":"9 R で地図を作成","heading":"9.7 演習","text":"ここでの演習は、新しくオブジェクト africa を使用する。\nこれは、spData のデータセット world と worldbank_df から、以下のように作成する。spDataLarge のデータセット zion と nlcd も使用する。E1. graphics (ヒント: plot()) と tmap パッケージ (ヒント: tm_shape(africa) + ...) を使って、Africa 全土の人間開発指数 (HDI) の地理的分布を示す地図を作成しなさい。それぞれの長所を経験に基づいて 2 つ挙げなさい。他の地図作成パッケージを 3 つ挙げ、それぞれの利点を挙げなさい。ボーナス: これら 3 つの他のパッケージを使って、さらに 3 つのアフリカの地図を作りなさい。E2. 前の演習で作成した tmap を拡張して、凡例に 3 つのビンを設定しなさい: “High” (0.7 を超える HDI)、“Medium” (0.55 と 0.7 の間の HDI)、“Low” (0.55 を下回る HDI)。\n- ボーナス: 例えば、凡例のタイトル、クラスラベル、色パレットを変更することで、マップの美観を改善しなさい。E3. africa の小地域を地図上に表示しなさい。\nデフォルトの色パレットと凡例のタイトルを変更しなさい。\n次に、この地図と前の練習で作成した地図を組み合わせて、一つのプロットし統合しなさい。E4. Zion 国立公園の土地被覆マップを作成しなさい。土地被覆カテゴリの認識に合わせてデフォルトの色を変更縮尺バーと北矢印を追加し、両方の位置を変更して地図の美観を向上ボーナス: Zion 国立公園の Utah 州との位置関係を示す挿入地図を追加 (ヒント: ユタを表すオブジェクトは us_states データセットから抽出できる)。E5. Eastern Africa の国々のファセットマップを作成しなさい。1 つのファセットは HDI を表し、もう 1 つのファセットは人口増加を表す (ヒント: それぞれ変数HDIとpop_growthを使用)国ごとに「小さな倍数」を設定E6. これまでのファセット地図の例に基づいて、East Africa の地図アニメーションを作成しなさい。各国を順番に表示HDI を示す凡例とともに各国を順番に表示E7. Africa における HDI のインタラクティブ地図を作成しなさい。tmapmapviewleafletボーナス: 各アプローチについて、凡例 (自動的に提供されない場合) とスケールバーを追加しなさい。E8. 交通政策や土地利用政策をよりエビデンスに基づいたものにするために使用できるウェブ地図アプリのアイデアを紙にスケッチしなさい。あなたが住んでいる都市で、1 日あたり数人のユーザー向けあなたが住んでいる国で、1 日あたり数十人のユーザー向け世界中、1 日あたり数百人のユーザーと大規模なデータ配信が必要な場合E9. coffeeApp/app.R のコードを更新し、Brazil を中心に表示するのではなく、ユーザーがどの国を中心に表示するかを選択しなさい。textInput() を使いなさいselectInput() を使いなさいE10. ggplot2 パッケージを使用して、Figure 9.1 と Figure 9.7 をできるだけ忠実に再現しなさい。E11. us_states と us_states_df を結合し、新しいデータセットを使って各州の貧困率を計算しなさい。\n次に、総人口に基づいて連続的な範囲カートグラムを作成しなさい。\n最後に、貧困率の 2 つの地図を作成し、比較しなさい：(1) 標準的なコロプレス地図と、(2) 作成したカートグラムの境界線を使った地図。\n1 枚目と 2 枚目の地図から得られる情報は何か?\n両者はどう違うのか?E12. Africa の人口増加を視覚化しなさい。\n次に、geogrid パッケージを使って作成した六角形と正方形のグリッドの地図と比較しなさい。","code":"\nlibrary(spData)\nafrica = world |> \n  filter(continent == \"Africa\", !is.na(iso_a2)) |> \n  left_join(worldbank_df, by = \"iso_a2\") |> \n  select(name, subregion, gdpPercap, HDI, pop_growth) |> \n  st_transform(\"ESRI:102022\") |> \n  st_make_valid() |> \n  st_collection_extract(\"POLYGON\")\nzion = read_sf((system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")))\nnlcd = rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))"},{"path":"gis.html","id":"gis","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10 GIS ソフトウェアへのブリッジ","text":"","code":""},{"path":"gis.html","id":"prerequisites-10","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"必須パッケージ","text":"本章では、QGIS、SAGA、GRASS GIS がインストールされていること、および以下のパッケージが添付されていることを条件とする。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(qgisprocess)\nlibrary(Rsagacmd)\nlibrary(rgrass)\nlibrary(rstac)\nlibrary(gdalcubes)"},{"path":"gis.html","id":"introduction-10","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.1 イントロダクション","text":"対話式コンソールを使うインタプリタ言語の特徴は、その対話の方法にある。技術的には、入力・評価・出力 (read-evaluate-print loop, REPL) と言い、R もその一つである。\nマウスで画面上のさまざまな場所をクリックする代わりに、コマンドを入力し、Enter を押すことで、コマンドを実行する。\nRStudio や VS Code などの対話型開発環境を使った作業では、通常ソースエディタでソースファイルに書き込み、Ctrl+Enter などのショートカットキーでコードの対話的実行を制御する。Command Line Interface (CLI) は R だけのものではない。初期のコンピュータ環境は、おおむねコマンドライン「シェル」に依存しており、GUI が一般的になったのは、コンピューターのマウスが普及した1990年代以降のことである。\n例えば、最も長い歴史を持つ GIS プログラムの一つである GRASS GIS は、洗練された GUI (Landa 2008) を獲得するまでは、主にコマンドラインでの対話に依存していた。\nほとんどの GIS パッケージは、グラフィカルユーザーインターフェース (Grafical User Interface, GUI) を採用している。\nQGIS、SAGA、GRASS GIS、gvSIG を、システムターミナルや組み込み CLI から操作することも可能であるが、「マウス操作」が一般的である。\nこれは多くの GIS ユーザーがコマンドラインの利点を見逃していることを意味する。\nQGIS (Sherman 2008) の作成者によれば、「近代的」GIS ソフトウェアの発展に伴い、マウス操作を好む人がほとんどである。それはいいことであるが、コマンドラインには、とてつもない量の柔軟性とパワーが待っている。繰り返しコマンドラインで作業をする場合、同じことを GUI で行う場合よりも短い時間で行うことができる。「CLI vs GUI」 の議論は、敵対的になる必要はない。作業の内容 (フィーチャを描く時は GUI の方が優れている)、再現可能性、ユーザーのスキルセットに応じて、どちらの選択肢も利点はある。\nGRASS GIS は、CLI をベースとしながらも GUI がある GIS の良い例である。\n同様に、R は CLI であり、そして RStudio のような IDE が GUI を提供することで、アクセシビリティを向上している。\nこのように、ソフトウエアは CLI または GUI というように完全に分けられるものではない。\nしかしながら、CLI は以下のような重要な点があることは押さえておきたい。繰り返し作業の自動化透明性と再現性を可能にする既存の機能を修正したり、新しい機能を実装するためのツールを提供することで、ソフトウェア開発を促進する将来性があり、かつ効率的なプログラミングスキルを身につけることができるデジタル時代に必須のタッチタイピング一方、GUI ベースの GIS システムにも有利な点がある。学習曲線が「浅い」ので、新しい言語を何時間も学ぶことなく、地理データを探索し、可視化することができるトレース、スナップ、トポロジーツールなど、「デジタイジング」 (新しいベクタデータセットの作成) のための優れたサポートを提供する57地上基準点によるジオリファレンス (georeference、ラスタ画像と既存地図とのマッチング)、オルソ補正 (orthorectification) が可能である立体視マッピング (LiDAR、Structure Motion など) に対応する専用 GIS のもう一つの利点は、「GIS ブリッジ」を経由して何百もの「ジオアルゴリズム」 を利用できることである (Neteler Mitasova 2008) 。\nブリッジを通して R の機能を拡張し、地理データ問題を解決することが本章のテーマである。R はインターフェース言語として誕生したため、再現可能なデータ分析と GIS をつなぐブリッジを構築する選択肢として当然のごとく選ばれた。\nR (および、その前身である S) は、他の言語 (特に FORTRAN と C) の統計アルゴリズムへのアクセスを提供しており、また C と FORTRAN にはない高レベルの REPL 環境からアクセスできるようになっていた (Chambers 2016)。\nR はこの伝統を受け継ぎ、特に C++ などの多くの言語へのインタフェースを提供している。R は GIS として設計されたものではない。しかし、専用の GIS とのインターフェースが可能なため、驚異的な地理空間能力を発揮する。\nGIS ブリッジを通すことで、R はさまざまな作業を実行でき、さらには CLI の持っている再現可能性、拡張性、生産性といった付加価値をもたらす。\nさらに、R は、インタラクティブ地図/地図アニメーション作成 (Chapter 9 参照) や空間統計モデリング ( Chapter 12 参照) など、ジオコンピュテーションのいくつかの分野では GIS を凌駕する性能を有している。この章では、Table 10.1 にまとめた 3 つの成熟したオープンソース GIS 製品への「ブリッジ」に焦点を当てる。QGIS qgisprocess; [Dunnington et al. (2024); Section 10.2]SAGA Rsagacmd; [Pawley (2023); Section 10.3]GRASS GIS rgrass; [Bivand (2023); Section 10.4]また、QGIS (docs.qgis.org 参照) や GRASS GIS (grasswiki.osgeo.org 参照) など GIS から R を実行する環境も開発されている。TABLE 10.1: 3 つのオープンソース GIS の比較。 Hybrid とは、ベクタと ラスタに対応していることを示す。また、R-GIS ブリッジを補完するために、3 つのブリッジの後で空間ライブラリへのインタフェース (Section 10.6)、空間データベース (Section 10.7)、地球観測データのクラウド処理 (Section 10.8) について簡単に紹介する。","code":""},{"path":"gis.html","id":"rqgis","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.2 qgisprocess: QGIS へのブリッジなど","text":"QGIS は、最も人気のあるオープンソース GIS である (Table 10.1; Graser Olaya (2015))。\nQGIS は、統一されたインターフェースで、QGIS 自身のジオアルゴリズム、GDAL、さらにインストールされている場合には GRASS GIS、SAGA などのプロバイダを利用することができる (Graser Olaya 2015)。\nバージョン 3.14 (2020年夏にリリース) 以降、QGIS は、さまざまジオコンピュテーション機能にアクセスできる qgis_process コマンドラインを提供している。\nqgis_process は、標準的 QGIS に備わる 300 以上のジオアルゴリズムと、GRASS GIS や SAGA など 1000 以上の外部プロバイダへのアクセスを提供している。qgisprocess パッケージは、R からアクセスすることも可能である。\nこのパッケージは、 システム上に最低でも QGIS、また本章で使用する関連するプラグインである GRASS GIS や SAGA を必要とする。\nインストールに関しては、qgisprocess ドキュメントを参照。あるいは、Docker がインストール済みであれば、本プロジェクトの qgis イメージから qgisprocess を使うこともできる。\nDocker をインストールしており、十分な実行環境のある方は、以下のコマンドで qgisprocess および関連プラグインを実行できる (geocompx/docker リポジトリを参照)。docker run -e DISABLE_AUTH=true -p 8786:8787 ghcr.io/geocompx/docker:qgisこのパッケージは、QGIS のインストールを自動的に検出しようとし、検出できない場合は警告を発する。 58\n設定に失敗した場合の解決策としては、options(qgisprocess.path = \"path//your_qgis_process\")、環境変数 R_QGISPROCESS_PATH を設定する方法が考えられる。\n上記の方法は、複数の QGIS がインストールされており、どれを使うかを決めたい場合にも使える。\n詳細については、qgisprocess ‘getting started’ vignette を参照。\n次に、どのプラグイン (異なるソフトウェアを意味する) が自分のコンピュータで利用できるかを調べてみよう。プラグイン GRASS GIS (grassprovider) と SAGA (processing_saga_nextgen) が存在しているが、有効になっていないことがわかる。\nこの二つは本章の後半で使用するので、有効化しよう。SAGA のほかにも、QGIS Python プラグイン Processing Saga NextGen をインストールしておく必要がある。\nこのプラグインは、プラグインの管理とインストール あるいは、Python パッケージの qgis-plugin-manager (Linux の場合) からインストールすることができる。qgis_providers() で、ソフトウェアの名称と対応するジオアルゴリズム数の一覧を表示する。出力表から、QGIS のジオアルゴリズム (native, qgis, 3d) と、サードパーティプロバイダの GDAL、SAGA、GRASS GIS の外部アルゴリズムを QGIS インターフェースを通して使用できることが確認できた。これで、R から QGIS などの地理計算をする準備ができた。\nそれでは、2 つの事例を試してみよう。\n最初のものは、異なる境界線を持つ 2 つのポリゴンデータセットを和集合を作成 (union) する方法を示している (Section 10.2.1)。\nもう一つは、ラスタ (Section 10.2.2) で表現された数値標高モデルから新しい情報を導き出すことに重点を置いている。","code":"\nlibrary(qgisprocess)\n#> Attempting to load the cache ... Success!\n#> QGIS version: 3.30.3-'s-Hertogenbosch\n#> ...\nqgis_plugins()\n#> # A tibble: 4 × 2\n#>   name                    enabled\n#>   <chr>                   <lgl>\n#> 1 grassprovider           FALSE\n#> 2 otbprovider             FALSE\n#> 3 processing              TRUE\n#> 4 processing_saga_nextgen FALSE\nqgis_enable_plugins(c(\"grassprovider\", \"processing_saga_nextgen\"), \n                    quiet = TRUE)\nqgis_providers()\n#> # A tibble: 7 × 3\n#>   provider provider_title    algorithm_count\n#>   <chr>    <chr>                       <int>\n#> 1 gdal     GDAL                           56\n#> 2 grass    GRASS                         306\n#> 3 qgis     QGIS                           50\n#> 4 3d       QGIS (3D)                       1\n#> 5 native   QGIS (native c++)             243\n#> 6 pdal     QGIS (PDAL)                    17\n#> 7 sagang   SAGA Next Gen                 509"},{"path":"gis.html","id":"qgis-vector","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.2.1 ベクタデータ","text":"異なる空間単位 (地域、行政単位など) を持つ 2 つのポリゴンオブジェクトがある場合を考えてみよう。\nこの 2 つのオブジェクトを統合して、すべての境界線と関連する属性を含む 1 つのオブジェクトにすることが目標である。\nSection 4.2.8 (Figure 10.1) でも見た不整合なポリゴンを再び利用する。\nどちらのポリゴンデータセットも spData パッケージで提供されており、その両方に地理的な CRS を使用したい (Chapter 7 も参照)。\nFIGURE 10.1: 二つの単位: 不一致 (黒い線) と集合ゾーン (赤い境界)。\n最初に、二つのベクタをマージ (merge) するアルゴリズムを探そう。\nqgis_algorithms() 関数は、利用可能なアルゴリズムをすべて表示する。\nこの関数は、利用可能なすべてのプロバイダと、それらが含むアルゴリズムを含むデータフレームを返す。59qgis_search_algorithms() 関数は、アルゴリズムを探すために使うことができる。\n関数の短い説明文に “union”という単語が含まれていると仮定すると、以下のコードを実行して、興味のあるアルゴリズムを見つけることができる。上記のリストにあるアルゴリズムの 1 つ \"native:union\" は、探している機能の可能性が高そうである。\n次のステップとしては、このアルゴリズムが何をするのか、どう使えばいいのかを調べよう。\nqgis_show_help() は、アルゴリズムが何をし、引数や出力についての要約を返す。60\nこれによって、出力が長くなる。\n以下のコマンドは、各行に \"native:union\" が必要とする引数を表すデータフレームを返し、各列は名前、説明、種類、デフォルト値、とりうる値を示す。union_arguments$name の引数は、INPUT、OVERLAY、OVERLAY_FIELDS_PREFIX、OUTPUT である。\nunion_arguments$acceptable_values は、各引数に対して、とりうる値のリストを持っている。\n多くの関数は、ベクタレイヤへのパスを入力に必要としているが、qgisprocess の関数は sf オブジェクトも受けることができる。\nラスタへのパスが必要とされる場合は、terra と stars のオブジェクトも対応している。\nこれは便利であるが、もし qgisprocess アルゴリズムに引き渡しだけであれば、パスを渡すことをお薦めする。というのも、qgisprocess はジオアルゴリズムの最初に、QGIS が読める形式である .gpkg や .tif に変換しているからである。\nこれはアルゴリズムの実行時間を長くしてしまう。qgisprocess の主な機能は qgis_run_algorithm() であり、QGIS に入力を送り、出力を受け取る。\nこれは、アルゴリズム名とヘルプに示される名前付き引数のセットを受け取り、期待される計算を実行する。\n今回のケースでは、INPUT、OVERLAY、OUTPUT の 3 つの引数が重要だと思われる。\n最初の INPUT は、主なベクタオブジェクト incongr_wgs であり、2 番目の OVERLAY は、aggzone_wgs である。\n最後の引数、OUTPUT は出力ファイル名だが、指定されていない場合、qgisprocess は自動的に tempdir() に一時ファイルを作成する。上記のコードを実行すると、2 つの入力オブジェクトが一時的な .gpkg ファイルに保存され、選択されたアルゴリズムがそれらに実行され、一時的な .gpkg ファイルが出力として返される。\nqgisprocess パッケージは、qgis_run_algorithm() の結果を、この場合は出力ファイルへのパスを含むリストとして保存する。\nこのファイルを R に読み戻すには、read_sf() (例, union_sf = read_sf(union[[1]]) を使うか、st_as_sf() を使って直接読み込むことができる。QGIS の和集合 (union) の操作は、2 つの入力レイヤの交差 (intersect) と対称差 (symmetrical difference) を用いて、2 つの入力レイヤを 1 つのレイヤにマージすることに注意 (ちなみに、これは GRASS GIS と SAGA で結合操作をするときのデフォルトでもある)。\nこれは st_union(incongr_wgs, aggzone_wgs) とは違う (演習参照)!その結果である union_sf は、2 つの入力オブジェクトよりも多くのフィーチャを持つポリゴンとなる。\nしかし、これらのポリゴンの多くは小さく、実際の領域を表しているわけではなく、2 つのデータセットの細部が異なるために生じたものであることに注意しておこう。\nこうした誤差によってできたものは、スライバー (sliver) ポリゴンと呼ばれている (Figure 10.2 の左側のパネルにある赤い色のポリゴンを参照)。\nスライバーを識別する一つの方法として、面積が比較的非常に小さいポリゴン、ここでは例えば 25,000 m2 を見つけ、次にそれを削除する。\n適切なアルゴリズムを探そう。今回見つかったアルゴリズム (v.clean) は、QGIS ではなく、GRASS GIS に含まれている。\nGRASS GIS の v.clean は、空間ベクタデータのトポロジーをクリーニングする強力なツールである。\n重要なのは、qgisprocess を通して使用できることである。前のステップと同様に、このアルゴリズムのヘルプを見るところから始めよう。ここでは出力を省略した。実際のヘルプテキストはかなり長く、多くの引数を含んでいる。61\nこれは、v.clean がマルチツールであり、さまざまな種類のジオメトリをクリーニングし、さまざまな種類のトポロジー問題を解決することができることがある。\nこの例では、いくつかの引数に絞って説明するが、v.clean の機能については、 アルゴリズムのドキュメントを勧める。このアルゴリズムの主な引数は input で、これはベクタオブジェクトである。\n次に tool の選択であるが、これはクリーニングの方法である。 62\nv.clean には、重複した形状の削除、線間の微小角度の削除、微小領域の削除など、12 種類のツールが存在する。\n今回は、後者のツール、rmarea を解説する。\nいくつかのツール (rmarea を含む) は、追加の引数 threshold を必要とし、その動作は選択されたツールに依存する。\nこの場合、rmarea ツールは、threshold で与えられた値より小さいか等しい領域をすべて削除する。\nなお、入力レイヤの空間参照系によらず単位は平方メートルである。このアルゴリズムを実行し、その出力を新しい sf オブジェクト clean_sf に変換してみよう。その結果、Figure 10.2 の右側のパネルでは、予想通り、灰色のポリゴンが削除されているように見える。\nFIGURE 10.2: 灰色部分を赤で強調 (左) と灰色部分を除去 (右)。\n","code":"\ndata(\"incongruent\", \"aggregating_zones\", package = \"spData\")\nincongr_wgs = st_transform(incongruent, \"EPSG:4326\")\naggzone_wgs = st_transform(aggregating_zones, \"EPSG:4326\")\n# 出力は表示せず\nqgis_algo = qgis_algorithms()\nqgis_search_algorithms(\"union\")\n#> # A tibble: 2 × 5\n#>   provider provider_title    group          algorithm         algorithm_title \n#>   <chr>    <chr>             <chr>          <chr>             <chr>           \n#> 1 native   QGIS (native c++) Vector overlay native:multiunion Union (multiple)\n#> 2 native   QGIS (native c++) Vector overlay native:union      Union  \nalg = \"native:union\"\nunion_arguments = qgis_get_argument_specs(alg)\nunion_arguments\n#> # A tibble: 5 × 6\n#>   name    description qgis_type default_value available_values acceptable_...\n#>   <chr>   <chr>       <chr>     <list>        <list>           <list>           \n#> 1 INPUT   Input layer source    <NULL>        <NULL>           <chr [1]>        \n#> 2 OVERLAY Overlay la… source    <NULL>        <NULL>           <chr [1]>        \n#> 3 OVERLA… Overlay fi… string    <NULL>        <NULL>           <chr [3]>        \n#> 4 OUTPUT  Union       sink      <NULL>        <NULL>           <chr [1]>        \n#> 5 GRID_S… Grid size   number    <NULL>        <NULL>           <chr [3]>  \n\n#> [[1]]\n#> [1] \"A numeric value\"                                                                                 \n#> [2] \"field:FIELD_NAME to use a data defined value taken from the FIELD_NAME\n#>      field\"                    \n#> [3] \"expression:SOME EXPRESSION to use a data defined value calculated using\n#>      a custom QGIS expression\"\nunion = qgis_run_algorithm(alg,\n  INPUT = incongr_wgs, OVERLAY = aggzone_wgs\n)\nunion\n#>  $ OUTPUT: 'qgis_outputVector' chr \"/tmp/...gpkg\"\nunion_sf = st_as_sf(union)\nqgis_search_algorithms(\"clean\")\n#> # A tibble: 1 × 5\n#>   provider provider_title group        algorithm      algorithm_title\n#>   <chr>    <chr>          <chr>        <chr>          <chr>\n#> 1 grass    GRASS          Vector (v.*) grass:v.clean v.clean\nqgis_show_help(\"grass:v.clean\")\nqgis_get_argument_specs(\"grass:v.clean\") |>\n  select(name, description) |>\n  slice_head(n = 4)\n#> # A tibble: 4 × 2\n#>   name      description\n#>   <chr>     <chr>\n#> 1 input     Layer to clean\n#> 2 type      Input feature type\n#> 3 tool      Cleaning tool\n#> 4 threshold Threshold (comma separated for each tool)\nclean = qgis_run_algorithm(\"grass7:v.clean\",\n  input = union_sf,\n  tool = \"rmarea\", threshold = 25000\n)\nclean_sf = st_as_sf(clean)"},{"path":"gis.html","id":"qgis-raster","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.2.2 ラスタデータ","text":"デジタル標高モデル (Digital Elevation Model, DEM) には、ラスタセルごとの標高情報が含まれている。\nDEM は、衛星航法、水流モデル、表面分析、可視化など、さまざまな用途で使用されている。\nここでは、DEM ラスタから統計学習における予測因子として利用可能な新しい情報を導き出してみたい。\n例えば、様々な地形パラメータは、地滑りの予測に役立つ (Chapter 12 参照)。このセクションでは、dem.tif を使用することにする。これは、Mongón 調査地域のデジタル標高モデルである (Land Process Distributed Active Archive Center からダウンロード、?dem.tif も参照)。\n解像度は約 30 m × 30 m で、投影型 CRS を使用している。terra パッケージの terrain() では、傾斜、アスペクト、TPI (Topographic Position Index)、TRI (Topographic Ruggedness Index)、粗さ、流れ方向など、地形の基本特性を算出することができる。\nとはいえ、GIS は、地形の特性に関する機能が他にもたくさんあり、文脈によってはより適しているものもある。\n例えば、地形湿潤指数 (Topologic Wetness Index, TWI) は、水文・生物学的プロセスの研究に有用であることがわかっている (Sørensen, Zinko, Seibert 2006)。\nこのインデックスのアルゴリズムリストを、\"wetness\" というキーワードで検索してみよう。上記のコードの出力は、目的のアルゴリズムが SAGA ソフトウェアに存在することを示唆するものである。63\nSAGA はハイブリッド GIS であるが、主にラスタ処理、ここでは特にデジタル標高モデル (土壌特性、地形属性、気候パラメータ) に重点を置いている。\nしたがって、SAGA は大規模な (高解像度の) ラスタデータセットの高速処理に特に優れている (Conrad et al. 2015)。\"sagang:sagawetnessindex\" アルゴリズムは、実際には修正された TWI であり、谷底に位置するセルに対してより現実的な土壌水分ポテンシャルをもたらすものである (Böhner Selige 2006)。ここでは、デフォルトの引数を使用する。\n与える引数は、入力となる DEM だけである。\nこのアルゴリズムを使う際は、パラメータ値が研究の目的にあっているか確認する必要がある。64QGIS から SAGA アルゴリズムを使う前に、デフォルトのラスタ形式を .tif から、SAGA のデフォルトの .sdat に変更しておこう。\nこれで、指定しなければ保存形式は .sdat となる。\nソフトウェア (SAGA, GDAL) のバージョンによっては必要はないが、SAGA のラスタに関する問題を未然に防ぐことができる。\"sagang:sagawetnessindex\" は、集水域、集水勾配、修正集水域、地形湿潤指数という 4 つのラスタを返す。qgis_as_terra() 関数で出力名を指定することで、選択した出力を読み出すことができる。\n選択された出力は、qgis_as_terra() 関数に出力名を与えることで読むことができる。\nQGIS から SAGA を使う作業は終わったので、デフォルト形式を .tif に戻す。Figure 10.3 の左パネルに出力された TWI マップを見ることができる。\n地形湿潤指数には単位がない。数値が小さいほど水がたまらず、数値が大きいほど水がたまるエリアであることを示す。また、デジタル標高モデルからの情報は、例えばジオモルフォン (geomorphon) に分類することができる。地形は、斜面、尾根、谷などの地形を表す 10 のクラスからなる地形学的表現型である (Jasiewicz Stepinski 2013)。\nこれらの表現型は、地滑りしやすさ、生態系サービス、人間の移動性、デジタル土壌マッピングなど、多くの研究で利用されている。ジオモルフォンのアルゴリズムのオリジナルの実装は GRASS GIS で作成され、qgisprocess のリストで \"grass:r.geomorphon\" として見つけることができる。ジオモルフォンの計算には、入力 DEM (elevation) が必要で、オプションの引数でカスタマイズすることができる。\nフラグ search は視線距離を計算する長さ、および -m は検索値を (セル数ではなく) メートル単位で提供することを指定する。\n追加論点の詳細は、原著論文と GRASS GIS documentation に記載されている。出力される dem_geomorph$forms は、10 個のカテゴリからなるラスタファイルで、それぞれが地形形状を表している。\nこれを qgis_as_terra() で R に読み込んで可視化したり (Figure 10.3 右図)、その後の計算で使うことができる。興味深いことに、Figure 10.3 に示すように、いくつかの地形と TWI 値の間にはつながりがある。\nTWI 値が最も大きいのは谷や窪地であり、最も小さいのは予想通り尾根であった。\nFIGURE 10.3: 研究対象地 Mongón の地形湿潤指数 (TWI、パネル左) とジオモルフォン (パネル右)。\n","code":"\nlibrary(qgisprocess)\nlibrary(terra)\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nqgis_search_algorithms(\"wetness\") |>\n  dplyr::select(provider_title, algorithm) |>\n  head(2)\n#> # A tibble: 2 × 2\n#>   provider_title algorithm\n#>   <chr>          <chr>\n#> 1 SAGA Next Gen  sagang:sagawetnessindex\n#> 2 SAGA Next Gen  sagang:topographicwetnessindexonestep\nqgis_show_help(\"sagang:sagawetnessindex\")\noptions(qgisprocess.tmp_raster_ext = \".sdat\")\ndem_wetness = qgis_run_algorithm(\"sagang:sagawetnessindex\",\n  DEM = dem\n)\ndem_wetness_twi = qgis_as_terra(dem_wetness$TWI)\n# plot(dem_wetness_twi)\noptions(qgisprocess.tmp_raster_ext = \".tif\")\nqgis_search_algorithms(\"geomorphon\")\n#> [1] \"grass:r.geomorphon\" \"sagang:geomorphons\" \nqgis_show_help(\"grass:r.geomorphon\")\n# 出力は非表示\ndem_geomorph = qgis_run_algorithm(\"grass7:r.geomorphon\",\n  elevation = dem,\n  `-m` = TRUE, search = 120\n)\ndem_geomorph_terra = qgis_as_terra(dem_geomorph$forms)"},{"path":"gis.html","id":"saga","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.3 SAGA","text":"System Automated Geoscientific Analyses (SAGA; Table 10.1) は、コマンドラインインタフェース (Windows では saga_cmd.exe、Linux では単に saga_cmd) を介して SAGA モジュールを実行する可能性を提供する (SAGA wiki modules を参照)。\nまた、Python インターフェース (SAGA Python API) も用意されている。\nRsagacmd は、前者を使って R 内で SAGA を実行している。この Section では、Rsagacmd を使用して、SAGA の seeded region growing アルゴリズムを使って、2000年9月の Peru の Mongón 調査地域の正規化差分植生指数 (normalized difference vegetation index, NDVI) の値が類似した地域を抽出する (Figure 10.4 左図)。65Rsagacmd を始めるには、saga_gis() 関数を実行する必要がある。\nこの関数は主に 2 つの目的がある。有効な SAGA ライブラリやツールへのリンクを含む新しいオブジェクトを動的に作成すること66raster_backend (ラスタデータを扱う際に用いる R パッケージ)、vector_backend (ベクタデータを扱う際に用いる R パッケージ)、cores (処理に用いる CPU コアの最大数、デフォルトは ) など、一般的なパッケージオプションを設定することこの saga オブジェクトは、利用可能なすべての SAGA ツールへの接続を含んでいる。\nこれはライブラリ (ツールのグループ) のリストとして構成されており、ライブラリの内部にはツールのリストがある。\nどのツールにも $ 記号でアクセスできる (TAB キーで自動補完することが可能)。シード領域拡大アルゴリズムは，主に 2 つのステップで動作する (Adams Bischof 1994; Böhner, Selige, Ringeler 2006)。\nまず、指定されたサイズのローカルウィンドウにおいて、最も分散の小さいセルを見つけることで、初期セル (seed) が生成される。\n次に、領域成長アルゴリズムを用いて、seed の近傍画素をマージし、均質な領域を作成する。上記の例では、まず imagery_segmentation ライブラリを示し、次にその seed_generation ツールを使用した。\nまた、次のステップでツールのコード全体を再入力しないように、sg オブジェクトに割り当てる。67\nsg と入力することで、ツールの簡単な概要と、パラメータ、説明、およびデフォルトのデータフレームが表示される。\nまた、tidy(sg) を使用すると、パラメータのテーブルだけを取り出すことができる。\nseed_generation ツールは、引数にラスタデータ (features) を必要とする。また、初期ポリゴンのサイズを指定する band_width などの追加パラメータを提供することができる。この出力は、3 つのオブジェクトからなるリストである。variance は局所分散のラスタマップ、 seed_grid は生成されたシードを含むラスタマップ、seed_points は生成されたシードを含む空間ベクタオブジェクトである。つぎの SAGA ツールとして seeded_region_growing を紹介しよう。68\nseed_region_growing ツールは、前のステップで計算した seed_grid と ndvi ラスタオブジェクトの 2 つの入力を必要とする。\nさらに、入力フィーチャを標準化するための normalize や neighbour (4 または 8-neighborhood)、 method などのパラメータを指定することができる。\n最後のパラメータには、0 または 1 を指定することができる (ラスタセルの値とその位置に基づいて領域を成長させるか、値のみを成長させるか)。\nこのメソッドの詳細な説明は、 Böhner, Selige, Ringeler (2006) を参照。ここでは、method を 1 に変更するだけである。つまり、出力される地域は、NDVI 値の類似性に基づいてのみ作成されることを意味する。このツールは、3 つのオブジェクトのリストを返す。このツールは、segments、similarity、table という 3 つのオブジェクトのリストを返す。\nsimilarity オブジェクトは、シードと他のセルとの類似性を示すラスタであり、table は入力シードに関する情報を格納したデータフレームである。\n最後に、ndvi_srg$segments は、結果として得られた領域 (Figure 10.4 右図) を表すラスタである。\nこれをポリゴンに変換するには、.polygons() と st_as_sf() を使用する (Section 6.5)。\nFIGURE 10.4: 正規化差分植生指数 (NDVI、左図) と、Mongón調査地域のシード領域成長アルゴリズムを用いて得られた NDVI ベースのセグメント。\n結果として得られるポリゴン (セグメント) は、類似した値を持つ領域を表す。\nまた、クラスタリング (k-means など)、地域化 (SKATER など)、教師あり分類法など、さまざまな手法でさらに大きなポリゴンに集約することができる。\n演習で試すことができる。R には、似たような値を持つポリゴン (いわゆるセグメント) を作成するという目的を達成するための他のツールもある。\nいくつかの画像分割アルゴリズムを実行できる SegOptim パッケージ (Gonçalves et al. 2019) や、地理空間データを扱うためにスーパーピクセルアルゴリズム SLIC を実装した supercells (Nowosad Stepinski 2022) などが含まれている。","code":"\nndvi = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))\nlibrary(Rsagacmd)\nsaga = saga_gis(raster_backend = \"terra\", vector_backend = \"sf\")\nsg = saga$imagery_segmentation$seed_generation\nndvi_seeds = sg(ndvi, band_width = 2)\n#plot(ndvi_seeds$seed_grid)\nsrg = saga$imagery_segmentation$seeded_region_growing\nndvi_srg = srg(ndvi_seeds$seed_grid, ndvi, method = 1)\nplot(ndvi_srg$segments)\nndvi_segments = ndvi_srg$segments |>\n  as.polygons() |>\n  st_as_sf()"},{"path":"gis.html","id":"grass","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.4 GRASS GIS","text":"米国陸軍建設工学研究所 (U.S. Army - Construction Engineering Research Laboratory, USA-CERL) は、1982年から1995年にかけて、地理資源解析支援システム (Geographical Resources Analysis Support System, GRASS GIS) の中核となるシステムを作成した [Table 10.1; Neteler Mitasova (2008)]。\nアカデミアは1997年からこの作業を継続した。\nSAGA と同様、グラスも当初はラスタ処理に注力し、その後、GRASS GIS 6.0 以降、高度なベクタ機能を追加している (Bivand, Pebesma, Gómez-Rubio 2013)。GRASS は、入力データを内部データベースに格納する。\nベクタデータに関して、GRASS GIS はデフォルトでトポロジカル GIS、すなわち隣接するフィーチャのジオメトリを一度だけ保存する。\nベクタ属性の管理にはデフォルトで SQLite を用い、属性はキーによってジオメトリ、すなわち GRASS GIS データベースにリンクされる (GRASS GIS vector management)。GRASS GIS を使う前に、GRASS GIS データベースを (R からも) セットアップする必要があるが、このプロセスに少し戸惑うかもしれない。\nまず、GRASS GIS のデータベースは専用のディレクトリを必要とし、そのディレクトリには location を置く必要がある (詳しくは grass.osgeo.org の GRASS GIS Database ヘルプページを参照)。\nlocation には、1 つのプロジェクトまたは 1 つの領域のジオデータが格納される。\n通常、1 つの場所の中に、異なるユーザや異なるタスクを参照するいくつかのマップセットを存在させることができる。\n各 location には、PERMANENT マップセット (自動的に作成される必須のマップセット) もある。\nプロジェクトのすべてのユーザと地理データを共有するために、データベース所有者は PERMANENT マップセットに空間データを追加することができる。\nさらに、PERMANENT マップセットには、ラスタデータの投影法、空間範囲、およびデフォルトの解像度が格納される。\nまとめると、GRASS GIS データベースは多くの location を含み (1 つのロケーションのデータはすべて同じ CRS を持つ)、それぞれの location は多くのマップセット (データセットのグループ) を格納することができる。\nGRASS GIS 空間データベースシステムの詳細は、Neteler Mitasova (2008) と GRASS GIS quick start を参照。\nこkでは R から手軽に GRASS GIS を使うため link2GI パッケージを使う。しかし、GRASS GIS データベースを順番に作ることもできる。\n作り方は GRASS within R を参照。\n以下の段落で解説するコードは、初めて GRASS GIS を使う方には難しいかもしれないが、コードを 1 行 1 行実行して途中の結果を確認することで、コードの元となる理由が明らかになる。ここでは、GIScience における最も興味深い問題の一つである巡回セールスマン問題 を用いた rgrass を紹介する。\nある巡回セールスマンが 24 件の顧客を訪問したいとする。\nさらに自宅を起点かつ終点とするので、結果的に 25 カ所を、最短距離で回りたい。\nこの問題に対する最適解は一つであるが、考えられる解をすべてチェックすることは、現代のコンピュータでは (ほとんど) 不可能である (Longley 2015)。\nこの場合、可能な解の数は (25 - 1)! / 2、すなわち 24 の階乗を 2 で割った数に相当する (2 で割るのは、順方向と逆方向を区別しないため)。\n1 回の繰り返しがナノ秒でも、9837145 年間に相当する。\n幸いなことに、この想像を絶する時間のごく一部で実行できる、巧妙でほぼ最適なソリューションがある。\nGRASS GIS は、これらの解決策の一つを提供する (詳細は、 v.net.salesmanを参照)。\n今回の使用例では、ロンドンの街角にある最初の 25 レンタルサイクルのステーション (顧客の代わり) 間の最短経路を見つけたい (最初の自転車ステーションは、巡回セールスマンの自宅に相当すると仮定する)。レンタルサイクルのステーションのデータの他に、この地域の道路網が必要である。\nosmdata パッケージで OpenStreetMap から ダウンロードすることができる (Section 8.5 も参照)。\nそのために、道路網のクエリ (OSM 言語では “highway” とラベル付けされている) を points のバウンディングボックス に制限し、対応するデータを sf オブジェクトとして読み込む。\nosmdata_sf() は、複数の空間オブジェクト (点、線、ポリゴンなど) を含むリストを返すが、ここでは線オブジェクトとその関連 ID のみを保持する。69これでデータが揃ったので、次に GRASS GIS のセッションを開始する。\n幸い、link2GI パッケージの linkGRASS() を使えば、たった一行のコードで GRASS GIS 環境をセットアップできる。\n空間オブジェクトは、空間データベースの投影と範囲を決定するものである。\nまず、linkGRASS() は、あなたのコンピュータにインストールされている全ての GRASS GIS を検索する。\nここでは ver_select を TRUE に設定しているので、見つかった GRASS GIS-installation の中から対話的に一つを選択することができる。\nもし、インストールが一つしかない場合は、linkGRASS() が自動的にそれを選択する。\n次に、linkGRASS() は、GRASS GIS への接続を確立する。GRASS GIS のジオアルゴリズムを使用する前に、GRASS GIS の空間データベースにデータを追加する必要がある。\n幸いなことに、便利な関数 write_VECT() がこれを代行してくれる。\n(ラスタデータには write_RAST() を使用する。)\nこの例では、最初の属性列のみを使用して、道路と自転車レンタル点データを追加し、GRASS GIS で london_streets と points という名前を付けている。rgrass パッケージは、入力と出力が terra オブジェクトであることを想定している。\nしたがって、write_VECT() を使用するためには、vect() 関数を使用して sf 空間ベクタを terra の SpatVector に変換する必要がある。70現在、両方のデータセットが GRASS GIS のデータベースに存在している。\nネットワークの解析を行うには、トポロジカルクリーンな道路ネットワークが必要である。\nGRASS GIS の \"v.clean\" は、重複、小角、ダングルの除去などを行う。\nここでは、後続のルート検索アルゴリズムが実際に交差点で右折または左折できるように、各交差点で改行し、その出力を streets_clean という名前の GRASS GIS オブジェクトに保存している。レンタルサイクルのステーションのいくつかのポイントは、正確に街路セグメント上に位置しない可能性がある。\nしかし、それらの間の最短経路を見つけるために、それらを最も近い道路セグメントに接続する必要がある。\n\"v.net\"のconnect-operatorはまさにこれを行う。\nその出力を streets_points_con に保存する。得られたクリーンなデータセットは \"v.net.salesman\" アルゴリズムの入力となり、最終的にすべての自転車レンタルステーション間の最短経路を見つけることができる。\nその引数の一つが center_cats で、これは入力として数値の範囲を必要とする。\nこの範囲は、最短ルートを計算するためのポイントを表している。\nここでは、すべての自転車ステーション間の経路を計算したいので、1-25 に設定しておく。\n巡回セールスマンアルゴリズムの GRASS GIS ヘルプページを参照するには、execGRASS(\"g.manual\", entry = \"v.net.salesman\") を実行する。結果を見るには、結果を R に入れ、ジオメトリのみ保持した sf オブジェクトに変換する。これを mapview で可視化する (Figure 10.5 と Section 9.4)。\nFIGURE 10.5: ロンドンの OSM 道路網の24の自転車レンタル地点 (青点) とその最短ルート (青線)\nその際、いくつか注意すべき点がある。GRASS GIS の空間データベースを使えば、より高速に処理できた。\nしかし、ここでは最初に地理データを書き出した。\nそして、新しいオブジェクトを作成し、最終結果だけを R にインポートした。\n現在利用可能なデータセットを調べるには、execGRASS(\"g.list\", type = \"vector,raster\", flags = \"p\") を実行する。また、R から既にある GRASS GIS 空間データベースにアクセスすることも可能であった。\nR にデータをインポートする前に、いくつかの (空間) 部分集合を作成したい場合がある。\nベクタデータには \"v.select\" と \"v.extract\" を使用する。\n\"db.select\" を使用すると、対応するジオメトリを返さずにベクタレイヤの属性テーブルの部分集合を選択することができる。また、実行中の GRASS GIS のセッションから R を起動することもできる (詳細は Bivand, Pebesma, Gómez-Rubio 2013 を参照)。GRASS GIS で提供されて入るジオアルゴリズムの素晴らしいドキュメントは、 GRASS GIS online help または execGRASS(\"g.manual\", flags = \"\") を参照。","code":"\ndata(\"cycle_hire\", package = \"spData\")\npoints = cycle_hire[1:25, ]\nlibrary(osmdata)\nb_box = st_bbox(points)\nlondon_streets = opq(b_box) |>\n  add_osm_feature(key = \"highway\") |>\n  osmdata_sf()\nlondon_streets = london_streets[[\"osm_lines\"]]\nlondon_streets = select(london_streets, osm_id)\nlibrary(rgrass)\nlink2GI::linkGRASS(london_streets, ver_select = TRUE)\nwrite_VECT(terra::vect(london_streets), vname = \"london_streets\")\nwrite_VECT(terra::vect(points[, 1]), vname = \"points\")\nexecGRASS(\n  cmd = \"v.clean\", input = \"london_streets\", output = \"streets_clean\",\n  tool = \"break\", flags = \"overwrite\"\n)\nexecGRASS(\n  cmd = \"v.net\", input = \"streets_clean\", output = \"streets_points_con\",\n  points = \"points\", operation = \"connect\", threshold = 0.001,\n  flags = c(\"overwrite\", \"c\")\n)\nexecGRASS(\n  cmd = \"v.net.salesman\", input = \"streets_points_con\",\n  output = \"shortest_route\", center_cats = paste0(\"1-\", nrow(points)),\n  flags = \"overwrite\"\n)\nroute = read_VECT(\"shortest_route\") |>\n  st_as_sf() |>\n  st_geometry()\nmapview::mapview(route) + points"},{"path":"gis.html","id":"いつ何を使うべきか","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.5 いつ、何を使うべきか？","text":"R-GIS のインターフェースは、個人の好みや作業内容、GIS の使い方に依存するため、一概にお勧めすることはできないし、研究分野にもよるだろう。\n前述の通り、SAGA は大規模 (高解像度) ラスタデータセットの高速処理に特に優れており、水文学者、気候学者、土壌学者に頻繁に利用されている (Conrad et al. 2015)。\n一方、GRASS GIS は、トポロジーに基づく空間データベースをサポートする唯一の GIS であり、ネットワーク分析だけでなくシミュレーション研究にも特に有用である。\nQGIS は、GRASS GIS や SAGA と比較して、特に初めて GIS を使う方にとって使いやすく、おそらく最も人気のあるオープンソースの GIS だと思われる。\nしたがって、qgisprocess は、ほとんどのユースケースに適切な選択である。\nその主なメリットは複数の GIS に統一的にアクセスできるため、重複した機能を含む 1,000 以上のジオアルゴリズム ( Table 10.1 ) を提供。例えば、QGIS、SAGA、GRASS GIS などのジオアルゴリズムを使ってオーバーレイ操作を実行することが可能である。データ形式の自動変換 (SAGAは .sdat グリッドファイル、GRASS GIS は独自のデータベース形式を使用するが、対応する変換は QGIS が行う。)地理的な R オブジェクトを QGIS ジオアルゴリズムに自動的に渡し、R に戻すことができる。名前付き引数、デフォルト値の自動取得をサポートする便利な機能 (rgrass からインスパイアされた)もちろん、他の R-GIS ブリッジを使用した方が良いケースもある。\nQGIS は、複数の GIS ソフトウェアパッケージへの統一インターフェースを提供する唯一の GIS であるが、対応するサードパーティのジオアルゴリズムのサブセットへのアクセスしか提供しない (詳細については、Muenchow, Schratz, Brenning (2017) を参照)。\nしたがって、SAGA と GRASS GIS の関数一式を使用するには、RSAGA と rgrass 以外は使わない方が良い。\nまた、ジオデータベースを用いてシミュレーションを行いたい場合 (Krug, Roura-Pascual, Richardson 2010)、qgisprocess が、呼び出しごとに常に新しい GRASS GIS セッションを開始するので、rgrass を直接使用してみよう。\n最後に、地形データ、空間データベース管理機能 (マルチユーザーアクセスなど) が必要な場合は、GRASS GIS の利用を勧める。なお、スクリプティング・インターフェースを持つ GIS ソフトウェアパッケージは以下のように数多くあるが、これらを利用できる専用の R パッケージはない: gvSig、OpenJump、Orfeo Toolbox。71","code":""},{"path":"gis.html","id":"gdal","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.6 GDAL へのブリッジ","text":"Chapter 8 で述べたように、GDAL は多くの地理データ形式をサポートする低レベルのライブラリである。\nGDAL は非常に効果的なので、ほとんどの GIS プログラムは、車輪の再発明や特注の読み書きコードを使用するのではなく、地理データのインポートとエクスポートのためにバックグラウンドで GDAL を使用している。\nしかし、GDAL が提供するのは、データ入出力だけではない。\nベクタデータとラスタデータの geoprocessing tools、ラスタデータをオンラインで提供するためのタイルを作成する機能、ベクタデータの高速ラスタ化がある。\nGDAL はコマンドラインツールであるため、R からは system() コマンドからアクセスすることができる。以下のコードは、この機能を実現するものである。\nlinkGDAL() は、GDAL が動作しているコンピュータを検索し、実行ファイルの場所を PATH 変数に追加して、GDAL を呼び出せるようにする (Windows で通常必要になる)。これで、system() 関数を使用して、任意の GDAL ツールを呼び出すことができる。\n例えば、ogrinfo は、ベクタデータセットのメタデータを提供する。\nここでは、このツールに 2 つのフラグを追加して呼び出する。 -al は全レイヤの全フィーチャをリストアップし、-は要約のみを取得する (完全なジオメトリのリストではない)。その他、よく使われる GDAL のツールは以下の通りgdalinfo: ラスタデータセットのメタデータを提供gdal_translate: 異なるラスタファイル形式間の変換ogr2ogr: 異なるベクタファイル形式間で変換gdalwarp: ラスタデータセットの再投影、変換、切り抜き (clip)gdaltransform: 座標変換GDAL ツールの全リストとそのヘルプファイルは https://gdal.org/programs/ 。link2GI が提供する GDAL への「リンク」は、R やシステムの CLI からより高度な GDAL の作業を行うための基盤として利用することができるだろう。\nTauDEM (http://hydrology.usu.edu/taudem) や Orfeo Toolbox (https://www.orfeo-toolbox.org/) は、コマンドラインインタフェースを提供する空間データ処理ライブラリ/プログラムである。上記の例は、R を介してシステムのコマンドラインからこれらのライブラリにアクセスする方法である。\nこれは、新しい R パッケージという形で、これらのライブラリへの適切なインタフェースを作成するための出発点となる可能性がある。しかし、新しいブリッジを作成するプロジェクトに飛び込む前に、既存の R パッケージのパワーと、system() の呼び出しがプラットフォームに依存しない (一部のコンピュータで失敗する) 可能性があることを認識しておくことが重要である。\n一方、sf は GDAL、GEOS、PROJ が提供するパワーのほとんどを Rcpp が提供する R/C++ インターフェースを介して R にもたらし、system() の呼び出しを回避している。72","code":"\nlink2GI::linkGDAL()\nour_filepath = system.file(\"shapes/world.gpkg\", package = \"spData\")\ncmd = paste(\"ogrinfo -al -so\", our_filepath)\nsystem(cmd)\n#> INFO: Open of `.../spData/shapes/world.gpkg'\n#>       using driver `GPKG' successful.\n#>\n#> Layer name: world\n#> Geometry: Multi Polygon\n#> Feature Count: 177\n#> Extent: (-180.000000, -89.900000) - (179.999990, 83.645130)\n#> Layer SRS WKT:\n#> ..."},{"path":"gis.html","id":"postgis","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.7 空間データベースへのブリッジ","text":"\n空間データベース管理システム (空間 DBMS) は、空間および非空間データを構造化して保存する。\n大規模なデータの集合を、一意の識別子 (主キーと外部キー) および暗黙のうちに空間を介して関連するテーブル (エンティティ) に整理することができる (たとえば、空間結合を考えてみてみよう)。\n地理的なデータセットはすぐに大きくなったり、乱雑になったりする傾向があるため、この機能は便利である。\nデータベースは、空間および非空間フィールドに基づく大規模なデータセットの保存とクエリを効率的に行うことができ、マルチユーザーアクセスとトポロジーのサポートを提供する。最も重要なオープンソースの空間データベースは PostGIS である (Obe Hsu 2015)。73\nPostGIS のような空間 DBMS への R ブリッジは重要で、数ギガバイトの地理データを RAM にロードすることなく、R セッションをクラッシュさせる可能性があるような巨大なデータストアにアクセスできる。\nこのセクションの残りの部分では、PostGIS Action, Second Edition の “Hello real world” に基づいて、R から PostGIS を呼び出す方法を紹介する (Obe Hsu 2015)。74QGIS Cloud (https://qgiscloud.com/) にある PostgreSQL/PostGIS データベースにアクセスしているため、この後のコードはインターネット接続している必要がある。75\n最初のステップは、データベース名、ホスト名、およびユーザー情報を指定して、データベースへの接続を作成することである。新しいオブジェクト conn は、R セッションとデータベースの間のリンクを確立したに過ぎない。\nデータを保存することはない。多くの場合、最初の質問は「データベースからどのテーブルが見つかるか」である。\nこれには、dbListTables() で次のように答えることができる。答えは、この 5 つのテーブルである。\nここでは、restaurants と highways のテーブルのみを対象としている。\n前者は米国内のファストフード店の位置を、後者は米国の主要な高速道路を表している。\nテーブルで利用可能な属性について調べるには、dbListFields を実行する。さて、利用可能なデータセットがわかったところで、いくつかのクエリを実行し、データベースに質問することができる。\nクエリは、データベースが理解できる言語 (通常はSQL) で提供される必要がある。\n最初のクエリは、highways テーブルから Maryland 州 (MD) の US Route 1 を選択する。\nなお、read_sf() は、データベースへのオープンな接続とクエリが提供されれば、データベースから地理データを読み込むことができる。\nさらに、read_sf() は、どの列がジオメトリを表すかを知る必要がある (ここでは、wkb_geometry)。この結果、MULTILINESTRING 型の us_route という名前の sf オブジェクトが生成される。また、前述したように、非空間的な質クエリだけでなく、空間的な性質をもとにデータセットをクエリすることも可能である。\nこれを示すために、次の例では選択した高速道路 (Figure 10.6) の周囲に 35 km (35,000 m) のバッファを追加している。なお、これはおそらく読者がすでに知っている (ST_Union()、ST_Buffer()) を使った空間クエリであった。\nまた、sf パッケージにも同名のものがあるが、こちらは小文字になっている (st_union()、st_buffer())。\n実際、sf パッケージの関数名は、PostGIS の命名規則にほぼ従っている。76最後のクエリは、35 km のバッファゾーン (Figure 10.6) 内にあるすべてのハーディーズレストラン (HDE) を検索する。空間 SQL クエリの詳細な説明は Obe Hsu (2015) を参照。\n最後に、次のようにデータベース接続を閉じるのがよい方法である。77\nFIGURE 10.6: 直前の PostGIS コマンドによる出力の例。高速道路 (黒線)、バッファ (黄色)、バッファ内の 4 つのレストラン (赤点)。\nPostGIS とは異なり、sf は空間ベクタデータのみをサポートしている。\nPostGIS データベースに格納されたラスタデータを照会・操作するには、rpostgis パッケージ (Bucklin Basille 2018)、または PostGIS インストールの一部に含まれる rastertopgsql などのコマンドラインツールを使用する必要がある。このサブセクションでは、PostgreSQL/PostGIS の簡単な紹介にとどめる。\nそれでも、地理的および非地理的データを空間 DBMS で保存しながら、さらなる (地理) 統計解析に必要なそれらのサブセットだけを R のグローバル環境にアタッチするという実践を奨励したい。\n提示された SQL クエリのより詳細な説明と PostgreSQL/PostGIS 一般のより包括的な紹介は Obe Hsu (2015) を参照。\nPostgreSQL/PostGIS は、非常に難解なオープンソースの空間データベースである。\nしかし、軽量なデータベースエンジンである SQLite/SpatiaLite や、バックグラウンドで SQLite を使用する GRASS GIS も同様と言える (Section 10.4 参照)。データセットが PostgreSQL/PostGIS では大きすぎる場合、大規模な空間データ管理とクエリ性能を必要とする場合、分散コンピューティングシステム上での大規模な地理クエリを検討する価値があるかもしれない。\nこのようなシステムは本書の範囲外ではあるが、この機能を提供するオープンソースソフトウェアが存在することは触れておく価値がある。\nこの分野の著名なプロジェクトには、GeoMesa と Apache Sedona がある。\n後者については、apache.sedona パッケージがインタフェースを提供している。","code":"\nlibrary(RPostgreSQL)\nconn = dbConnect(\n  drv = PostgreSQL(),\n  dbname = \"rtafdf_zljbqm\", host = \"db.qgiscloud.com\",\n  port = \"5432\", user = \"rtafdf_zljbqm\", password = \"d3290ead\"\n)\ndbListTables(conn)\n#> [1] \"spatial_ref_sys\" \"topology\"        \"layer\"           \"restaurants\"\n#> [5] \"highways\"\ndbListFields(conn, \"highways\")\n#> [1] \"qc_id\"        \"wkb_geometry\" \"gid\"          \"feature\"\n#> [5] \"name\"         \"state\"\nquery = paste(\n  \"SELECT *\",\n  \"FROM highways\",\n  \"WHERE name = 'US Route 1' AND state = 'MD';\"\n)\nus_route = read_sf(conn, query = query, geom = \"wkb_geometry\")\nquery = paste(\n  \"SELECT ST_Union(ST_Buffer(wkb_geometry, 35000))::geometry\",\n  \"FROM highways\",\n  \"WHERE name = 'US Route 1' AND state = 'MD';\"\n)\nbuf = read_sf(conn, query = query)\nquery = paste(\n  \"SELECT *\",\n  \"FROM restaurants r\",\n  \"WHERE EXISTS (\",\n  \"SELECT gid\",\n  \"FROM highways\",\n  \"WHERE\",\n  \"ST_DWithin(r.wkb_geometry, wkb_geometry, 35000) AND\",\n  \"name = 'US Route 1' AND\",\n  \"state = 'MD' AND\",\n  \"r.franchise = 'HDE');\"\n)\nhardees = read_sf(conn, query = query)\nRPostgreSQL::postgresqlCloseConnection(conn)#> \n#> ── tmap v3 code detected ───────────────────────────────────────────────────────\n#> [v3->v4] `tm_polygons()`: use `col_alpha` instead of `border.alpha`."},{"path":"gis.html","id":"cloud","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.8 クラウドへのブリッジ","text":"近年、インターネット上では、クラウド技術の利用が目立ってきている。\nこの中には、空間データの保存や処理に利用されることも含まれている。\nAmazon Web Services、Microsoft Azure / Planetary Computer、Google Cloud Platform などの主要なクラウドコンピューティングプロバイダは、Sentinel-2 アーカイブのようなオープンな地球観測データの巨大なカタログをプラットフォーム上で提供している。\nR を使えば、これらのアーカイブから直接データに接続し、処理することができる。理想的には、同じクラウドや地域のマシンから接続することができる。このような画像アーカイブをクラウド上でより簡単に、より効率的に利用するために、SpatioTemporal Asset Catalog (STAC)、cloud-optimized GeoTIFF (COG) 画像形式、データキューブが有望視されている。\nSection 10.8.1 では、これらの個々の開発について紹介し、R からどのように利用できるかを簡単に説明する。ここ数年、大規模なデータアーカイブをホストするだけでなく、地球観測データを処理するクラウドベースのサービスも多数始まっている。\nその中には、R を含むプログラミング言語と様々なクラウドサービスとの間の統一的なインタフェースである OpenEO イニシアチブも含まれている。\nOpenEO の詳細については、Section 10.8.2 を参照。","code":""},{"path":"gis.html","id":"staccog","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.8.1 クラウドの STAC、COG、その他のデータキューブ","text":"STAC (SpatioTemporal Asset Catalog) は、時空間データの汎用記述フォーマットで、画像、合成開口レーダー (synthetic aperture radar, SAR) データ、点群など、クラウド上の様々なデータセットの記述に使用されている。\nSTAC-API は、単純な静的カタログ記述の他に、カタログのアイテム (画像など) を空間、時間、その他のプロパティで照会するウェブサービスを提供している。\nR では、rstac パッケージ (Simoes, Souza, et al. 2021) が STAC-API エンドポイントに接続し、アイテムを検索することができる。\n以下の例では、Sentinel-2 Cloud-Optimized GeoTIFF (COG) dataset Amazon Web Services から、事前に定義した関心領域と時間に交差するすべての画像を要求している。\n結果は、見つかったすべての画像とそのメタデータ (雲量など)、および AWS 上の実際のファイルを指す URL を含んでいる。クラウドストレージはローカルのハードディスクとは異なり、従来の画像ファイル形式はクラウドベースのジオプロセシングではうまく機能しない。\nクラウドに最適化された GeoTIFF は、画像の矩形部分や低解像度の画像の読み込みが非常に効率的になる。\nGDAL (およびそれを使ったパッケージ) はすでに COG を扱うことができるので、R ユーザーであれば COG を扱うために何かをインストールする必要はない。\nただし、データ提供者のカタログを閲覧する際には、COG が利用可能であることが大きなプラスになることを覚えておこう。領域が大きい時、要求された画像を扱うのはまだ比較的困難である。それらは異なる地図投影を使用することがあり、空間的に重なることがあり、空間解像度はしばしばスペクトルバンドに依存する。\ngdalcubes パッケージ (Appel Pebesma 2019) は、個々の画像から抽象化し、画像コレクションを 4 次元データキューブとして作成し処理するために使用することができる。以下のコードは、前回の STAC-API 検索で返された Sentinel-2 画像から、低解像度 (250 m) の最大 NDVI コンポジットを作成する最小限の例を示している。クラウドカバーによる画像のフィルタリングを行うために、画像コレクションを作成する際に各 STAC 結果アイテムに適用されるプロパティフィルタ関数を提供している。\nこの関数は、画像の利用可能なメタデータを入力リストとして受け取り、関数がTRUEを返す画像のみを考慮するような単一の論理値を返す。\nこの場合、10% 以上のクラウドカバーがある画像は無視する。\n詳しくは、こちらの OpenGeoHub サマースクール 2021 で発表したチュートリアルを参照。78STAC、COGs、データキューブ を組み合わせて、衛星画像の (大規模) コレクションをクラウド上で解析するクラウドネイティブワークフローを形成する.\nこれらのツールは、例えば、大規模な地球観測データの土地利用や土地被覆の分類を可能にする sits パッケージのバックボーンを既に形成している。\nこのパッケージは、クラウドサービスで利用可能な画像コレクションから EO データキューブを構築し、様々な機械学習と真相学習アルゴリズムを用いてデータキューブの土地分類を実行するものである。\nsits の詳細については、https://e-sensing.github.io/sitsbook/ または関連記事 (Simoes, Camara, et al. 2021) を参照。","code":"\nlibrary(rstac)\n# Sentinel-2 データの STAC-API endpoint に接続し、\n# AOI と交差する画像を検索\ns = stac(\"https://earth-search.aws.element84.com/v0\")\nitems = s |>\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = c(7.1, 51.8, 7.2, 52.8),\n              datetime = \"2020-01-01/2020-12-31\") |>\n  post_request() |>\n  items_fetch()\nlibrary(gdalcubes)\n# クラウドカバーで画像をフィルタし、画像コレクションを生成\ncloud_filter = function(x) {\n    x[[\"eo:cloud_cover\"]] < 10\n}\ncollection = stac_image_collection(items$features, \n                                   property_filter = cloud_filter)\n# データキューブの範囲、解像度 (250m、毎日)、CRS を定義\nv = cube_view(srs = \"EPSG:3857\", extent = collection, dx = 250, dy = 250,\n              dt = \"P1D\") # \"P1D\" は ISO 8601 期間文字列\n# データキューブを生成し処理\ncube = raster_cube(collection, v) |>\n  select_bands(c(\"B04\", \"B08\")) |>\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |>\n  reduce_time(\"max(NDVI)\")\n# gdalcubes_options(parallel = 8)\n# plot(cube, zlim = c(0, 1))"},{"path":"gis.html","id":"openeo","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.8.2 openEO","text":"OpenEO (Schramm et al. 2021) は、データ処理のための共通言語を定義することによって、クラウドサービス間の相互運用性を支援するイニシアチブである。\n最初のアイデアはr-spatial.org blog postで説明されており、ユーザーができるだけ少ないコード変更で簡単にクラウドサービス間を変更できるようにすることを目的としている。\n標準化プロセスでは、データへのインタフェースとして多次元データキューブモデルを使用している。\n8 種類のバックエンドの実装が用意されており (https://hub.openeo.org)、ユーザーは R、Python、JavaScript、QGIS、Web エディタで接続し、コレクションに対してプロセスを定義 (およびチェーン) することができる。\nバックエンドによって機能や利用できるデータが異なるため、openeo R パッケージ (Lahn 2021) は接続されたバックエンドから利用できるプロセスとコレクションを動的にロードする。\nその後、ユーザーは画像コレクションのロード、プロセスの適用と連鎖、ジョブの送信、結果の探索とプロットを行うことができる。以下のコードは、openEO platform backend に接続し、利用可能なデータセット、プロセス、出力フォーマットを要求し、Sentinel-2 データから最大 NDVI 画像を計算するプロセスグラフを定義し、最後にバックエンドにログインした後にグラフを実行する。\nopenEO プラットフォームのバックエンドには無料版があり、既存の機関やインターネットフォームのアカウントから登録することが可能である。","code":"\nlibrary(openeo)\ncon = connect(host = \"https://openeo.cloud\")\np = processes() # 利用可能なプロセスをロード\ncollections = list_collections() # 利用可能なコレクションをロード\nformats = list_file_formats() # 利用可能な出力フォーマットをロード\n# Sentinel-2 コレクションをロード\ns2 = p$load_collection(id = \"SENTINEL2_L2A\",\n                       spatial_extent = list(west = 7.5, east = 8.5,\n                                             north = 51.1, south = 50.1),\n                       temporal_extent = list(\"2021-01-01\", \"2021-01-31\"),\n                       bands = list(\"B04\", \"B08\"))\n# NDVI vegetation index を計算\ncompute_ndvi = p$reduce_dimension(data = s2, dimension = \"bands\",\n                                  reducer = function(data, context) {\n                                      (data[2] - data[1]) / (data[2] + data[1])\n                                  })\n# maximum over time を計算\nreduce_max = p$reduce_dimension(data = compute_ndvi, dimension = \"t\",\n                                reducer = function(x, y) {\n                                    max(x)\n                                })\n# GeoTIFF で出力\nresult = p$save_result(reduce_max, formats$output$GTiff)\n# ログイン https://docs.openeo.cloud/getting-started/r/#authentication 参照\nlogin(login_type = \"oidc\", provider = \"egi\", \n      config = list(client_id = \"...\", secret = \"...\"))\n# プロセスを実行\ncompute_result(graph = result, output_file = tempfile(fileext = \".tif\"))"},{"path":"gis.html","id":"演習-8","chapter":"10 GIS ソフトウェアへのブリッジ","heading":"10.9 演習","text":"E1. qgisprocess で r.sun GRASS GIS を使用して、system.file(\"raster/dem.tif\", package = \"spDataLarge\") の3月21日午前11時の全球日射量を計算しなさい。E2. **Rsagacmd* を使い、system.file(\"raster/dem.tif\", package = \"spDataLarge\") の集水域と集水勾配を計算しなさい。E3. SAGA セクションで作成した ndvi_segments オブジェクトの作業を続けなさい。\nndvi ラスターから平均 NDVI 値を抽出し、kmeans() を使用して 6 つのクラスターにグループ化しなさい。\n結果を可視化しなさい。E4. data(random_points, package = \"spDataLarge\") をアタッチし、 system.file(\"raster/dem.tif\", package = \"spDataLarge\") を　R に読み込みなさい。\nrandom_points からランダムに点を選択し、この点から見えるすべての dem ピクセルを見つけなさい (ヒント: viewhedindex{viewshed} は GRASS GIS を使って計算できる)。\n結果を視覚化する。\n例えば、hillshade、digital elevation model、viewhed 出力、ポイントをプロットしなさい。\nさらに、mapview を試してみよう。E5. システムコールで gdalinfo を 使い、好きなディスクに保存されているラスタファイルを見なさい。\nどのような情報があるか？E6. gdalwarp を使ってラスタファイルの解像度を下げなさい (例えば、解像度が 0.5 の場合、それを 1 に変更する)。注意: この演習では -tr と -r フラグを使用する。E7. この章で紹介したクラウド QGIS にある PostgreSQL/PostGIS データベースからすべてのカリフォルニアの高速道路をクエリしなさい。E8. ndvi.tif ラスタ (system.file(\"raster/ndvi.tif\", package = \"spDataLarge\")) は、2000年9月22日のランドサットデータに基づいて Mongón の調査地域で計算された NDVI を含んでいる。\nrstac、gdalcubes、および terra を使用して、同じエリアの Sentinel-2 の画像をダウンロードしなさい。\n2020-08-01 から 2020-10-31 までの Sentinel-2 画像をダウンロードし、NDVI を計算し、ndvi.tifの結果と比較しなさい。","code":""},{"path":"algorithms.html","id":"algorithms","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11 スクリプト、アルゴリズム、関数","text":"","code":""},{"path":"algorithms.html","id":"prerequisites-11","chapter":"11 スクリプト、アルゴリズム、関数","heading":"必須パッケージ","text":"この章では、主に Base R を使用するため、必要なソフトウェアは最小限である。\nこれから開発するアルゴリズムの結果を確認するために sf パッケージだけを使用する。\nChapter 2 で紹介した地理クラスと、それを使ってさまざまな入力ファイル形式を表現する方法 (Chapter 8 参照) について理解していることを前提にしている。","code":""},{"path":"algorithms.html","id":"intro-algorithms","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11.1 イントロダクション","text":"Chapter 1 は、ジオコンピュテーションは既存のツールを使うだけでなく、「共有可能な R スクリプトや関数の形で」新しいツールを開発することが重要であることを示した。\n本章では、これらの再現性のあるコードの構成要素について学ぶ。\nまた、Chapter 10 で使用されているような低レベルの幾何学的アルゴリズムも紹介する。\nこれを読めば、このようなアルゴリズムの仕組みを理解し、複数のデータセットに対して、多くの人が、何度も使えるようなコードを書くことができるようになるはずである。\n本章だけでは、熟練したプログラマになることはできない。\nプログラミングは難しく、十分な練習が必要である (Abelson, Sussman, Sussman 1996) :プログラミングをそれ自体の知的活動として理解するためには、プログラミングに目を向けなければならないし、プログラムを読み、書かなければならない。しかし、プログラミングを学ぶ強い理由がある。\nこの章では、プログラミングそのものを教えるわけではない。プログラミングについては、Wickham (2019)、Gillespie Lovelace (2016)、Xiao (2016) を推奨する。これらの書籍は R や他の言語について教えてくれる。また、地理データに焦点を当て、プログラミング能力を伸ばすための基礎を作ることができる。本章は、再現性の重要性について、例を示しながら強調していきたい。\n再現性の利点は、他の人があなたの研究を複製することを可能にするだけではない。\n再現性のあるコードは、一度だけ実行されるように書かれたコードよりも、計算効率、スケーラビリティ (より大きなデータセットに対して実行するコードの能力)、適応やメンテナンスのしやすさなど、あらゆる面で優れていることが多いのである。スクリプトは、再現可能な R コードの基礎であり、このトピックは、Section 11.2 でカバーされている。\nアルゴリズムは、Section 11.3 で説明されているように、一連のステップを使用して入力を変更し、その結果、出力を得るためのレシピである。\n共有と再現を容易にするために、アルゴリズムを関数に配置することができる。\nそれが、Section 11.4 のトピックである。\nポリゴンの重心を求める例で、これらの概念を結びつけていこう。\nChapter 5 で、重心の関数 st_centroid() をすでに紹介したが、この例は、一見単純な操作が比較的複雑なコードの結果であることを強調し、次の観察を保証する (Wise 2001) 。空間データの問題で最も興味深いのは、人間にとっては些細なことに見えることが、コンピュータにとっては驚くほど難しいということである。この例は、Xiao (2016) にならい、「世の中にあるものを複製するのではなく、世の中のものがどのように機能しているかを示す」という本章の第二の目的も反映している。","code":""},{"path":"algorithms.html","id":"scripts","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11.2 スクリプト","text":"パッケージで配布される関数が R コードの構成要素だとすれば、スクリプトはそれらを論理的な順序でまとめる接着剤となる。\nスクリプトとは、再現可能なワークフローを作り出す目的で、手動または targets などの自動化ツールで保存・実行される (Landau 2021)。\nプログラミングの初心者にとってスクリプトは敷居が高く聞こえるかもしれないが、単なるプレーンテキストファイルである。\nスクリプトは、通常はその言語を表す拡張子で保存される。例えば、Python は .py、Rust は .rs である。\nR スクリプトは一般に、.R 拡張子で保存され、実行内容を反映した名前が付けられる。\n例として、この本のリポジトリの code フォルダに格納されているスクリプトファイル 11-hello.R がある。\n11-hello.R は、次の 2 行のコードが含まれているだけの簡単なスクリプトで、そのうち 1 行はコメントである。このスクリプトの中身は、とりたててスゴいものではないが、「スクリプトは複雑である必要はない」という点を示している。\n保存されたスクリプトは、source() を使って、その全体を呼び出したり、実行したりすることができる。\nこのコマンドの出力から、コメント行は無視され、print() コマンドが実行されることがわかる。以下のように bash や PowerShell などのシステムコマンドラインシェルから R スクリプトを呼び出すこともできる。RScript 実行可能ファイルが 設定されていれば、システムのシェルで Hello geocompr と表示される。\nスクリプトファイルに何を書くべきで何を書くべきではないかについて厳密なルールがあるわけではない。壊れた再現性のないコードになることもよくあるので、テストが必要である。\n有効な R を含まないコード行は、エラーを防ぐため、行頭に # を追加してコメントアウトする必要がある。11-hello.R スクリプトの 1 行目を参照。\n守るべき基本的ルールもある。順番に書く。映画の脚本と同じように、スクリプトも「設定」「データ処理」「結果保存」といった明確な順番が必要である (映画でいうところの「始まり」「中間」「終わり」にほぼ相当する)他の人 (と未来の自分) が理解できるように、スクリプトにコメントを追加してみよう。\nこれは、例えば、RStudio で、「折りたたみ可能な」コードセクションの見出しを作成するショートカット Ctrl+Shift+R を使って行うことができる特に、スクリプトは再現可能であるべきである。どんなコンピュータでも動作する自己完結型のスクリプトは、調子の良い日に自分のコンピュータでしか動作しないスクリプトよりも有用である。\nこれには、必要なパッケージを最初に添付し、データを永続的なソース (信頼できるウェブサイトなど) から読み込み、前のステップが実行されたことを確認することが含まれる。79パッケージ化しないかいぎり R スクリプトで再現性を強制するのは難しいが、それを助けるツールはある。\nRStudio は、デフォルトで R スクリプトを「コードチェック」し、不具合のあるコードに赤い波線を引く (下図参照)。\nrepex パッケージは、再現性のためのツールである。\nFIGURE 11.1: RStudio でのコード確認の様子。この例は 11-centroid-alg.R スクリプトの 19 行目のカッコが閉じられていないことを示している。\n\nこのセクションの内容は、あらゆるタイプの R スクリプトに適用できる。\nジオコンピュテーションのためのスクリプトで特に考慮すべき点は、GDAL など外部ライブラリへの依存が多い点である。実際、Chapter 8 のデータ入出力では GDAL をたくさん使用した。\nGIS ソフトウェアの依存は、Chaptger 10 で解説したようにより多くの特別なジオアルゴリズムを実行するために必要になる。\n地理データを扱うスクリプトは、入力データセットが特定のファイル形式であることを必要とする。\nこのような依存関係は、スクリプトのコメントとして、またはスクリプトの一部であるプロジェクトの他の場所でコメントするか、renv や Docker などで依存性として記述するべきである。「保守的」プログラミング技術と適切なエラーメッセージは、要件が満たされていない時に依存性を確認し、ユーザと対話する時間を節約する。\nR では () で表される 文を使用し、特定の条件が揃った時のみにメッセージを送ったりコードを実行するようにコードを書く。\n以下のコード例は、特定のファイルがない場合にユーザにメッセージを送る (訳註: メッセージに日本語を使わないことを推奨する。)。このスクリプトが行う作業は、以下の再現例で示されている。このスクリプトは、poly_mat という、長さ 9 単位の辺を持つ正方形 (この意味は次のセクションで明らかになる) という前提条件のオブジェクトに対して動作する。\nこの例では、インターネットに接続していることを前提に、source() が URL (ここでは短縮版を使用) で動作することを示している。\nそうでない場合は、github.com/geocompx/geocompr からダウンロードできる geocompr フォルダのルートディレクトリから R を実行していると仮定して、source(\"code/11-centroid-alg.R\") で同じスクリプトを呼び出すことができる。","code":"\n# Aim: provide a minimal R script\nprint(\"Hello geocompr\")\nsource(\"code/11-hello.R\")\n#> [1] \"Hello geocompr\"Rscript code/11-hello.R\nif (!file.exists(\"required_geo_data.gpkg\")) {\n  message(\"No file, required_geo_data.gpkg is missing!\")\n} \n#> No file, required_geo_data.gpkg is missing!\npoly_mat = cbind(\n  x = c(0, 9, 9, 0, 0),\n  y = c(0, 0, 9, 9, 0)\n)\n# geocompr リポジトリの code/11-centroid-alg.R の短い URL\nsource(\"https://t.ly/0nzj\")#> [1] \"The area is: 81\"\n#> [1] \"The coordinates of the centroid are: 4.5, 4.5\""},{"path":"algorithms.html","id":"geometric-algorithms","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11.3 幾何学的アルゴリズム","text":"アルゴリズムは、コンピュータにおける料理のレシピに相当するものと理解することができる。\n入力に対して実行されると、有用または美味しい出力が得られる指示の完全なセットである。\n入力とは、料理においては小麦粉や砂糖などの食材で、アルゴリズムの場合はデータと入力パラメータとなる。\n美味しいケーキはレシピの結果であるのと同様に、アルゴリズムの成功は環境や社会的に利点のある出力となりうる。\n具体的なケーススタディに入る前に、アルゴリズムとスクリプト (Section 11.2 ) や関数 (Section 11.4 で説明するように、アルゴリズムを一般化し、他でも使えるようにしたり簡単にする) の関係について簡単に説明する。「アルゴリズム」という言葉は、西暦825年のバグダッドで出版されたされた、初期の数学教科書に端を発している。\nこの本はラテン語に翻訳され、人気を博しただけでなく、著者の名字である al-Khwārizmī が「科学用語として不滅の名声を得て、Alchoarismi、Algorismi、そして最終的には algorithm になった」 (Bellos 2011) 。\nコンピュータ時代には、アルゴリズムは、問題を解決する一連のステップを指し、その結果、あらかじめ定義された出力が得られる。\n入力は、適切なデータ構造で正式に定義されなければならない (Wise 2001)。\nアルゴリズムは、多くの場合、コードで実装される前に、処理の目的を示すフローチャートや疑似コードとして開始される。\n使い勝手をよくするために、一般的なアルゴリズムは関数内にパッケージ化されていることが多く、(関数のソースコードを見ない限り) 一部または全部の手順が隠されている場合がある (Section 11.4 を参照)。Chapter 10 で見たような ジオアルゴリズムは、地理的なデータを取り込み、一般的には地理的な結果を返すアルゴリズムである (同じものを表す別の用語として、GIS アルゴリズム、幾何学的アルゴリズムがある)。\n簡単そうに聞こえるだろうが、このテーマは奥が深く、計算幾何学という学問分野全体がその研究に専念している (Berg et al. 2008)。このテーマに関する書籍も多数出版されている。\n例えば、O’Rourke (1998) は、再現可能で自由に利用できる C コードを用いて、徐々に難しくなる幾何学的アルゴリズムの範囲を紹介している。幾何学的アルゴリズムの例としては、ポリゴンの重心を求めるものがある。\n重心の計算には多くのアプローチがあり、中には特定のタイプの空間データに対してのみ機能するものもある。\n本節では、視覚化しやすいアプローチを選択する。ポリゴンを多くの三角形に分割し、それぞれの重心を求める。このアプローチは、他の重心アルゴリズムと並んで Kaiser Morin (1993) によって議論された (簡単な説明は O’Rourke 1998)。\nコードを書く前に、このアプローチをさらに個別のタスクに分解するのに役立つ (以降、ステップ 1 からステップ 4 と呼ぶが、これらは模式図や疑似コード として提示すこともできる)。ポリゴンを連続した三角形に分割する各三角形の重心を求めるそれぞれの三角形の面積を求める三角形の中心点の面積加重平均を求める一見、簡単そうに見えるが、言葉をコードに変換するには、入力に制約がある場合でも、試行錯誤を繰り返しながら作業を進める必要がある。\nこのアルゴリズムは、180°以上の内角を持たない凸ポリゴンに対してのみ動作し、星形は使用できない (パッケージの decido と sfdct は外部ライブラリを使用して非凸ポリゴンを三角測量できる。geocompx.org の algorithm vignetteに示されている。)。ポリゴンの最も単純なデータ構造は、x と y の座標の行列で、各行はポリゴンの境界を順にたどる頂点を表し、最初と最後の行は同一である (Wise 2001)。\n今回は、GIS Algorithms (Xiao 2016 Python コードは github.com/gisalgs を参照) の例を参考に、Figure 11.2 に示すように、5 つの頂点を持つポリゴンを Base R で作成する。これで、例となるデータセットができたので、上記のステップ 1 に着手する準備が整った。\n以下のコードでは、1 つの三角形 (T1) を作成して、この方法を示している。また、 数式 \\(1/3(+ b + c)\\) (\\(\\) から \\(c\\) は三角形の頂点を表す座標) に基づいて重心を計算するステップ 2 も示している。\nFIGURE 11.2: ポリゴン重心計算問題。\nステップ 3 では、各三角形の面積を求めるので、大きな三角形の不釣り合いな影響を考慮した加重平均を計算する。\n三角形の面積を計算する公式は次の通りである (Kaiser Morin 1993)。\\[\n\\frac{A_x ( B_y − C_y ) + B_x ( C_y − A_y ) + C_x ( A_y − B_y )}{ 2 }\n\\]ここで、\\(\\) から \\(C\\) は三角形 T1 の 3 点、\\(x\\) と \\(y\\) は x と y の次元を指す。\nこの式を、三角形の行列表現のデータを扱う R コードに翻訳すると、次のようになる (関数 abs() は、正の結果を保証する)。このコードチャンクは正しい結果を出力する。80\nこのコードは不格好で、別の三角行列で実行する場合、再入力しなければならない点が問題である。\nより一般化するために、このコードを Section 11.4 で関数に変換する方法を見てみよう。ステップ 4 では、ステップ 2 と 3 を 1 つの三角形だけでなく、すべての三角形に対して行う必要がある (上の例)。\nこのため、ポリゴンを表すすべての三角形を作成するためのイテレーション (繰り返し) が必要である。Figure 11.3 に示す。\nlapply() と vapply() が各三角形の反復処理に使われているのは、Base R で簡潔な解が得られるからである。81\nFIGURE 11.3: 複数の三角による繰り返し重心アルゴリズム。繰り返し 2 と 3 における X は、面積加重重心。\nこれで、ステップ 4 を完了し、総面積を sum() で計算し、ポリゴンの重心座標を weighted.mean(C [, 1] , ) と weighted.mean(C [, 2] , ) でポリゴンの重心座標を計算する (注意深い読者のための練習: これらのコマンドが動作することを確認してみよう)。\nアルゴリズムとスクリプトの関連性を示すために、このセクションの内容を凝縮して 11-centroid-alg.R とした。\nSection 11.2 の最後で、このスクリプトが正方形の重心を計算する方法を見た。\nアルゴリズムをスクリプト化することの素晴らしい点は、新しい poly_mat オブジェクト上で動作することである (st_centroid() を参照してこれらの結果を検証するには、以下の演習を参照)。上記の例では、低レベルの地理的な操作は、base R で第一原理から開発することができることが示されている。\nまた、すでに試行錯誤したソリューションが存在する場合、車輪の再発明をする価値はないことも示している。\nもし、ポリゴンの重心を求めるだけなら、poly_mat を sf オブジェクトとして表現し、代わりに既存の sf::st_centroid() 関数を使用する方が早かっただろう。\nしかし、第一原理でアルゴリズムを書くことの大きな利点は、プロセスのすべての段階を理解できることであり、他の人のコードを使うときには保証されないことである。\nさらに考慮すべきは性能である。R は C++ のような低レベルの言語と比較すると数値計算が遅いことが多く、最適化が困難である (Section 1.4 参照)。\n新しい手法の開発を目的とするのであれば、計算効率を優先させるべきではない。\nこれは、「早すぎる最適化はプログラミングにおける諸悪の根源 (あるいは少なくともそのほとんど)」という言葉に集約される (Knuth 1974)。アルゴリズム開発は大変な作業である。\nこのことは、Base R を使った重心アルゴリズムの開発に費やした作業量から明らかである。このアルゴリズムは、実世界での応用が限られている問題に対する一つの、むしろ非効率的なアプローチに過ぎない。というのも、実際には凸ポリゴンは珍しいからである。\nこの経験は、GEOS や CGAL (計算幾何学アルゴリズムライブラリ, Computational Geometry Algorithms Library) など、高速に動作し、かつ幅広い入力ジオメトリタイプに対応する低レベル地理ライブラリの理解につながるはずである。\nこのようなライブラリのオープンソース化の大きな利点は、そのソースコードが、研究、理解、(技術と自信があれば) 改変のために容易に利用できることである。82","code":"\n# ポリゴンを表現する行列を作成\nx_coords = c(10, 20, 12, 0, 0, 10)\ny_coords = c(0, 15, 20, 10, 0, 0)\npoly_mat = cbind(x_coords, y_coords)\n# 原点を作成:\nOrigin = poly_mat[1, ]\n# 三角形の行列を作成:\nT1 = rbind(Origin, poly_mat[2:3, ], Origin) \nC1 = (T1[1,] + T1[2,] + T1[3,]) / 3\n# 行列 T1 で表される三角形の面積を計算\nabs(T1[1, 1] * (T1[2, 2] - T1[3, 2]) +\n    T1[2, 1] * (T1[3, 2] - T1[1, 2]) +\n    T1[3, 1] * (T1[1, 2] - T1[2, 2])) / 2\n#> [1] 85\ni = 2:(nrow(poly_mat) - 2)\nT_all = lapply(i, function(x) {\n  rbind(Origin, poly_mat[x:(x + 1), ], Origin)\n})\n\nC_list = lapply(T_all,  function(x) (x[1, ] + x[2, ] + x[3, ]) / 3)\nC = do.call(rbind, C_list)\n\nA = vapply(T_all, function(x) {\n  abs(x[1, 1] * (x[2, 2] - x[3, 2]) +\n        x[2, 1] * (x[3, 2] - x[1, 2]) +\n        x[3, 1] * (x[1, 2] - x[2, 2]) ) / 2\n  }, FUN.VALUE = double(1))\nsource(\"code/11-centroid-alg.R\")\n#> [1] \"The area is: 245\"\n#> [1] \"The coordinates of the centroid are: 8.83, 9.22\""},{"path":"algorithms.html","id":"functions","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11.4 関数","text":"アルゴリズムと同様に 、関数は入力を受け取り、出力を返す。\nしかし、関数は、「レシピ」そのものではなく、特定のプログラミング言語での実装を指している。\nR では、関数はそれ自体がオブジェクトであり、モジュール方式で作成したり結合したりすることができる。\n例えば、重心生成アルゴリズムのステップ 2 を引き受ける関数を以下のように作成することができる。上記の例では、関数の 2 つの重要な構成要素を示している。(1) 関数の中身 (body) は、中括弧内のコードで、関数が入力に対して何をするかを定義する。 (2) 引数 (argument) は 、関数が扱う引数のリスト。この場合は x である (3 番目の重要なコンポーネント、環境はこのセクションの範囲外)。\nデフォルトでは、関数は最後に計算したオブジェクトを返す (t_centroid() の場合は重心の座標)。83この関数は、前のセクションのポリゴンの例から最初の三角形の面積を計算する以下のコマンドのように、渡した入力に対して動作する (Figure 11.3 を参照)。また、三角形の面積を計算する関数を作成することができる。ここでは、t_area() と名付ける。なお、この関数を作成した後は、1 行のコードで三角形の面積を計算できるようになり、冗長なコードの重複を避けることができる。\n関数は、コードを一般化するためのメカニズムである。\n新たに作成した関数 t_area() は、これまで使ってきた「三角行列」データ構造と同じ寸法を持つと仮定した任意のオブジェクト x を受け取り、その面積を返すもので、T1 で図示すると次のようになる。関数を使って、高さ 1、底辺 3 の新しい三角行列の面積を求めることで、その一般化可能性を検証することができる。関数の便利な点は、モジュール化されていることである。\n出力が何であるかが分かっていれば、ある関数を別の関数の構成要素として利用することができる。\nしたがって、関数 t_centroid() と t_area() は、スクリプト 11-centroid-alg.R というより大きな関数のサブコンポーネントとして使うことができる。このスクリプトは、任意の凸ポリゴンの面積を計算する。\n以下のコードでは、凸ポリゴンに対する sf::st_centroid() の動作を模倣する関数 poly_centroid() を作成している。84poly_centroid() などの関数はさらに拡張して、さまざまなタイプの出力を提供することができる。\n例えば、結果をクラス sfg のオブジェクトとして返すには、結果を返す前に、「ラッパー」関数を用いて poly_centroid() の出力を変更することができる。以下のように、sf::st_centroid() からの出力と同じであることが確認できる。","code":"\nt_centroid = function(x) {\n  (x[1, ] + x[2, ] + x[3, ]) / 3\n}\nt_centroid(T1)\n#> x_coords y_coords \n#>     14.0     11.7\nt_area = function(x) {\n  abs(\n    x[1, 1] * (x[2, 2] - x[3, 2]) +\n    x[2, 1] * (x[3, 2] - x[1, 2]) +\n    x[3, 1] * (x[1, 2] - x[2, 2])\n  ) / 2\n}\nt_area(T1)\n#> [1] 85\nt_new = cbind(x = c(0, 3, 3, 0),\n              y = c(0, 0, 1, 0))\nt_area(t_new)\n#>   x \n#> 1.5\npoly_centroid = function(poly_mat) {\n  Origin = poly_mat[1, ] # create a point representing the origin\n  i = 2:(nrow(poly_mat) - 2)\n  T_all = lapply(i, function(x) {rbind(Origin, poly_mat[x:(x + 1), ], Origin)})\n  C_list = lapply(T_all, t_centroid)\n  C = do.call(rbind, C_list)\n  A = vapply(T_all, t_area, FUN.VALUE = double(1))\n  c(weighted.mean(C[, 1], A), weighted.mean(C[, 2], A))\n}\npoly_centroid(poly_mat)\n#> [1] 8.83 9.22\npoly_centroid_sfg = function(x) {\n  centroid_coords = poly_centroid(x)\n  sf::st_point(centroid_coords)\n}\npoly_sfc = sf::st_polygon(list(poly_mat))\nidentical(poly_centroid_sfg(poly_mat), sf::st_centroid(poly_sfc))\n#> [1] TRUE"},{"path":"algorithms.html","id":"programming","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11.5 プログラミング","text":"この章では、スクリプトからアルゴリズムという厄介なトピックを経由して関数へと移った。\n抽象的な議論だけでなく、具体的な問題を解決するために、それぞれの実用例を作成した。スクリプト 11-centroid-alg.R を導入し、「ポリゴンマトリックス」で実行した。このスクリプトを動作させるための個々のステップは、アルゴリズム、つまり計算レシピとして記述した。アルゴリズムを一般化するために、このアルゴリズムをモジュール関数に変換し、最終的にそれらを組み合わせて、前節の関数 poly_centroid() を作成した。これらのステップは、簡単なことである。\nしかし、プログラミングの技術とは、スクリプト、アルゴリズム、関数を組み合わせて、性能の良いシステムにすることなのである。\nできたものは、堅牢で、皆が簡単に使えるものであるべきである。\nこの本を読んでいるほとんどの人がそうであるように、プログラミングの初心者であれば、前のセクションの結果を追って再現できることは、大きな達成感を得ることができるはずである。\nプログラミングができるようになるまでには、何時間もかけて熱心に勉強し、練習する必要がある。新しいアルゴリズムを効率的に実装する際の課題は、実運用で使用することを意図していない単純な関数を作成するために費やした作業量を考慮すると、はるかに遠い。現在の状態では、poly_centroid() はほとんどの (非凸) ポリゴンで失敗する!\nここから生じる疑問は、関数をどのように一般化するかということである。\n(1) 非凸ポリゴンを三角測量する方法を探す (geocompx.github.io/geocompkg/articles/ のオンライン記事 Algorithms Extended で扱っている話題) と (2) 三角メッシュに依存しない他の重心のアルゴリズムを調べるという 2 つの選択肢がある。もっと大きな疑問は、高性能なアルゴリズムがすでに実装され、st_centroid() のような関数にパッケージされているのに、ソリューションをプログラミングする価値があるのだろうか、ということである。\nこの具体的なケースにおける還元論的な答えは「価値はない」である。\n広い意味で、プログラミングを学ぶことの利点を考えると、答えは「場合による」である。\nプログラミングでは、あるメソッドを実装しようとすると、すでに誰かがその苦労をしていることに気づき、何時間も無駄にすることがよくある。\n本章は、ジオアルゴリズムのプログラミングへの第一歩として捉えることができる。\nしかし、一般化された解決策をプログラムする場合と、既存の高水準の解決策を利用する場合の教訓とも言える。\n新しい関数を作るのが最善の場合もあれば、すでにある関数を使うのが良い場合もある。「車輪の再発明をするな」という言葉は、他の人生の歩みと同様、いやそれ以上にプログラミングに当てはまる。\nプロジェクトの最初に少し調査して考えることで、プログラミングの時間をどこに費やすのがベストなのかを決めることができる。\nまた、以下の 3 つの原則は、簡単なスクリプトであれ、何百もの関数で構成されるパッケージであれ、コードを書くときに労力を最大限に活用するのに役立つ。DRY (don’t repeat ): コードの繰り返しを最小限に抑え、より少ないコード行数で特定の問題を解決することを目指す。\nこの原則は、「R Data Science」の「Functions」の章において、コードの繰り返しを減らすための関数の使用を参照して説明されている (Grolemund Wickham 2016)。KISS (keep simple stupid): この原則は、複雑な解決策よりも単純な解決策を最初に試し、必要に応じて依存関係を使用し、スクリプトを簡潔に保つことを目指すことを示唆。\nこの原則は、「ものごとはできるかぎりシンプルにすべきだ。しかし、シンプルすぎてもいけない。 」“things made simple possible, simpler” という名言 のコンピュータ版である。Modularity: コードを明確に分割することで、メンテナンスが容易になる。\n関数は、たった一つのことをするだけにして、それに専念するべきである。\nもし関数が長くなりすぎたら、それを複数の小さな関数に分割し、それぞれを別の目的に再利用することを考えよう。この章だけで、すぐに完璧な関数を作成できるようになることは保証していない。\nしかし、この章の内容は、いつ挑戦するのが適切かを判断するのに役立つと確信している (問題を解決する既存の関数がない場合、プログラミングタスクが自分の能力の範囲内にある場合、そのソリューションの利点が開発にかかる時間コストを上回ると思われる場合)。\n上記の原則と、上記の例を通しての実践的な経験を組み合わせることで、スクリプト、パッケージ作成、プログラミングのスキルを手に入れることができる。\nプログラミングへの最初の一歩は時間がかかるが (以下の演習は急がないように)、長期的な見返りは大きくなるだろう。","code":""},{"path":"algorithms.html","id":"ex-algorithms","chapter":"11 スクリプト、アルゴリズム、関数","heading":"11.6 演習","text":"E1. 本書の GitHub リポジトリの 11-centroid-alg.R スクリプトを読みなさい。ベストプラクティスのうちどれを使っているか？RStudio などの IDE を使い、自分のパソコンでスクリプトを作成しなさい (スクリプトを 1 行ずつ打ち込み、適宜コメントを入れると良い。コピペはしない。この作業でスクリプトの入力の仕方を学ぶことができる。)。正方形ポリゴン (poly_mat = cbind(x = c(0, 9, 9, 0, 0), y = c(0, 0, 9, 9, 0)) で作成) の例を使い、スクリプトを 1 行ずつ実行しなさい。再現可能性を高めるためにはどのように変更したら良いか?ドキュメンテーションをより良くするためにはどうしたら良いか?E2. 幾何アルゴリズムのセクションで、poly_mat のポリゴンの面積は 245 で、重心は座標 (8.8, 9.2) であると計算した。このアルゴリズムのスクリプトである 11-centroid-alg.R を参照し、自分のパソコンで結果を再現しなさい (ボーナス: コピペせずに自分で入力しなさい)。結果は正しいか? poly_mat を st_polygon() 関数で sfc オブジェクトに変換し (poly_sfc という名前)、st_area() 関数と st_centroid() 関数を用いて検証しなさい (ヒント: この関数は、クラス list() を引数に取る)。E3. 我々が作成したアルゴリズムは凸包に対してのみ動作すると記載されている。凸包を定義し (ジオメトリ操作の章を参照)、凸包でないポリゴンでアルゴリズムをテストしなさい。ボーナス 1: なぜこの方法が凸の外皮に対してのみ機能するのかを考え、他の種類の多角形に対して機能させるためにアルゴリズムに加える必要がある変更点に注意する。ボーナス 2: 11-centroid-alg.R の内容を基に、行列形式で表現された線分の全長を求めることができる、Base R 関数のみを使ったアルゴリズムを書きなさい。E4. 関数のセクションでは、sfg クラスの出力 (poly_centroid_sfg()) と matrix 型の出力 (poly_centroid_type_stable()) を生成する poly_centroid() 関数の異なるバージョンを作成した。\nさらに、型が安定で (sf クラスの入力しか受け付けない) sf オブジェクトを返すバージョン (例えば poly_centroid_sf()) を作成し、関数を拡張しなさい (ヒント: sf::st_coordinates(x) コマンドでオブジェクト x を行列に変換する必要があるかもしれない)。poly_centroid_sf(sf::st_sf(sf::st_sfc(poly_sfc))) を実行し、動作するか検証しなさいpoly_centroid_sf(poly_mat) を実行しようとした時、どのようなエラーメッセージが表示されたか?","code":""},{"path":"spatial-cv.html","id":"spatial-cv","chapter":"12 統計的学習","heading":"12 統計的学習","text":"","code":""},{"path":"spatial-cv.html","id":"prerequisites-12","chapter":"12 統計的学習","heading":"必須パッケージ","text":"本章では、Chapter 2 から Chapter 7 までの内容を学習し、演習を行うなどして、地理データ解析 に習熟していることを前提としている。\n一般化線形モデル (Generalized Linear Model, GLM) と機械学習に精通していることを強く推奨する James et al. (2013)。この章では、以下のパッケージを使用する。85データは必要に応じて読み込む。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(future)             # 並列処理\nlibrary(lgr)                # logging framework for R\nlibrary(mlr3)               # 機械学習アルゴリズムへの統一インターフェース\nlibrary(mlr3learners)       # 最重要の機械学習アルゴリズム\nlibrary(mlr3extralearners)  # その他の機械学習アルゴリズム\nlibrary(mlr3spatiotempcv)   # 時空間リサンプリング\nlibrary(mlr3proba)          # mlr3extralearners::list_learners() が使う\nlibrary(mlr3tuning)         # ハイパーパラメータのチューニング\nlibrary(mlr3viz)            # mlr3 オブジェクトのプロット関数\nlibrary(progressr)          # 進捗状況を報告\nlibrary(pROC)               # ROC 値を計算"},{"path":"spatial-cv.html","id":"intro-cv1","chapter":"12 統計的学習","heading":"12.1 イントロダクション","text":"統計的学習は、データのパターンを特定し、そのパターンから予測するための統計的・計算的モデルの使用に関するものである。\nその起源から、統計的学習は R の の大きな強みの一つである ( Section 1.4 参照)。86\n統計的学習とは、統計学と機械学習の手法を組み合わせたもので、教師あり手法と教師なし手法に分類される。\nどちらも物理学、生物学、生態学から地理学、経済学に至るまで、ますます多くの分野で利用されるようになっている (James et al. 2013)。この章では、クラスタリングのような教師なし技術ではなく、学習データセットが存在する教師あり技術に焦点を当てていきたい。\n応答変数は、二値 (地すべりの発生など)、カテゴリ (土地利用)、整数 (種の豊富さ)、数値 (土壌酸性度の pH 測定値) のいずれでもよい。\n教師あり技術は、観測のサンプルについて既知の応答と、1 つまたは複数の予測変数の間の関係をモデル化する。多くの機械学習において、研究の主な目的は優れた予測を行うことである。\n機械学習が「ビッグデータ」の時代に繁栄しているのは、その手法が入力変数に関する仮定をほとんど必要とせず、巨大なデータセットを扱えるからである。\n機械学習は、将来の顧客行動予測、推奨サービス (音楽、映画、次に買うもの)、顔認識、自律走行、テキスト分類、予知保全 (インフラ、産業) などのタスクに資するものである。この章では、地すべりの発生モデルという事例をもとに説明する。\nこの応用例は、Chapter 1 で定義されているジオコンピュテーションの応用的な性質とリンクしており、機械学習が、予測を唯一の目的とする場合に統計学の分野から借用する方法を示している。\nそこで、この章では、まず、GLM (. Zuur et al. 2009) の助けを借りて、モデリングと交差検証 (cross validation, CV) の概念を紹介する。\nこれを踏まえて、この章では、より典型的な機械学習 アルゴリズム 、すなわちサポートベクタマシン (Support Vector Machine, SVM) を実装している。\nモデルの予測性能は、地理データが特殊であることを考慮した空間交差検証 (空間 CV) を用いて評価していこう。CV データセットをトレーニングセットとテストセットに (繰り返し) 分割することで、モデルが新しいデータに対して汎化する能力を決定する。\n学習データを使ってモデルを適合させ、テストデータに対して予測したときの性能をチェックする。\nCV は過適合を検出するのに役立つ。なぜなら、学習データをあまりに忠実に予測するモデル (ノイズ) は、テストデータでのパフォーマンスが低くなる傾向があるからである。空間データをランダムに分割することで、テスト点と空間的に隣接する学習点を得ることができる。\n空間的に自己相関していると、このシナリオではテストとトレーニングのデータセットが独立しておらず、結果として CV は過適合の可能性を検出できなくなる。\n空間交差検証 は、本章の中心テーマであり、この問題を軽減する。。繰り返しになるが、この章ではモデルの予測性能に焦点を当てる。\n予測地図は扱わない。\nこれは、Chapter 15 で扱う。","code":""},{"path":"spatial-cv.html","id":"case-landslide","chapter":"12 統計的学習","heading":"12.2 ケーススタディ: 地すべりの発生しやすさ","text":"このケーススタディは、Ecuador 南部の地すべり地点のデータセットに基づいている。図は Figure 12.1、詳細は Muenchow, Brenning, Richter (2012) で説明されている。\n論文で使用されたデータセットの部分集合は spDataLarge パッケージで提供されており、以下のように読み込むことができる。上記のコードでは、lsl、sf という名前の data.frame、study_mask という名前の sf オブジェクト、そして ta (Section 2.3.4 を参照) という名前の地形属性ラスタ SpatRaster という 3 つのオブジェクトをロードしている。\nlsl は要因列 lslpts を含み、TRUEは観測された地すべり「開始点」に対応し、座標は列 x と y に格納されている。 87\nsummary(lsl$lslpts) に示すように、地すべり地点が 175 箇所、非地すべり地点が 175 箇所ある。\n非地すべり点 175 点は、地すべりポリゴン周辺の小さな緩衝地帯の外に位置しなければならないという制約のもと、調査地域からランダムにサンプル化されたものである。\nFIGURE 12.1: Ecuador 南部における地すべり発生地点 (赤) と地すべりの影響を受けていない地点 (青)。\nTable 12.1 に、lsl の最初の 3 行を有効数字 2 桁に丸めたものを掲載している。\nTABLE 12.1: TABLE 12.2: lsl データセットの構成。\n地すべりの発生しやすさをモデル化するためには、いくつかの予測因子が必要である。\n地形属性は地すべりと関連することが多いので (Muenchow, Brenning, Richter 2012)、すでに ta から lsl まで、以下の地形属性を抽出している。slope : 傾斜角 (°)cplan : 斜面の収束・発散を表す平面曲率 (rad m-1) で、水の流れを表現する。cprof : 流れの加速度の指標としてのプロファイル曲率 (rad m-1)、傾斜角のダウンスロープ変化としても知られている。elev : 調査地域の植生と降水量の異なる標高帯を表す標高 (m .s.l.)log10_carea : ある地点に向かって流れる水の量を表す集水面積の十進対数 (log10 m2) のこと。R-GIS ブリッジ (Chapter 10 参照) を用いて地形属性を計算し、地すべり地点に抽出することは、有意義な演習となるだろう (本章末の演習の項参照)。","code":"\ndata(\"lsl\", \"study_mask\", package = \"spDataLarge\")\nta = terra::rast(system.file(\"raster/ta.tif\", package = \"spDataLarge\"))"},{"path":"spatial-cv.html","id":"conventional-model","chapter":"12 統計的学習","heading":"12.3 R による従来のモデリング手法","text":"何十もの学習アルゴリズムへの統一的なインタフェースを提供するアンブレラパッケージである mlr3 パッケージを紹介する (Section 12.5) が、その前に R の従来のモデリングインタフェースについて見ておく価値がある。\nこの教師あり統計学習の入門は、空間交差検証 を行うための基礎となり、この後に紹介する mlr3 のアプローチの把握に貢献する。教師あり学習では、予測変数の関数として応答変数を予測する (Section 12.4)。\nR では、モデリング関数は通常、数式を使って指定する (R の数式の詳細については、?formula を参照)。\n次のコマンドは、一般化線形モデルを指定し、実行する。3 つの入力引数のそれぞれを理解しておくとよいだろう。地すべりの発生状況 (lslpts) を予測変数の関数として指定した式モデルの種類を指定する family で、この場合は応答が二値なので binomial としている (?family を参照)応答と予測変数 (列として) を含むデータフレームこのモデルの結果を表示すると次のようになる (summary(fit) にはより詳細な説明がある)。クラス glm のモデルオブジェクト fit は、応答と予測変数の間の適合関係を定義する係数を含む。\nまた、予測にも利用することができる。\nこれは一般的な predict() メソッドで行われ、この場合、関数 predict.glm() を呼び出す。\ntype を response に設定すると、下図のように lsl の各観測値に対する (地すべり発生の) 予測確率が返される (?predict.glm を参照)。予測ラスタに係数を適用することで、空間分布図を作成することができる。\nこれは、手動または terra::predict() で行うことができる。\nモデルオブジェクト (fit) に加えて、後者の関数は、モデルの入力データフレーム (Figure 12.2) と同じ名前の予測子 (ラスタレイヤ) を持つ SpatRaster も必要とする。\nFIGURE 12.2: GLM を用いた地すべり感受性の空間分布図.\nここで、予測を行う際には、空間自己相関構造があってもなくても平均的に予測精度は変わらないと仮定しているため、空間自己相関を無視する。\n空間自己相関 を、モデルと予測に組み入れることも可能である。\nこれは本書の範疇を越えるが、いくつかの資料を紹介する。回帰クリギングの予測値は、回帰の予測値と回帰の残差のクリギングを組み合わせたものである (Goovaerts 1997; Hengl 2007; Bivand, Pebesma, Gómez-Rubio 2013)。また、一般化最小二乗モデルに空間相関 (依存関係) 構造を追加することもできる (nlme::gls(), . Zuur et al. 2009; . F. Zuur et al. 2017)。また、混合効果モデリング・アプローチを使用することもできる。\n基本的に、ランダム効果は、応答変数に従属構造を課し、それによって、あるクラスの観測が、他のクラスの観測よりも互いに類似していることを可能にする (. Zuur et al. 2009)。\nクラスは、例えば、ハチの巣、フクロウの巣、植生トランセクト、標高の層別などである。\nこの混合モデリングのアプローチは、正規かつ独立に分布するランダム切片を仮定している。\nこれは、正規分布で空間的に依存するランダム切片を使用することによっても拡張することができる。\nしかし、このためには、ベイズ・モデリング・アプローチに頼らなければならないだろう (Blangiardo Cameletti 2015; . F. Zuur et al. 2017)。空間分布図は、モデルの非常に重要なアウトカムの一つである。\nさらに重要なのは、モデルの予測性能が低ければ、予測マップは役に立たないので、基盤となるモデルがどれだけ優れているかということである。\n二項モデルの予測性能を評価する最も一般的な尺度の一つは、Area Receiver Operator Characteristic Curve (AUROC) である。\nこれは 0.5 から 1.0 の間の値で、0.5 はランダム化より良くないモデル、1.0 は 2 つのクラスを完全に予測することを示す。\nしたがって、AUROC が高いほど、モデルの予測力が優れていることになる。\n次のコードチャンクは、応答と予測値を入力とする roc() を用いて、モデルの AUROC 値を計算するものである。\nauc() は、曲線の下の面積を返す。AUROC の値 0.82 は良好な適合性を示している。\nしかし、これは完全なデータセットに対して計算したものであるため、楽観的すぎる推定値である。\n偏りを抑えた評価を導き出すためには、交差検証を用いる必要があり、空間データの場合は空間交差検証 を利用する必要がある。","code":"\nfit = glm(lslpts ~ slope + cplan + cprof + elev + log10_carea,\n          family = binomial(),\n          data = lsl)\nclass(fit)\n#> [1] \"glm\" \"lm\"\nfit\n#> \n#> Call:  glm(formula = lslpts ~ slope + cplan + cprof + elev + log10_carea, \n#>     family = binomial(), data = lsl)\n#> \n#> Coefficients:\n#> (Intercept)        slope        cplan        cprof         elev  log10_carea  \n#>    2.51e+00     7.90e-02    -2.89e+01    -1.76e+01     1.79e-04    -2.27e+00  \n#> \n#> Degrees of Freedom: 349 Total (i.e. Null);  344 Residual\n#> Null Deviance:       485 \n#> Residual Deviance: 373   AIC: 385\npred_glm = predict(object = fit, type = \"response\")\nhead(pred_glm)\n#>      1      2      3      4      5      6 \n#> 0.1901 0.1172 0.0952 0.2503 0.3382 0.1575\n# 予測する\npred = terra::predict(ta, model = fit, type = \"response\")\npROC::auc(pROC::roc(lsl$lslpts, fitted(fit)))\n#> Area under the curve: 0.8216"},{"path":"spatial-cv.html","id":"intro-cv","chapter":"12 統計的学習","heading":"12.4 (空間) 交差検証の紹介","text":"交差検証 は、リサンプリング法のファミリーに属する (James et al. 2013)。\n基本的な考え方としては、データセットをトレーニングセットとテストセットに (繰り返し) 分割し、トレーニングデータを使ってモデルを適合させ、それをテストセットに適用する。\n予測値とテストセットの既知の応答値を比較することにより (二項式の場合は AUROC のような性能指標を使用)、学習した関係を独立したデータに一般化するモデルの能力について、バイアスを低減した評価を得ることができる。\n例えば、5 倍交差検証を 100 回繰り返すとは、データをランダムに 5 分割 (フォールド) し、各フォールドをテストセットとして 1 回使用することを意味する (Figure 12.3 の上段を参照)。\nこれは、各観測が 1 つのテストセットで 1 回使用されることを保証し、5 つのモデルの適合を必要とする。\nその後、この手順を 100 回繰り返す。\nもちろん、データの分割は繰り返しごとに異なる。\n全体として、これは 500 のモデルに合計される。一方、すべてのモデルの平均性能指標 (AUROC) は、モデルの全体的な予測力である。しかし、地理的なデータは特殊である。\nChapter 13 で見るように、地理学の「第一法則」は、互いに近い地点は、一般に、遠い地点よりも似ているとするものである (Miller 2004)。\nつまり、従来の CV では学習点とテスト点が近すぎることが多いため、点が統計的に独立していないことになる (Figure 12.3 の最初の行を参照)。\n「テスト」観測の近くにある「トレーニング」観測は、一種の「カンニング」を提供することができる。\nすなわち、学習データセットでは利用できないはずの情報である。\nこの問題を軽減するために、観測を空間的に不連続なサブセットに分割する「空間分割」が使用される (k-means クラスタリングで観測の座標を使用; Brenning (2012b); Figure 12.3 の 2 行目)。\nこの分割戦略が、従来の CV との唯一の違いである。\nその結果、空間 CV はモデルの予測性能のバイアスを低減させ、過適合を回避するのに役立つ。\nFIGURE 12.3: 1 回の繰り返しの交差検証で選択されたテストおよびトレーニングの観測の空間的な可視化。ランダム (上段) および空間分割 (下段)。\n","code":""},{"path":"spatial-cv.html","id":"spatial-cv-with-mlr3","chapter":"12 統計的学習","heading":"12.5 mlr3 を用いた空間交差検証","text":"\n統計的学習のためのパッケージは何十種類もある。例えば CRAN machine learning task view で説明されている。\n交差検証やハイパーパラメータのチューニング方法など、各パッケージに精通することは時間のかかる作業である。\n異なるパッケージのモデル結果を比較するのは、さらに手間がかかる。\nこれらの問題を解決するために開発されたのが、mlr3 パッケージとエコシステムである。\nこれは「メタパッケージ」として機能し、分類、回帰 、生存時間分析、クラスタリングなど、一般的な教師あり・教師なしの統計学習技術への統一的なインタフェースを提供する (Lang et al. 2019; Bischl et al. 2024)。\n標準化された mlr3 インターフェースは、8 つの「ビルディングブロック」に基づいている。\nFigure 12.4 に示すように、これらは明確な順序を持っている。\nFIGURE 12.4: mlr3 パッケージの基本的な構成要素 (Bischl et al. 2024)。この図の再利用を快く承諾していただいた。\nmlr3 のモデリングプロセスは、主に3つのステージで構成されている。\nまず、task で、データ (応答変数と予測変数を含む) とモデルの種類 (回帰や分類など) を指定する。\n次に、learnerは、作成されたタスクに適用される特定の学習アルゴリズムを定義する。\n第三に、リサンプリングアプローチでは、モデルの予測性能、すなわち新しいデータへの汎化能力を評価する (Section 12.4 も参照)。","code":""},{"path":"spatial-cv.html","id":"glm","chapter":"12 統計的学習","heading":"12.5.1 一般化線形モデル","text":"GLM を mlr3 で使うためには、地すべりデータを含む task を作成する必要がある。\n応答は二値 (2 カテゴリの変数) で、空間次元を持つので、mlr3spatiotempcv パッケージの as_task_classif_st() を使用し、分類タスクを作成する (Schratz et al. 2021 、非空間 task には mlr3::as_task_classif()、回帰には as_task_regr() を使用。他の task の詳細は、?Task を参照。) 。88\nas_task_ 関数の最初の必須引数は、x である。\nx は、入力データが応答変数と予測変数を含んでいることを想定している。\ntarget の引数は応答変数の名前を示し (ここでは lslpts)、positive は応答変数の 2 つの因子レベルのうちどちらが地すべり開始点を示すかを決定する (ここでは TRUE)。\nlsl データセットの他のすべての変数が予測因子として機能する。\n空間 CV のためには、いくつかの追加引数を与える必要がある。\ncoordinate_names 引数は、座標列の名前を期待する (Section 12.4 と Figure 12.3 を参照)。\nさらに、使用する CRS (crs) を示し、その座標を予測因子としてモデリング (coords_as_features) に使用するかどうかを決定する必要がある。なお、mlr3spatiotempcv::as_task_classif_st() は、backend パラメータの入力として sf-オブジェクトも受け付ける。\nこの場合、引数 coords_as_features のみを追加して指定するとよいだろう。\nlsl を sf-オブジェクトに変換しなかったのは、as_task_classif_st() がバックグラウンドで非空間的な data.table オブジェクトに戻してしまうだけだからである。短時間のデータ探索では、mlr3viz パッケージの autoplot() 関数は、すべての予測因子に対する応答とすべての予測因子に対する応答をプロットするので便利だろう (図示していない)。task を作成したら、使用する統計的学習方式を決定する 学習器 (learner) を選択する必要がある。\n分類の学習器 は classif. で始まり、回帰の学習器は regr. で始まる (詳しくは ?Learner を参照)。\nmlr3extralearners::list_mlr3learners() は、利用可能なすべての学習器と、どのパッケージから mlr3 がそれらをインポートしているかをリストアップする (Table 12.3)。\n二値応答変数をモデル化できる学習器について調べるには、次のように実行する。TABLE 12.3: パッケージ mlr3 の二項タスク向け にある学習器のサンプル。これにより、すべての学習器が 2 クラス問題 (地すべりの有無) をモデル化することができるようになった。\nSection 12.3 で使用され、mlr3learners では classif.log_reg として実装されている二項分類方式を選択することにする。\nさらに、予測の種類を決める predict.type を指定する必要がある。prob は、地すべり発生の予測確率を 0 から 1 の間で決定する (これは type = response の predict.glm() に対応する)。学習器のヘルプページにアクセスし、どのパッケージから取得したものかを調べるには、次のように実行する。mlr3 でモデリングするためのセットアップ手順は、面倒に思えるだろう。\nしかし、この一つのインターフェースで、mlr3extralearners::list_mlr3learners() が示す 130 種類以上の学習器にアクセスできることを思い出してほしい。各学習器のインターフェースを学ぶことはもっと退屈である。\nさらに、リサンプリング技術の簡単な並列化と、機械学習のハイパーパラメータを調整できることも利点である (Section 12.5.2 を参照)。\n最も重要なことは、mlr3spatiotempcv (Schratz et al. 2021) の (空間) リサンプリングは簡単で、リサンプリング法の指定と実行という 2 つのステップを追加するだけでよいということである。\n100 回繰り返される 5 回空間交差検証 : task で提供された座標に基づいて 5 つのパーティションが選ばれ、パーティショニングは 100 回繰り返される。 89空間リサンプリングを実行するために、先に指定したタスク、学習器、リサンプリング戦略を用いて、resample() を実行する。\n500 個のリサンプリングパーティションと 500 個のモデルを計算するため、多少時間がかかる (最新のノートパソコンで 15 秒程度)。\n性能指標として、今回も AUROC を選択した。\nこれを取得するために、リサンプリング結果出力オブジェクト (score_spcv_glm) の score() メソッドを使用する。\nこれは、500 行の data.table オブジェクトを返す。前述のコードチャンクの出力は、モデルの予測性能のバイアスを低減した評価である。\n書籍の GitHub リポジトリに extdata/12-bmr_score.rds として保存している。\n必要であれば、以下のように読み込むことができる。全 500 モデルの平均 AUROC を計算するために、以下を実行した。これらの結果を整理するために、100 回繰り返した 5 回非空間交差検証の AUROC 値と比較してみよう (Figure 12.5 ; 非空間CVのコードはここでは示さないが、演習セクションで検討する)。\n予想通り (Section 12.4 参照)、空間交差検証の結果は、従来の交差検証アプローチよりも平均して低い AUROC 値をもたらし、空間自己相関のため、後者の空間自己相関による楽観的な予測性能が強調された。\nFIGURE 12.5: 空間CV と従来の 100 回繰り返し 5 回 CV におけるGLM AUROC 値の差を示す箱ひげ図。\n","code":"\n# 1. task を作成\ntask = mlr3spatiotempcv::as_task_classif_st(\n  mlr3::as_data_backend(lsl), \n  target = \"lslpts\", \n  id = \"ecuador_lsl\",\n  positive = \"TRUE\",\n  coordinate_names = c(\"x\", \"y\"),\n  crs = \"EPSG:32717\",\n  coords_as_features = FALSE\n  )\n# 予測因子それぞれに対して応答をプロット\nmlr3viz::autoplot(task, type = \"duo\")\n# 変数それぞれに対して相互にプロット\nmlr3viz::autoplot(task, type = \"pairs\")\nmlr3extralearners::list_mlr3learners(\n  filter = list(class = \"classif\", properties = \"twoclass\"), \n  select = c(\"id\", \"mlr3_package\", \"required_packages\")) |>\n  head()\n# 2. 学習器を指定\nlearner = mlr3::lrn(\"classif.log_reg\", predict_type = \"prob\")\nlearner$help()\n# 3. リサンプリングを指定\nresampling = mlr3::rsmp(\"repeated_spcv_coords\", folds = 5, repeats = 100)\n# メッセージを減らす\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n# 空間交差検証 を実行し、リサンプル結果 glm (rr_glm) に保存\nrr_spcv_glm = mlr3::resample(task = task,\n                             learner = learner,\n                             resampling = resampling)\n# AUROC を計算しデータフレームに格納\nscore_spcv_glm = rr_spcv_glm$score(measure = mlr3::msr(\"classif.auc\"))\n# 必要な列だけ残す\nscore_spcv_glm = dplyr::select(score_spcv_glm, task_id, learner_id, \n                               resampling_id, classif.auc)\nscore = readRDS(\"extdata/12-bmr_score.rds\")\nscore_spcv_glm = dplyr::filter(score, learner_id == \"classif.log_reg\", \n                               resampling_id == \"repeated_spcv_coords\")\nmean(score_spcv_glm$classif.auc) |>\n  round(2)\n#> [1] 0.77"},{"path":"spatial-cv.html","id":"svm","chapter":"12 統計的学習","heading":"12.5.2 機械学習のハイパーパラメータの空間的チューニング","text":"Section 12.4 では、統計的学習の一環として、機械学習を導入した。\nもう一度確認しよう。Jason Brownlee による機械学習の以下の定義に従う。機械学習、より具体的には予測モデリングの分野では、説明可能性を犠牲にして、モデルの誤差を最小化すること、あるいは可能な限り正確な予測を行うことに主眼が置かれている。\n応用機械学習では、統計学を含む多くの異なる分野からアルゴリズムを借用、再利用、盗用し、こうした目的のために使用する。Section 12.5.1 では、GLM を用いて地すべりしやすさを予測した。\nここでは、同じ目的のためにサポートベクタマシン (Support Vector Machine, SVM)を紹介する。\nランダムフォレストモデルは SVM よりも人気があるだろう。しかし、ハイパーパラメータのチューニングがモデル性能に与えるプラスの効果は、SVM の場合の方が顕著である (Probst, Wright, Boulesteix 2018)。\n本節では、 (空間) ハイパーパラメータのチューニングが主な目的であるため、SVM を用いることにする。\nランダムフォレストモデルを適用したい方は、この章を読んでから Chapter 15 に進むことを勧める。この章では、現在取り上げられている概念と技術を応用して、ランダムフォレストモデルに基づく空間分布図を作成する方法を説明する。SVM クラスを分離するための最適な「超平面」を探索し (分類 の場合)、特定のハイパーパラメータで「カーネル」を推定して、クラス間の非線形境界を作成する (James et al. 2013)。\n機械学習には、ハイパーパラメータとパラメータがある。\nパラメータはデータから推定できるが、ハイパーパラメータは学習開始前に設定しなければならない (mlr3 本のmachine mastery blogとhyperparameter optimization chapter も参照)。\n最適なハイパーパラメータは、通常、交差検証法を用いて定義された範囲内で決定する。\nこれをハイパーパラメータチューニングという。kernlab が提供 SVM 実装の中には、ハイパーパラメータを自動的に、通常はランダムなサンプルに基づいて調整することができるものもある (Figure 12.3 の上段を参照)。\nこれは非空間データでは有効だが、空間データではあまり意味がなく、「空間チューニング」を行う必要がある。空間チューニングを定義する前に、Section 12.5.1 で紹介した mlr3 ビルディングブロックを SVM 用に設定することにする。\n分類のタスクは変わらないので、Section 12.5.1 で作成した task オブジェクトを再利用すればよい。\nSVM を実装している学習器は、mlr3extralearners パッケージの listLearners() を用いて検索することができる。オプションのうち、kernlab パッケージの ksvm() を使用することにする (Karatzoglou et al. 2004)。\n非線形関係を許容するために、ksvm() のデフォルトでもある、一般的な放射状基底関数 (またはガウス) カーネル (\"rbfdot\") を使用する。\ntype 引数に \"C-svc\" を設定することで、ksvm() が確実に分類タスクを解く。\n1 つのモデルの失敗でチューニングが止まらないように、フォールバック学習器を追加で定義している (詳細は https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-fallback を参照)。次の段階は、リサンプリング戦略を指定することである。\nここでも 100 回繰り返しの 5 回空間交差検証を使用する。これは Section 12.5.1 の GLM のリサンプリングに使われたコードと全く同じであることに注意しておこう。ここまでは、Section 12.5.1 で説明したものと同じである。\nしかし、次のステップは新しく、ハイパーパラメータ を調整する。\n性能評価とチューニングに同じデータを使用すると、楽観的すぎる結果になる可能性がある (Cawley Talbot 2010)。\nこれは、ネストされた空間交差検証 を用いることで回避することができる。\nFIGURE 12.6: CV におけるハイパーパラメータのチューニングと性能推定レベルの模式図 (図は Schratz et al. (2019) から引用した。快く再利用の許可をいただいた)。\nこれは、各フォールドを空間的に不連続な 5 つのサブフォールドに再び分割し、最適なハイパーパラメータ (tune_level 以下のコードチャンクのオブジェクト。視覚的表現については Figure 12.6 を参照) を決定するために使用することを意味する。\nさらに、値 C と Sigma のランダムな選択は、あらかじめ定義された調整空間 (search_space オブジェクト) に制限されている。\n同調空間の範囲は、文献で推奨されている値で選択した (Schratz et al. 2019)。\n最適なハイパーパラメータの組み合わせを見つけるために、これらのサブフォルダそれぞれにおいて、ハイパーパラメータ C とシグマにランダムな値を選択して 50 のモデル (以下のコードチャンクの terminator オブジェクト) を適合させた。次の段階は、auto_tuner() を用いてハイパーパラメータチューニングを定義するすべての特性に従って学習器 lrn_ksvm を修正することである。このチューニングは、1 つのフォールドに対して最適なハイパーパラメータを決定するために、250 のモデルを適合させるように設定されている。\nこれを 1 回ずつ繰り返すと、1,250 個 (250 * 5) のモデルができあがる。\n100 回繰り返すということは、合計 125,000 個のモデルを適合して最適なハイパーパラメータ( Figure 12.3 )を特定することになる。\nこれを性能推定に使用し、さらに 500 個のモデル (5 folds * 100 repetitions; Figure 12.3 参照) の適合が必要である。\n性能推定処理の連鎖をさらにわかりやすくするために、コンピュータに与えた命令を書き出してみよう。パフォーマンスレベル ( Figure 12.6 の左上部分) - データセットを 5 つの空間的に不連続な (外側の) サブフォールドに分割する。チューニング・レベル ( Figure 12.6 の左下部分) - パフォーマンス・レベルの最初のフォールドを使用し、ハイパーパラメータのチューニングのために、それを再び 5 つの (内側の) サブフォールドに空間的に分割する。 これらの内部サブフォールドのそれぞれで、ランダムに選択された 50 個のハイパーパラメータを使用する、つまり、250 個のモデルを適合させる。性能推定 - 前のステップ (チューニング・レベル) から最適なハイパーパラメータの組み合わせを使用し、性能レベルの最初の外側のフォールドに適用して性能を推定する (AUROC )。残りの 4 つの外側のフォールドについて、手順 2 と 3 を繰り返す手順 2～4 を 100 回繰り返すハイパーパラメータのチューニングと性能推定のプロセスには、計算量が必要である。\nモデルの実行時間を短縮するために、mlr3 では、future パッケージの助けを借りて、並列化を使用する可能性を提供している。\nこれからネストした CV を実行するので、内側ループと外側ループのどちらを並列化するか決めることができる (Figure 12.6 の左下部分を参照)。\n前者は 125,000 個のモデルを実行するのに対し、後者は 500 個しか実行しないので、内側のループを並列化するのは当然である。\n内側のループの並列化を設定するために、実行する。さらに、future には、利用可能なすべてのコア (デフォルト) ではなく、半分だけを使用するように指示した。これは、1 つのコアを使用する場合に、他のユーザーが同じ高性能計算機クラスタで作業する可能性を考慮した設定である。これで、ネストされた空間交差検証を計算するための準備ができた。\nresample() パラメータの指定は、GLM を使用したときと全く同じ手順で行う。唯一の違いは、store_models と encapsulate の引数である。\n前者を TRUE に設定すると、ハイパーパラメータのチューニング結果を抽出できる。これは、チューニングに関するフォローアップ分析を計画する場合に重要である。\n後者は、モデルの 1 つがエラーを投げても処理が継続されるようにするものである。\nこれにより、1 つのモデルが失敗しただけで処理が停止することを避けることができ、大規模なモデルの実行には望ましい。\n処理が完了すると、故障したモデルを見ることができる。\n処理終了後、future::ClusterRegistry(\"stop\") で明示的に並列化を停止するのがよいだろう。\n最後に、出力オブジェクト (result) を、別の R セッションで使用する場合に備えてディスクに保存する。\n125,500 個のモデルで空間交差検証を行うため、時間がかかることをご了承の上、実行してみよう。\n現代のコンピュータで、たった半日で実行できる。\n実行時間は、CPU 速度、選択したアルゴリズム、選択したコア数、データセットなど多くの側面に依存することに注意しておこう。ローカルでコードを実行したくない方のために、書籍の GitHub リポジトリに score_svm を保存してある。\n以下のように読み込むことができる。最終的な AUROC: モデルが 2 つのクラスを識別する能力を見てみよう。GLM は、この特定のケースでは、SVM よりもわずかに優れているようである (集計された AUROC は 0.77)。\n絶対的に公平な比較を保証するためには、2 つのモデルが全く同じパーティションを使用していることを確認する必要がある。ここでは示していないが、バックグラウンドで黙々と使用しているものである (詳しくは本書の GitHub リポジトリにある code/12_cv.R を参照)。\nそのために、mlr3 は関数 benchmark_grid() と benchmark() を提供している (https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking, Bischl et al. 2024 参照) 。\nこれらの機能については、「演習」でより詳しく解説する。\nまた、SVM のランダムな探索に 50 回以上の反復を使用すると、おそらくより良い AUROC (Schratz et al. 2019) を持つモデルになるハイパーパラメータが得られるであろうことに注意しておこう。\n一方、ランダムサーチの反復回数を増やすと、モデルの総数も増え、その分実行時間も長くなる。これまで、空間交差検証 は、学習アルゴリズムが未知のデータに対して汎化する能力を評価するために利用されていた。\n空間分布図作成では、完全なデータセットでハイパーパラメータを調整する。\nこれについては、Chapter 15 で説明する。","code":"\nmlr3_learners = mlr3extralearners::list_mlr3learners()\n#> This will take a few seconds.\nmlr3_learners |>\n  dplyr::filter(class == \"classif\" & grepl(\"svm\", id)) |>\n  dplyr::select(id, class, mlr3_package, required_packages)\n#>               id   class      mlr3_package              required_packages\n#>           <char>  <char>            <char>                         <list>\n#> 1:  classif.ksvm classif mlr3extralearners mlr3,mlr3extralearners,kernlab\n#> 2: classif.lssvm classif mlr3extralearners mlr3,mlr3extralearners,kernlab\n#> 3:   classif.svm classif      mlr3learners        mlr3,mlr3learners,e1071\nlrn_ksvm = mlr3::lrn(\"classif.ksvm\", predict_type = \"prob\", kernel = \"rbfdot\",\n                     type = \"C-svc\")\nlrn_ksvm$encapsulate(method = \"try\", \n                     fallback = lrn(\"classif.featureless\", \n                                    predict_type = \"prob\"))\n# パフォーマンス推定レベル\nperf_level = mlr3::rsmp(\"repeated_spcv_coords\", folds = 5, repeats = 100)\n# ５つに分割\ntune_level = mlr3::rsmp(\"spcv_coords\", folds = 5)\n# ランダムに選択されたハイパーパラメータの限界値を定義\nsearch_space = paradox::ps(\n  C = paradox::p_dbl(lower = -12, upper = 15, trafo = function(x) 2^x),\n  sigma = paradox::p_dbl(lower = -15, upper = 6, trafo = function(x) 2^x)\n)\n# 50 個のランダムに選択されたハイパーパラメータを使用\nterminator = mlr3tuning::trm(\"evals\", n_evals = 50)\ntuner = mlr3tuning::tnr(\"random_search\")\nat_ksvm = mlr3tuning::auto_tuner(\n  learner = lrn_ksvm,\n  resampling = tune_level,\n  measure = mlr3::msr(\"classif.auc\"),\n  search_space = search_space,\n  terminator = terminator,\n  tuner = tuner\n)\nlibrary(future)\n# 外側のループを順次実行し、内側のループを並列化する。\nfuture::plan(list(\"sequential\", \"multisession\"), \n             workers = floor(availableCores() / 2))\nprogressr::with_progress(expr = {\n  rr_spcv_svm = mlr3::resample(task = task,\n                               learner = at_ksvm, \n                               # 外側リサンプリング (パフォーマンスレベル) \n                               resampling = perf_level,\n                               store_models = FALSE,\n                               encapsulate = \"evaluate\")\n})\n# 並列化を終了\nfuture:::ClusterRegistry(\"stop\")\n# AUROC 値を計算\nscore_spcv_svm = rr_spcv_svm$score(measure = mlr3::msr(\"classif.auc\")) %>%\n# 必要な列のみ残す\nscore_spcv_svm = dplyr::select(score_spcv_svm, task_id, learner_id, \n                               resampling_id, classif.auc)\nscore = readRDS(\"extdata/12-bmr_score.rds\")\nscore_spcv_svm = score[learner_id == \"classif.ksvm.tuned\" & \n                         resampling_id == \"repeated_spcv_coords\"]\n# 最終的な AUROC 平均\nround(mean(score_spcv_svm$classif.auc), 2)\n#> [1] 0.74"},{"path":"spatial-cv.html","id":"conclusions","chapter":"12 統計的学習","heading":"12.6 結論","text":"リサンプリング手法は、データサイエンティストのツールボックスの重要なものの一つである (James et al. 2013)。\nこの章では、CV を用いて、様々なモデルの予測性能を評価した。\nSection 12.4 で述べたように、空間座標を持つ観測は、空間自己相関のために統計的に独立でない場合があり、交差検証の基本的な仮定に違反する。\n空間交差検証 この問題は、空間的自己相関によってもたらされるバイアスを低減することで解決される。mlr3 パッケージは、線形回帰、一般化加法モデルなどのセミパラメトリックモデルなどの統計学習、あるいはランダムフォレスト、SVM 、ブースト回帰木 (Bischl et al. 2016; Schratz et al. 2019) などの機械学習 技術と組み合わせることで、 (空間) リサンプリング技法を容易にしている。\n機械学習アルゴリズムは、ハイパーパラメータの入力を必要とすることがある。その最適な「チューニング」には、大規模な計算資源を必要とする数千回のモデル実行が必要で、多くの時間、RAM、コアを消費することがある。\nmlr3 は、並列化を可能にすることでこの問題に取り組んでいる。機械学習全体、そして空間データを理解するための機械学習は大きな分野であり、この章では基本的なことを説明したが、まだまだ学ぶべきことはある。\nこのような方向性で、以下の資料を勧める。mlr3 book (Bischl et al. (2024); https://mlr3book.mlr-org.com/) と、特に chapter handling spatio-temporal data を参考。ハイパーパラメータチューニングに関する学術論文 (Schratz et al. 2019)mlr3spatiotempcv の使用方法に関する学術論文 (Schratz et al. 2021)時空間データの場合、空間的と時間的の自己相関を考慮した上で CV を行う必要がある (Meyer et al. 2018)。","code":""},{"path":"spatial-cv.html","id":"演習-9","chapter":"12 統計的学習","heading":"12.7 演習","text":"E1. terra::rast(system.file(\"raster/ta.tif\", package = \"spDataLarge\"))$elev で読み込んだ elev データセットから、R-GIS ブリッジ (GIS ソフトウェアへのブリッジの章を参照) を用いて以下の地形属性を計算しなさい。傾斜角度平面曲率プロファイル曲率集水域E2. slope、cplan、cprof、elev、log_carea という新しい変数を追加し、対応する出力ラスタから lsl データフレーム (data(\"lsl\", package = \"spDataLarge\")) に値を抽出しなさい。E3. 導き出された地形属性ラスタを GLM と組み合わせて、Figure 12.2に示すような空間予測マップを作成しなさい。\ndata(\"study_mask\", \"package=\"spDataLarge\") を実行すると、調査地域のマスクが添付される。E4. GLM 学習器に基づき、100 回繰り返した 5 フォールドの非空間交差検証と空間交差検証を計算し、箱ひげ図を用いて両方のリサンプリング戦略からの AUROC 値を比較しなさい。ヒント: 非空間リサンプリング戦略を指定する必要がある。追加ヒント: mlr3::benchmark() と mlr3::benchmark_grid() を使って、練習問題 4 から 6 を一度に解くことができます (詳しくは https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-fallback を参照)。\nその際、計算には非常に時間がかかり、おそらく数日かかることを覚悟しよう。\nもちろん、これはシステムに依存する。\n自由に使える RAM とコアが多ければ多いほど、計算時間は短くなる。E5. 二次判別分析 (quadratic discriminant analysis, QDA) を用いて地すべり感受性をモデル化しなさい。\nQDA の予測性能を評価しなさい。\nQDA と GLM の空間交差検証平均 AUROC 値の差は?E6. ハイパーパラメータを調整せずに SVM を実行しなさい。\nrbfdot カーネルを \\(sigma\\) = 1 と C = 1 で使用しなさい。\nkernlab の ksvm() でハイパーパラメータを指定しないままにしておくと、自動的に非空間的なハイパーパラメータチューニングが初期化しなさい。","code":""},{"path":"transport.html","id":"transport","chapter":"13 交通解析","heading":"13 交通解析","text":"","code":""},{"path":"transport.html","id":"必須パッケージ","chapter":"13 交通解析","heading":"必須パッケージ","text":"この章では、以下のパッケージを使用する90","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spDataLarge)\nlibrary(stplanr)    # 地理交通データを処理\nlibrary(tmap)       # 地図作成 (Chapter 9 参照)\nlibrary(ggplot2)    # データ可視化パッケージ\nlibrary(sfnetworks) # 空間ネットワークのクラスと関数"},{"path":"transport.html","id":"introduction-13","chapter":"13 交通解析","heading":"13.1 イントロダクション","text":"交通ほど地理的空間が明確な分野も少ないだろう。\n1970年に Waldo Tobler が述べたように、移動の努力 (距離の克服) は、地理学の「第一法則」の中心である (Waldo R. Tobler 1970)。Everything related everything else, near things related distant things.この「法則」は、空間自己相関など、地理学の重要な概念の基礎となるものである。\nそれは、友情のネットワークや生態系の多様性といった多様な現象に適用され、「距離の摩擦」を構成する時間、エネルギー、金銭といった交通コストによって説明することができる。\nこの観点からすると、交通テクノロジーは破壊的であり、移動する人間やモノを含む地理的な実体間の関係を変化させる。「交通の目的は空間を克服することである」 (Rodrigue, Comtois, Slack 2013)。交通とは、本質的に空間的な活動であり、出発点「」から目的点「B」に移動し、その間にある無限の地域を通過することである。\nしたがって、交通研究者は、移動パターンを理解し、介入によってそのパフォーマンスをどのように改善できるかを理解するため、昔から地理的・計算的手法に注目してきた (Lovelace 2021)。本章では、交通システムの地理的分析について、さまざまな地理的レベルで紹介する。エリア単位: 交通パターンは、主な移動手段 (車、自転車、徒歩など) や、特定のゾーンに住む人々の平均移動距離など、ゾーンごとの集計を参照して理解することができる (Section 13.3 で解説する)。希望線 (desire line): 地理空間における場所 (点またはゾーン) 間を何人が移動したか (または移動できたか) を記録した「起点-終点」データを表す直線で、Section 13.4 の話題である。ノード : ノードとは、共通の起点と終点を表すことができる交通システムの点であり、バス停や鉄道駅などの公共交通機関の駅、Section 13.5 のトピックである。ルート: 希望線に沿ったルート網とノード間のルートを表す線である。\nルート (1 つの線または複数のセグメントの場合がある) と、ルートを生成するルートエンジンは、Section 13.6 で詳しく述べる。ルートネットワーク (network): これらは、ある地域の道路、小道、その他の線形特徴のシステムを表し、Section 13.7 で取り上げている。\nこれらは地理的な特徴 (短いセグメントでネットワーク全体を作り上げる) として表現することもできるし、相互接続されたグラフとして構造化することもできる。異なるセグメント上の交通のレベルは、交通モデルでは「フロー」と呼ばれる (Hollander 2016)。もうひとつの重要なレベルは、私やあなたのような移動する存在を表すエージェントである。\nエージェントは、 MATSim のようなソフトウェアのおかげで、計算によって表現することができる。これは、エージェントベースモデリング (agent-based modelling, ABM)のアプローチを用いて、高い空間および時間分解能で交通システムのダイナミクスを捉えるものである (Horni, Nagel, Axhausen 2016)。\nABM は、R の空間クラスと統合できる可能性が高い交通研究の強力なアプローチであるが (Thiele 2014; Lovelace Dumont 2016)、本章の範囲外である。\n地理的レベルやエージェントの次に来るのは、ほとんどの交通モデルの分析の基本単位であるトリップであり、出発地「」から目的地「B」までの単一目的の旅である (Hollander 2016)。\nトリップは、異なるレベルの交通システムを結合し、単純化すると、ゾーンの重心 (ノード) を結ぶ地理的な希望線として、あるいは交通ルートネットワークに沿った経路として表現することができる。\nこの文脈では、エージェントは通常、交通ネットワーク内を移動する点である。交通システムは動的である (Xie Levinson 2011)。\nこの章では、交通システムの地理的な分析に焦点を当てるが、変化のシナリオをシミュレートするために、この手法をどのように利用できるかについて、Section 13.8 にて詳しくみていこう。\n地理学上の交通モデルの目的は、このような時空間システムの複雑さを、その本質を捉える形で単純化することにあると解釈できる。\n適切なレベルの地理的分析を選択することで、最も重要な特徴や変数を失うことなく、この複雑さを単純化し、より良い意思決定とより効果的な介入を可能にすることができる (Hollander 2016)。一般的に、モデルは特定の問題を解決するために設計される。\nそのため、本章では、次章で紹介する政策シナリオを軸に、Bristol 市内で自転車を増やす方法は何があるかを問う。\nChapter 14 では、ジオコンピュテーションの応用として、新しい自転車店の立地の優先順位付けを行う。\n新しい自転車利用・インフラが効果的に配置されると、人々が自転車利用するようになり、自転車利用・ショップの需要や地域の経済活動を高めることができる。\nこれは、交通システムの重要な特徴を強調している。交通システムは、より広範な現象や土地利用パターンと密接に関連している。","code":""},{"path":"transport.html","id":"bris-case","chapter":"13 交通解析","heading":"13.2 Bristol のケーススタディ","text":"本章で使用する事例は、England 西部の都市 Bristol で、Wales の首都 Cardiff から東に 30 km ほど離れた場所にある。\nこの地域の交通網の概要は、Figure 13.1 に示されており、自転車、公共交通、自家用車のための多様な交通インフラが示されている。\nFIGURE 13.1: Bristol の交通網は、アクティブ (緑)、公共 (鉄道、青)、自家用車 (赤) の各移動手段を色分けして表現している。黒い境界線は、市街地の境界線 (黄色でハイライト) と、より広い TTWA (Travel Work Area) を表している。\nBristol は England で 10 番目に大きい市で、人口は 50 万人であるが、そのトラベルキャッチメントエリアはもっと大きい (Section 13.3 参照)。\n市内には航空宇宙、メディア、金融サービス、観光などの企業が集まり、2 つの主要な大学とともに、活気ある経済が展開されている。\nBristol は一人当たりの平均所得が高いが、深刻な貧困地域も含まれている (Bristol City Council 2015)。交通の面では、Bristol は鉄道や道路の便がよく、アクティブトラベル (訳註: 「アクティブトラベル」とは自転車または徒歩による交通のこと) のレベルも比較的高い。\nActive People Survey によると、国民の 19% が月に 1 回以上自転車を利用し、88% が歩いている (全国平均はそれぞれ 15%、81%)。\n2011年の国勢調査では、自転車で通勤していると答えた人は全体の 8% だったのに対し、全国ではわずか 3% にとどまっている。多くの都市と同様に Bristol も渋滞、大気質、運動不足などの大きな問題を抱えている。\n自転車は、これらの問題すべてに効率的に取り組むことができる。典型的な速度は、徒歩の時速 4〜6 km に対して時速 15〜20 km と、徒歩よりも自動車による移動を置き換える可能性が大きい。\nこのため、Bristol の交通戦略では、自転車利用について野心的な計画を立てている。この章では、交通研究における政策的配慮の重要性を強調するため、人々を車から解放し、より持続可能な手段、特に徒歩と自転車に乗せることを任務とする人々 (交通プランナー、政治家、その他の利害関係者) にエビデンスを提供する目的で書かれている。\nより広い目的として、ジオコンピュテーションがどのように証拠に基づく交通計画をサポートできるかを示すことである。\nこの章では、次のことを学ぶ。都市における交通行動の地理的パターンを説明するマルチモード・トリップをサポートする主要な公共交通機関のノードを特定する。移動の「希望線」を分析、多くの人が短距離をドライブする場所を見つける自動車を減らし自転車を増やすような自転車ルートの位置を特定する本章の実用的な面から始めるため、次節で、移動パターンに関するゾーンデータをロードする。\nこのようなゾーンレベルのデータは少ないが、地域の交通システム全体を基本的に理解するためには不可欠な場合が多い。","code":""},{"path":"transport.html","id":"transport-zones","chapter":"13 交通解析","heading":"13.3 交通ゾーン","text":"交通システムは主に線形のフィーチャやノード (例えば、ルートや駅) に基づいているが、連続した空間を具体的な単位に分割するために、面的なデータから始めることがしばしば意味を持つ (Hollander 2016)。\n調査地域 (ここでは Bristol) を定義する境界線に加えて、交通研究者にとって特に関心の高い 2 つのゾーンタイプ、すなわち発着地ゾーンがある。\n多くの場合、発着地には同じ地理的単位が使用される。\nしかし、学校や商店などの「トリップアトラクター」が多い地域では、「 Workplace Zones」のような異なるゾーニングシステムが、トリップ先の密度上昇を表すのに適切だろう (Office National Statistics 2014)。調査地域を定義する最も簡単な方法は、OpenStreetMap が返す最初のマッチング境界であることが多い。\nこれは osmdata を使用して bristol_region = osmdata::getbb(\"Bristol\", format_out = \"sf_polygon\") のようなコマンドで取得することができる。\nこの結果、最大の一致する都市地域の境界を表す sf オブジェクトが得られ、バウンディングボックスの長方形ポリゴンまたは詳細なポリゴン境界のいずれかが得られる。91\nイギリスの Bristol については、spDataLarge パッケージにある Bristol の公式な境界を表す詳細なポリゴンが返される。\nFigure 13.1 の内側の青い境界を参照。なお、この方法にはいくつかの問題点がある。OSM から最初に返された境界は、自治体が使用する正式な境界とは異なる場合があるOSM が正式な境界線を返したとしても、人々が移動する場所とはほとんど関係がないため、交通研究にとっては不適切かもしれない。Travel Work Area (TTWA)) は、水文学的における流域のようなゾーニングシステムを構築することで、これらの問題に対処している。\nTTWA はまず、人口の75％が通勤で利用する連続したゾーンと定義されており (Coombes, Green, Openshaw 1986)、この章ではこの定義を用いる。\nBristol は、周辺の町からのトラベル者を惹きつける主要な雇用主であるため、その TTWA は市域よりもかなり大きい (Figure 13.1 参照)。\nこの交通方向の境界を表すポリゴンは、本章の冒頭でロードした spDataLarge パッケージが提供するオブジェクト bristol_ttwa に格納されている。本章で使用する出発地と目的地は同じである。中間の地理的解像度で公式に定義されたゾーン (公式名称は Middle layer Super Output Areas MSOA) である。\nそれぞれ約 8,000 人が暮らしている。\nこのような行政区域は、特定の介入から最も恩恵を受ける可能性のある人々のタイプなど、交通分析に不可欠な状況を提供することができる (例 Moreno-Monroy, Lovelace, Ramos (2017))。ゾーンの地理的解像度は重要である。通常、地理的解像度の高い小さなゾーンが望ましいが、大きな地域で数が多いと、処理に影響を及ぼすことがある。\n特に、起点-終点 (OD) 分析は、 ゾーン数に応じて非線形に可能性の数が増加する (Hollander 2016)。\nゾーンが小規模の場合、匿名性のルールに関連する問題も発生する。\nゾーン内の個人の特定を推測できないようにするため、詳細な社会人口統計変数は、低い地理的解像度でしか利用できない。\n例えばイギリスでは、年齢や性別による移動手段の内訳は、自治体レベルでは利用できるが\n100 世帯程度で構成される出力エリアレベルでは利用できない。\n詳細は、www.ons.gov.uk/methodology/geography を参照。\n本章で使用する 102 のゾーンは、Figure 13.2 に図示されているように bristol_zones に格納されている。\n人口が密集しているところでは、ゾーンが小さくなっていることに注意しておこう。\nbristol_zones は、交通に関する属性データは含まず、各ゾーンの名称とコードのみである。トラベルデータを追加するために、Section 3.2.4 で説明されている一般的なタスクである属性結合を実行する。\nここでは、ons.gov.uk データポータルで提供されている、英国の2011年国勢調査の出勤時間に関するトラベルデータ bristol_od を使用する。\nbristol_od は、英国の2011年国勢調査によるゾーン間の通勤に関する起点 (Origin)ー終点 (Desitination) (OD) データセットである (Section 13.4 参照)。\nSection 13.4 1 列目は出発地のゾーン ID、2 列目は目的地のゾーンである。\nbristol_od は、bristol_zones よりも多くの行を持ち、ゾーンそのものよりもゾーン間の移動を表している。先ほどのコードチャンクの結果、各ゾーンに 10 組以上の OD ペアがあることがわかった。つまり、下図のように、bristol_zones に結合する前に発着地データを集約する必要がある (発着地データは Section 13.4 に記述されている)。上記のチャンクは、原産地別にデータをグループ化した (o の列に含まれる)bristol_od データセットの変数が数値であれば、それを集計して、各ゾーンに住む人の交通手段別の総数を求める92グループ化変数 o の名前を変更し、bristol_zones オブジェクトの ID 列 geo_code と一致するようにした結果として得られるオブジェクト zones_attr は、ゾーンを表す行と ID 変数を持つデータフレームである。\nID が zones データセットのものと一致するかどうかは、%% 演算子を用いて以下のように確認することができる。その結果、新しいオブジェクトには 102 のゾーンがすべて存在し、zone_attr、ゾーン上に結合できる形になっていることがわかった。93\nこれは、結合関数 left_join() を使って行われる (なお、inner_join() でも同じ結果になる)。結果は、調査地域の各ゾーンを起点とするトリップの総数 (ほぼ 25 万) とその移動手段 (自転車、徒歩、自動車、電車) を表す列が新たに追加されている zones_joined である。\nトリップの起点の地理的な分布は、左パネル (Figure 13.2) に示されている。\nこのことから、ほとんどのゾーンは、調査エリア内で 0〜4,000 のトリップを発生させていることがわかる。\nBristol の中心部付近に住む人の移動が増え、郊外に住む人の移動が減っている。\nこれはなぜだろうか。調査地域内のトリップだけを扱っていることを思い出そう。\n周辺ゾーンのトリップ数が少ないのは、周辺ゾーンにいる人の多くが、調査地域外の他の地域へ移動するためであると考えられる。\n調査地域外のトリップは、モデルで表現されていないゾーンに行くトリップをカバーする特別な目的地IDによって、地域モデルに含めることができる (Hollander 2016)。\nしかし、bristol_od のデータは、このようなトリップを無視している。「ゾーン内」モデルということである。OD データセットが出発地のゾーンに集約されるのと同じように、目的地のゾーンに関する情報を提供するために集約することもできる。\n人は、中心部に引き寄せられるように集まる傾向がある。\nこのことは、右図 (Figure 13.2) で表される空間分布が比較的不均一であり、最も多い目的地ゾーンが Bristol 市の中心部に集中していることを説明している。\nその結果、任意のモードによるトラベル先数を報告する新しい列を含む、zones_od が以下のように作成される。Figure 13.2 の簡易版は、以下のコードで作成する (図を再現するには、本書の GitHub リポジトリの code フォルダ の 13-zones.R を参照。tmap によるファセット地図の詳細は Section 9.2.7 を参照)。\nFIGURE 13.2: 地域内に居住・勤務するトリップ (通勤者) 数。左の地図は通勤トリップの出発地、右の地図は目的地のゾーン (13-zones.R スクリプトで生成)。\n","code":"\nnames(bristol_zones)\n#> [1] \"geo_code\" \"name\"     \"geometry\"\nnrow(bristol_od)\n#> [1] 2910\nnrow(bristol_zones)\n#> [1] 102\nzones_attr = bristol_od %>% \n  group_by(o) %>% \n  summarize_if(is.numeric, sum) %>% \n  dplyr::rename(geo_code = o)\nsummary(zones_attr$geo_code %in% bristol_zones$geo_code)\n#>    Mode    TRUE \n#> logical     102\nzones_joined = left_join(bristol_zones, zones_attr, by = \"geo_code\")\nsum(zones_joined$all)\n#> [1] 238805\nnames(zones_joined)\n#> [1] \"geo_code\"   \"name\"       \"all\"        \"bicycle\"    \"foot\"      \n#> [6] \"car_driver\" \"train\"      \"geometry\"\nzones_destinations = bristol_od |> \n  group_by(d) |> \n  summarize(across(where(is.numeric), sum)) |> \n  select(geo_code = d, all_dest = all)\nzones_od = inner_join(zones_joined, zones_destinations, by = \"geo_code\")\nqtm(zones_od, c(\"all\", \"all_dest\")) +\n  tm_layout(panel.labels = c(\"Origin\", \"Destination\"))"},{"path":"transport.html","id":"desire-lines","chapter":"13 交通解析","heading":"13.4 希望線","text":"希望線 (desire line) は、出発地と目的地をつなぎ、人々がゾーン間で行きたいと望む場所を表している。\nこれは、建物や風の強い道路などの障害物がなければ、-B 間を最も早く移動できる「蜂の飛行線」または「カラスの飛行線」ルートを表している (希望線をルートに変換する方法については、次のセクションで説明する)。\n一般的に、希望線は各ゾーンの地理的 (または人口加重) 重心を始点および終点として地理的に表現される。\nこのセクションでは、このタイプの希望線を作成して使用するが、OD データを基にした分析の空間的なカバレッジと精度を高めるために、複数の開始点と終了点を可能にする「ジッタリング」技術は知っておく価値がある (Lovelace, Félix, Carlino 2022)。データセット bristol_od に、希望線を表すデータをすでに読み込んでいる。\nこの起点ー終点 (OD) データフレームオブジェクトは、o で表されるゾーンと d、Table 13.1 で示されるゾーン間の移動人数を表している。\nOD データをすべてのトリップで並べ、上位5つだけをフィルタリングするには、次のように入力する (非空間属性操作の詳細については、Chapter 3 を参照)。TABLE 13.1: Bristol ODデータフレームの上位5組の起点と終点の サンプルで、調査地域のゾーン間の トラベル希望線を表している。この表は、Bristol での通勤・通学パターンを示すものである。\nこれは、上位 5 つの OD ペアにおいて、徒歩が最も人気のある交通手段であること、ゾーン E02003043 が人気のある目的地であること (Bristol の中心部、上位 5 つの OD ペアの目的地)、ゾーン E02003043 のある部分から別の部分へのゾーン内トリップ (Table 13.1 の最初の行) がデータセットで最も移動した OD ペアであることを実証している。\nしかし、政策的な観点から見ると、Table 13.1 で示される生データは、限られた用途にしか使えない。2,910 組の OD のうち、ごく一部しか含まれていないという事実を除けば、政策的措置が必要な場所はどこか、徒歩や自転車による移動がどの程度の割合を占めるのかについては、ほとんどわからないのである。\n次のコマンドは、これらのアクティブモードによって作られる各希望線の割合を計算する。OD ペアは、大きく分けてゾーン間とゾーン内の 2 種類がある。\nゾーン間 OD ペアは、目的地と出発地が異なるゾーン間の移動を表する。\nゾーン内 OD ペアは、同一ゾーン内の移動を表す (Table 13.1 の上段参照)。\n以下のコードチャンクでは、od_bristol をこの2種類に分割している。次のステップは、ゾーン間 OD ペアを、stplanr 関数 od2line() を用いて地図上にプロットできる希望線を表す sf オブジェクトに変換することである。94結果の図は Figure 13.3 に示されており、その簡易版は以下のコマンドで作成される (図を正確に再現するには 13-desire.R のコードを、 tmap による視覚化の詳細については Chapter 9 を参照)。\nFIGURE 13.3: Bristol のトリップパターンを表す希望線は、幅がトリップ数、色がアクティブモード (徒歩と自転車) によるトリップの割合を表している。4 本の黒い線は、Table 13.1のゾーン間 OD ペアを表している。\nこの地図から、地域の交通パターンを支配しているのは中心市街地であり、そこに政策を優先させるべきであることがわかるが、周辺には副都心も多く見られる。\n希望線は交通システムの重要な一般化された構成要素である。\nより具体的な構成要素としては、(希望線のような仮想的な直線ではなく) 特定の目的地を持つノードがある。\nノードについては次節で述べる。","code":"\nod_top5 = bristol_od |> \n  slice_max(all, n = 5)\nbristol_od$Active = (bristol_od$bicycle + bristol_od$foot) /\n  bristol_od$all * 100\nod_intra = filter(bristol_od, o == d)\nod_inter = filter(bristol_od, o != d)\ndesire_lines = od2line(od_inter, zones_od)\n#> Creating centroids representing desire line start and end points.\nqtm(desire_lines, lines.lwd = \"all\")"},{"path":"transport.html","id":"nodes","chapter":"13 交通解析","heading":"13.5 ノード","text":"地理的な交通データにおけるノードは、ネットワークを構成する主に一次元のフィーチャ (線) とゼロ次元のフィーチャ (点) である。\n交通ノードには 2 種類ある。ネットワーク上に直接存在しないノード、例えばゾーン重心 (次のセクションで取り上げる) あるいは家や職場などの個人の発着地交通網の一部であるノード。\n技術的には、ノードは交通ネットワーク上のどの点にも位置することができるが、実際には、ルートの交差点 (ジャンクション) やバス停や駅など交通ネットワークに出入りする点など、特殊な頂点である場合が多い95交通ネットワークは、グラフとして表すことができ、その中で各セグメントは、ネットワーク内の 1 つ以上の他のエッジに (地理的な線を表すエッジを介して) 接続されている。\nネットワーク外のノードは「重心コネクタ」で追加可能。重心コネクタとは、ネットワーク上の近くのノードへの新しいルートセグメントである (Hollander 2016) 。96\nネットワークの各ノードは、ネットワーク上の個々のセグメントを表す 1 つ以上の「エッジ」によって接続されている。\n交通ネットワークがグラフとして表現できることを Section 13.7 で確認する。公共交通機関の駅や停留所は特に重要なノードである。道路の一部であるバス停、または線路から数百メートルの歩行者入口ポイントによって表される大規模な鉄道駅のいずれかのタイプのノードとして表現されることができる。\nここでは、Bristol における自転車の増加という研究課題に関連して、公共交通機関のノードを説明するために鉄道駅を使用しよう。\n鉄道駅のデータは、bristol_stations の spDataLarge で提供されている。通勤において自動車から他の交通への転換を阻む共通の障壁は、自宅から職場までの距離が遠すぎると徒歩や自転車では無理だということである。\n公共交通機関は、都市への一般的なルートにおいて、高速かつ大量に利用できるオプションを提供することで、この障壁を軽減することができる。\nアクティブトラベルの観点から、公共交通機関を利用した長距離移動の「行程」は、旅を 3 つに分けている。住宅地から公共交通機関の駅までの起点となる行程公共交通機関 (通常、出発地の最寄り駅から目的地の最寄り駅まで)降車駅から目的地までの行程Section 13.4 で行った分析に基づき、公共交通機関のノードを使って、バスと (この例では) 鉄道を利用できるトラベルのための 3 分割の希望線を構築することができる。\n最初の段階は、公共交通機関の利用が多い希望線を特定することである。ここでは、先に作成したデータセット desire_lines にすでに電車での移動回数を表す変数が含まれているので簡単である (公共交通機関の利用可能性は、OpenTripPlanner などの公共交通ルート探索サービスを使って推定することも可能)。\nアプローチを簡単にするために、レールの使用量の上位 3 つの希望線のみを選択することにする。そこで、これらの線を 3 つに分解し、公共交通機関の乗り換えを表現することにする。\nこれは、希望線を、トラベルの出発地、公共交通機関、目的地の 3 つの線ジオメトリからなる複合線に変換することで実現できる。\nこの作業は、行列の作成 (起点、終点、鉄道駅を表す「経由点」)、最近傍の特定、複合線への変換の 3 段階に分けることができる。\nこの一連の処理は、line_via() 関数が行う。\nこの stplanr 関数は入力された線と点を受け取り、希望線のコピーを返す (この動作の詳細については geocompr.github.io ウェブサイトの Desire Lines Extended vignette と ?line_via を参照)。\n出力は入力と同じだが、公共交通機関のノードを使った旅を表す新しいジオメトリの列がある (以下に示す)。Figure 13.4 に示すように、最初の desire_rail の行に、自宅から出発駅まで、そこから目的地まで、そして最後に目的地から目的地までの移動を表す 3 つのジオメトリリスト列が追加されている。\nこの場合、目的地までの距離は非常に短いが (歩行距離)、出発地までの距離は十分であるため、往路の駅まで自転車での移動を促すためのインフラ投資を正当化することができる。Figure 13.4 にある 3 つの出発地の駅周辺の住宅地では、人々が通勤するために自転車を利用することができる。\nFIGURE 13.4: 鉄道利用率の高い直線的な希望線 (黒) を、公共交通機関 (グレー) を経由して出発駅 (赤) へ、そして目的地 (ごく短い青線) へという 3 レグに変換する中間点として使用される駅ノード (赤い点)。\n","code":"\ndesire_rail = top_n(desire_lines, n = 3, wt = train)\nncol(desire_rail)\n#> [1] 9\ndesire_rail = line_via(desire_rail, bristol_stations)\nncol(desire_rail)\n#> [1] 12"},{"path":"transport.html","id":"routes","chapter":"13 交通解析","heading":"13.6 ルート","text":"\n地理学者から見れば、ルートとは直線でなくなった希望線である。出発地と目的地は同じだが、から B へのルートはより複雑である。\nルートは、ローカルまたはリモートで実行されるルート探索サービスを使用して、希望線 (より一般的には起点と終点のペア) から生成される。希望線が 2 つの頂点 (始点と終点) しか持たないのに対し、ルートは任意の数の頂点を持つことができ、-B 間の点を直線で結んだものと定義される。これは線ジオメトリ (linestring) の定義である。\n長距離をカバーするルートや複雑なネットワークに沿ったルートは何千もの頂点を持つことができるが、グリッドベースや簡略化された道路ネットワーク上のルートは頂点数が少なくなる傾向がある。ルートは、希望線から生成されるか、より一般的には、希望線を表す座標ペアを含むマトリックスから生成される。\nこのルート検索プロセスは、広義に定義されたさまざまなルート検索エンジン、すなわち、起点から目的地までの移動方法を記述した形状と属性を返すソフトウェアやウェブサービスによって行われる。\nルート検索エンジンは、以下のように、R と相対的に実行される場所に基づいて分類することができる。ルート計算を可能にする R パッケージを使用したメモリ内ルート検索 (Section 13.6.2)R から呼び出せる、R の外部にあるローカルホスティングのルート検索エンジン (Section 13.6.3)R から呼び出せる Web API を提供する、外部エンティティによるリモートホスティング型ルート検索エンジン (Section 13.6.4)それぞれについて説明する前に、ルート検索エンジンを分類する他の方法について概説しておく価値がある。\nルート検索エンジンはマルチモードである。つまり、複数の交通手段からなるトリップを計算することができるし、そうでないこともある。\nマルチモードなルート検索エンジンは、それぞれが異なる交通手段で作られた複数の旅程 (leg)からなる結果を返すことができる。\n住宅地から商業地までの最適なルートは、(1) 最寄りのバス停まで歩く、(2) 目的地に最も近いノードまでバスに乗る、(3) 目的地まで歩く、などが考えられる (入力パラメータ式がある場合)。\nこの 3 つの旅程間の移行点は、一般的に「入口」(ingress) と「出口」(egress) と呼ばれ、公共交通機関の乗り降りを意味する。\nR5 のようなマルチモードなルート検索エンジンは、OpenStreetMap Routing Machine (OSRM) のような「ユニモーダル」ルート検索エンジンよりも高度で、入力データ要件も大きくなる (Section 13.6.3 に記載)。マルチモードエンジンの大きな強みは、電車やバスなどの「トランジット」(公共交通機関) トリップを表現する能力にある。\nマルチモデルルート検索エンジンは、公共交通機関のネットワークを表す入力データセットを必要とする。一般的には、General Transit Feed Specification (GTFS) ファイルで、これは tidytransit および gtfstools パッケージ内の関数で処理できる (GTFS ファイルを処理する他のパッケージやツールは利用可能)。\n特定の (公共ではない) 交通手段に焦点を当てたプロジェクトでは、単一モードなルート検索エンジンで十分かもしれない。\nルート検索エンジン (または設定) を分類するもう一つの方法は、ルート、レッグ、セグメントという出力の地理的なレベルによるものである。","code":""},{"path":"transport.html","id":"route-legs-segments","chapter":"13 交通解析","heading":"13.6.1 ルート、レッグ、セグメント","text":"ルート検索エンジンは、ルート、レッグ、セグメントという 3 つの地理的なレベルで出力を生成することができる。ルート レベルの出力には、出発地と目的地のペアごとに 1 つのフィーチャ (通常はデータフレーム表現におけるマルチライン文字列と関連する行) が含まれ、トリップごとに 1 つのデータ行があることを意味するレッグ レベルの出力には、各起点と終点のペアに 1 つのフィーチャと関連する属性が含まれまる。1 つのモードを含むだけのトリップの場合 (たとえば、自宅から職場まで車で行き、車まで の短い徒歩は無視)、レッグはルートと同じで、車の旅になる。公共交通機関を利用するトリップの場合、レッグは重要な情報を提供する。r5r の関数 detailed_itineraries() はレッグを返すが、紛らわしいことに、これは「セグメント」と呼ばれることもあるセグメントレベルの出力は、交通ネットワークの各小セクションのレコードを持つ、ルートに関する最も詳細な情報を提供する。一般的にセグメントは、OpenStreetMap の道と同じか、同じ長さになっている。cyclestreets 関数 journey() はセグメントレベルのデータを返し、これは stplanr の route() 関数が返す出発地と目的地レベルのデータでグループ化することによって集約できるほとんどのルート検索エンジンは、デフォルトでルートレベルを返すが、マルチモードエンジンは一般的にレッグレベル (単一の交通モードによる連続した移動ごとに 1 つの機能) の出力を提供する。\nセグメントレベルの出力は、より詳細な情報を提供するという利点がある。\ncyclestreets パッケージは、ルートごとに複数の「静かさ」レベルを返し、サイクルネットワークの「最も弱いリンク」の特定を可能にする。\nセグメントレベル出力の欠点は、ファイルサイズの増大と余分な詳細情報に関連する複雑さである。ルートレベルの結果は、関数 stplanr::overline() を使用してセグメントレベルの結果に変換することができる (Morgan Lovelace 2020)。\nセグメントやレッグレベルのデータを扱う場合、トラベルの開始点と終了点を表す列でグループ化し、セグメントレベルのデータを含む列を要約/集計することで、ルートレベルの統計情報を返すことができる。","code":""},{"path":"transport.html","id":"memengine","chapter":"13 交通解析","heading":"13.6.2 R のメモリ内ルート検索","text":"R のルート検索エンジンは、R のオブジェクトとしてメモリに格納されているルートネットワークをルート計算のベースとして使用することができる。\n選択肢としては、sfnetworks 、 dodgr、 cppRouting といったパッケージがあり、それぞれ次節のテーマであるルートネットワークを表す独自のクラス体系を提供している。R ネイティブなルート検索は高速で柔軟な反面、現実的なルート計算のための専用ルート検索エンジンに比べて、一般に設定が困難である。\nルート検索は難しい問題であり、ダウンロードしてローカルでホストできるオープンソースのルート検索エンジンに何百時間もの時間が費やされている。\n一方、R ベースのルート検索エンジンは、モデル実験や、ネットワークへの変更の影響の統計的分析に適しているかもしれない。\nルートネットワークの特性 (または異なるルートセグメントの種類に関連する重み) を変更し、ルートを再計算し、多くのシナリオの下で結果を分析することを1つの言語で行うことは、研究用途にメリットがある。","code":""},{"path":"transport.html","id":"localengine","chapter":"13 交通解析","heading":"13.6.3 ローカル型専用ルート検索エンジン","text":"ローカル型ルート検索エンジンには、OpenTripPlanner、Valhalla、OpenStreetMap Routing Machine (OSRM) (これは「ユニモーダル」のみ対応)、R5 などがある。\nR からこうしたサービスにアクセスするには、opentripplanner、valhallr、r5r、osrm などのパッケージがある (Morgan et al. 2019; Pereira et al. 2021)。\nローカルにホストされたルート検索エンジンは、ユーザのコンピュータ上で実行されるが、R とは別のプロセスで実行される。\n利点としては、実行速度が速く、異なる交通手段に対する重み付けプロファイルを制御できるという点がある。\n反対に欠点は、複雑なネットワークをローカルに表現することが難しい、定義済みルートプロファイルがない、時間的ダイナミクス (例えば交通)、特殊なソフトウェアが必要となる、などが挙げられる。","code":""},{"path":"transport.html","id":"remoteengine","chapter":"13 交通解析","heading":"13.6.4 リモート型専用ルート検索エンジン","text":"リモート型ルート検索エンジンは、Web API を使用して、起点と終点に関するクエリを送信し、専用のソフトウェアが動作する強力なサーバーで生成された結果を返す。\nOSRM の一般公開されているサービスのような、オープンソースルート検索エンジンに基づくルート検索サービスは、R から呼び出された場合、ローカルにホストされたインスタンスと全く同じように動作し、単に更新される「ベース URL」を指定する引数を必要とするだけである。\nしかし、外部のルート検索サービスは専用のマシンでホストされているため (通常、正確なルートを生成するインセンティブを持つ営利企業が資金を提供している)、以下のような利点がある。世界中 (または通常少なくとも広い地域) にルート検索サービスを提供すること確立されたルート検索サービスは、通常定期的に更新され、トラフィックレベルに対応することができるルート検索サービスは通常、専用のハードウェアとソフトウェアで実行され、ロードバランサーなどのシステムにより一貫したパフォーマンスを確保できるリモートルート検索サービスのデメリットとしては、バッチジョブができない場合の速度 (ルートごとにインターネットでのデータ転送に頼ることが多い)、価格 (例えば Google ルート検索 API では、無料のクエリ回数に制限がある)、ライセンス問題などが挙げられる。\ngoogleway と mapbox という二つのパッケージは、それぞれ Google と Mapbox のルート検索サービスへのアクセスを提供する。\n無料 (ただし料金に制限あり) のルート検索サービスは、 OSRM、 osrm からアクセスできる openrouteservice.org、openrouteservice などがある。最後のパッケージは CRAN にはない。\nまた、CycleStreets.net のような、より具体的なルート検索サービスもある。これは、「サイクリストによるサイクリストのための」サイクル・ジャーニー・プランナーで非営利の交通技術会社である。\nR では cyclestreets パッケージを通して CycleStreets ルートにアクセスできるが、多くのルート検索サービスには R インターフェースがなく、パッケージ開発の大きなチャンスとなっている。Web API へのインターフェースを提供する R パッケージを構築することはやりがいのある経験になることだろう。","code":""},{"path":"transport.html","id":"contraction-hierarchies-and-traffic-assigment","chapter":"13 交通解析","heading":"13.6.5 縮約階層とトラフィック割り当て","text":"縮約階層とトラフィック割当は、交通モデリングにおける高度だが重要なトピックである。\n多くの経路を計算することは計算資源を大量に消費し、何時間もかかることがあるため、経路計算を高速化するためのアルゴリズムがいくつか開発された。\n縮約階層 (contraction hierarchy) はよく知られたアルゴリズムで、ネットワークのサイズにもよるが、ルート検索課題の大幅な (場合によっては 1,000 倍以上の) 高速化につながる。\n縮約階層は、前のセクションで述べたルート検索エンジンの舞台裏で使われている。交通の割当てはルート検索と密接に関係する問題で、実際には 2 点間の最短経路が最速とは限らず、特に混雑している場合は最速になる。\nこの処理では、13.4 で説明するような OD データセットを受け取り、 ネットワークの各セグメントにトラフィックを割り当てて、13.7 で説明するような ルートネットワークを生成する。\nこの問題に対する解は、Wardrop の利用者均衡の原則である。この原則は、現実に即して、ネットワーク上のフローを見積もる際に混雑を、数学的に定義されたコスト・フロー関係を参照しながら考慮すべきであることを示している (Wardrop 1952)。\nこの最適化問題は、cppRouting パッケージで実装されている反復アルゴリズムによって解くことができる。このパッケージは、高速ルート検索のための縮約階層も実装している。","code":""},{"path":"transport.html","id":"ルート検索-実例紹介","chapter":"13 交通解析","heading":"13.6.6 ルート検索: 実例紹介","text":"前のセクションで生成された希望線をすべてルート探索する代わりに、政策上関心のある希望線に焦点を当てることにする。\nデータ全体を処理する前に、一部のデータに対して計算量の多い処理を実行することは、賢明な方法である。ルート検索も然り。\nルート検索は、ジオメトリが詳細でかつローとオブジェクトの属性が多いと、時間とメモリを消費し、オブジェクトが巨大になる。\nこのため、この節では、ルートを計算する前に希望線をフィルタする。自転車トラベルの利点は、自動車トラベルを置き換えるときに最も大きく、比較的短いトラベルは長いトラベルよりも自転車に乗る可能性が高いという観察に基づいて、自転車トラベルの可能性を推定することに重点を置いて希望線のサブセットをフィルタリングする (Lovelace et al. 2017)。\n短いトリップ (5 km 程度、時速 20 km で 15 分程度で走れる距離) は、比較的自転車で移動する確率が高く、電動自転車で移動すると最大距離が延びる (Lovelace et al. 2017)。\nこれらの考察に基づき、希望線をフィルタし、多くの (100 以上の) 短い (ユークリッド距離 2.5〜5 km) トリップを駆動する OD ペアを表すオブジェクト desire_lines_short を返す次のコードチャンクに反映しよう。上記のコードで、st_length() は Section 4.2.3 にあるように、各希望線の長さを計算している。\nSection 3.2.1 で述べるように、dplyr の filter() 関数で、上記の条件に基づいて desire_lines データセットにフィルタをかけている。\n次に、この希望線をルートに変換する。\nこれは一般に公開されている OSRM サービスを用いて、以下のコードにある stplanr 関数 route() と route_osrm() で行われる。出力は routes_short で、(少なくとも OSRM ルート検索エンジンによれば) 自転車利用に適したトランスポートネットワーク 上のルートを表す sf オブジェクトで、各希望線に対して一つずつ出力される。\n注意: 上記のコマンドのような外部のルート検索エンジンの呼び出しは、インターネット接続 (そして、今回は必要ないが、環境変数に保存された API キーも必要な場合がある) でのみ動作する。\ndesire_lines オブジェクトに含まれる列に加えて、新しいルートデータセットには distance (今回はルートの距離を参照) と duration (秒単位) の列が含まれ、それぞれのルートについて有用な追加情報を提供する可能性がある。\n自転車ルートと並行する車による短い希望線をプロットする。\n道路網への介入に優先順位をつける効果的な方法を提供するため、ルート幅を置き換えられる可能性のある車の旅の数に比例させる (Lovelace et al. 2017)。\nFigure 13.5 は、自動車による短距離ルートを示している (ソースコードは github.com/geocompx を参照)。97\nFIGURE 13.5: 短距離 (ユークリッド距離 5 km 未満) の自動車移動が多数 (100 回以上) 行われたルート (赤) と、同じ移動を表す希望線 (黒) および重心 (点) を重ねたもの。\nインタラクティブ地図で可視化してみると、Bristol 中心部から約 10 km 北の Bradley Stoke 周辺で多くの短距離自動車トラベルが行われていることがわかる。\nWikipedia によると、Bradley Stoke は「民間投資によって建設されたヨーロッパ最大のニュータウン」であり、公共交通機関の整備が限定的であることを示唆している。\nさらに、この町は、「M4 高速道路と M5 高速道路など、大規模な (自転車利用に不利な) 道路構造に囲まれている」 (Tallon 2007)。トラベル希望線をトラベルルートに変換することは、政策の観点から多くの利点がある。\nルート検索エンジンによって計算された正確なルートをたどるトリップがどれだけあるか (あったとしても) 確認することはできないことを覚えておくことが重要である。\nしかし、ルートや道路・区間レベルの結果は、政策に大きく関連する可能性がある。\nルートセグメントの結果は、利用可能なデータ に従って、最も必要な場所に投資を優先させることを可能にすることができる (Lovelace et al. 2017)。","code":"\ndesire_lines$distance_km = as.numeric(st_length(desire_lines)) / 1000\ndesire_lines_short = desire_lines |> \n  filter(car_driver >= 100, distance_km <= 5, distance_km >= 2.5)\nroutes_short = route(l = desire_lines_short, route_fun = route_osrm,\n                     osrm.profile = \"car\")"},{"path":"transport.html","id":"route-networks","chapter":"13 交通解析","heading":"13.7 ルートネットワーク","text":"\n一般に路線は、希望線と同じレベルのデータ (または重複する可能性のあるセグメントのレベル) を含むが、路線網データセットは、交通網をほぼ完全に表現するものである。\n路線網の各セグメント (交差点間の連続した道路区間にほぼ相当) は、一度だけ存在する。しかし、セグメントの平均長はデータソースによって異なる (このセクションで使用した OSM 由来の bristol_ways データセットのセグメントの平均長は 200 m 強で、標準偏差は 500 m 近い)。\nセグメント長にばらつきがあるのは、地方では交差点が遠く、密集した都市部では数メートルおきに交差点があるなど、セグメントの切れ目があるためと思われる。路線網は、インプットの場合もあれば、アウトプットの場合もあり、両方の場合もある。\nルート計算を行う交通研究は、内部または外部のルート検索エンジンのルートネットワークデータを必要とする (後者の場合、ルート網データは必ずしも R にインポートされるわけではない)。\nしかし、路線ネットワークは、多くの交通研究プロジェクトにおいて重要なアウトプットでもある。特定の区間で発生しうるトリップ数などのデータをまとめ、路線ネットワークとして表現することで、最も必要なところに優先的に投資することができる。\nルートレベルのデータから得られる出力としてルートネットワークを作成する方法を示すために、モーダルシフトの簡単なシナリオを想像してみたい。\nルート距離 0～3 km の車移動の 50% が自転車に置き換えられ、その割合はルート距離が 1 km 増えるごとに 10 ポイントずつ下がり、6 km の車移動の 20％ が自転車に置き換えられ、8 km 以上の車移動が自転車に置き換えられないと想像しよう。\nこれはもちろん非現実的なシナリオ (Lovelace et al. 2017) ではあるものの、出発点としては有用であろう。\nこの場合、自動車から自転車へのモーダルシフトを次のようにモデル化することができる。約 4,000 のトリップが車から自転車に切り替わるというシナリオを作成したので、この更新されたモデル化された自転車利用活動がどこで行われるかをモデル化することができるようになった。\nこれには、stplanr パッケージの関数 overline() を使用する。\nこの関数は、ルートと要約する属性の名前を含むオブジェクトを第 1 と第 2 の引数として受け取り、分岐点 (2 つ以上の線の形状が出会うところ) で線を切断し、それぞれのユニークなルートセグメント (Morgan Lovelace 2020) について集約した統計量を計算するものである。前出の 2 つのコードチャンクの出力は、以下の Figure 13.6 に要約している。\nFIGURE 13.6: 距離関数による自動車から自転車への移行率 (左) と、この関数のルートネットワークレベルの結果 (右)。\n道路種別や幅員などの属性がセグメントレベルで記録されている交通網は、一般的な路線網の一種である。\nこのような路線網のデータセットは、OpenStreetMap から世界中で入手でき、osmdata や osmextract などのパッケージでダウンロードすることが可能である。\nOSMのダウンロードと準備の時間を節約するために、以下の出力に示すように、ケーススタディ地域の交通ネットワークのサンプルを表す LINESTRING ジオメトリと属性を持つ sf オブジェクト、spDataLarge パッケージから bristol_ways オブジェクトを使用する (詳細は ?bristol_ways を参照)。出力は、bristol_ways が交通ネットワーク上の 6,000 以上のセグメントを表していることを示している。\nこのネットワークや他の地理的なネットワークは、数学的なグラフとして表現することができ、ネットワーク上のノードとエッジで接続されている。\nこのようなグラフを扱うために、多くの R パッケージが開発されており、特に igraph が有名である。\nルートネットワークを手動で igraph オブジェクトに変換することはできるが、地理的な属性は失われる。\nこの igraph の制限を克服するために、路線ネットワークをグラフと地理的な線として同時に表現する sfnetworks パッケージ (van der Meer et al. 2023) が開発された。\nこれは、グラフおよび地理的な線で表現し、以下に見るように tidy な文法である。前のコードチャンクの出力 (スペースの関係上、最終的な出力は最も重要な 8 行のみを含むように短縮されている) は、ways_sfn がグラフ形式と空間形式の両方のノードとエッジを含む複合オブジェクトであることを表している。\nways_sfn は sfnetwork クラスで、 igraph パッケージの igraph クラスをベースにしている。\n下の例では、各エッジを通る最短ルートの数を意味する ‘edge betweenness’ が計算されている (詳しくは ?igraph::betweenness を参照)。\nこのデータセットに対して、エッジの間隔を計算した結果を図に示す。図には、比較のために overline() 関数で計算した自転車ルートネットワークデータセットをオーバーレイで表示している。\nこの結果は、グラフの各エッジがセグメントを表していることを示している。道路ネットワークの中心に近いセグメントは最も高い betweenness 値を持ち、一方、Bristol 中心部に近いセグメントはより高い自転車利用の可能性を持っていることが、これらの単純化されたデータセットに基づいて示されている。\nFIGURE 13.7: 路線ネットワークデータセット。グレーの線は簡略化された道路網を表し、セグメントの太さは betweenness に比例する。緑色の線は、上記のコードで計算された潜在的な自転車利用フロー (片道) である。\nまた、sfnetworks パッケージを用いると、このルートネットワークのグラフ表現を使って、出発地と目的地の間の最短ルートを求めることができる。\n\nこのセクションで紹介した方法は比較的単純で、実際にはもっと可能なことはある。\nsfnetworks が提供するグラフと空間の二つの機能により、多くの新しい強力な技法が可能になるが、このセクションで完全にカバーすることはできない。このセクションは、この分野のさらなる探求と研究のための強力な出発点を提供する。\n上で使った例のデータセットが比較的小さい。\nデータのサブセットで手法をテストし、十分な RAM を確保することが助けになるが、R5 (Alessandretti et al. 2022) などの大規模ネットワークに最適化された交通ネットワーク分析ができる他のツールも調べる価値があるだろう。","code":"\nuptake = function(x) {\n  case_when(\n    x <= 3 ~ 0.5,\n    x >= 8 ~ 0,\n    TRUE ~ (8 - x) / (8 - 3) * 0.5\n  )\n}\nroutes_short_scenario = routes_short |> \n  mutate(uptake = uptake(distance / 1000)) |> \n  mutate(bicycle = bicycle + car_driver * uptake,\n         car_driver = car_driver * (1 - uptake))\nsum(routes_short_scenario$bicycle) - sum(routes_short$bicycle)\n#> [1] 3255\nroute_network_scenario = overline(routes_short_scenario, attrib = \"bicycle\")\nsummary(bristol_ways)\n#>      highway       maxspeed             ref                     geometry   \n#>  cycleway:1721   Length:6160        Length:6160        LINESTRING   :6160  \n#>  rail    :1017   Class :character   Class :character   epsg:4326    :   0  \n#>  road    :3422   Mode  :character   Mode  :character   +proj=long...:   0\nbristol_ways$lengths = st_length(bristol_ways)\nways_sfn = as_sfnetwork(bristol_ways)\nclass(ways_sfn)\n#> [1] \"sfnetwork\" \"tbl_graph\" \"igraph\"\nways_sfn\n#> # A sfnetwork with 5728 nodes and 4915 edges\n#> # A directed multigraph with 1013 components with spatially explicit edges\n#> # Node Data:     5,728 × 1 (active)\n#> # Edge Data:     4,915 × 7\n#>    from    to highway maxspeed ref                              geometry lengths\n#>   <int> <int> <fct>   <fct>    <fct>                    <LINESTRING [°]>     [m]\n#> 1     1     2 road    <NA>     B3130 (-2.61 51.4, -2.61 51.4, -2.61 51.…    218.\n#> # … \nways_centrality = ways_sfn |> \n  activate(\"edges\") |>  \n  mutate(betweenness = tidygraph::centrality_edge_betweenness(lengths)) \nbb_wayssln = tmaptools::bb(route_network_scenario, xlim = c(0.1, 0.9), ylim = c(0.1, 0.6), relative = TRUE)\ntm_shape(zones_od) +\n  tm_fill(fill_alpha = 0.2, lwd = 0.1) +\n  tm_shape(ways_centrality |> st_as_sf(), bb = bb_wayssln, is.main = TRUE) +\n  tm_lines(lwd = \"betweenness\", \n           lwd.scale = tm_scale(n = 2, values.scale = 2),\n           lwd.legend = tm_legend(title = \"Betweenness\"),\n           col = \"#630032\", col_alpha = 0.75) +\n  tm_shape(route_network_scenario) +\n  tm_lines(lwd = \"bicycle\",\n           lwd.scale = tm_scale(n = 2, values.scale = 2),\n           lwd.legend = tm_legend(title = \"自転車トリップ数 (modeled, one direction)\"),\n           col = \"darkgreen\", col_alpha = 0.75) +\n  tm_scalebar() + tm_layout(fontfamily = \"HiraginoSans-W3\")\n#> [v3->v4] `tm_layout()`: use text.fontfamily instead of fontfamily"},{"path":"transport.html","id":"prioritizing-new-infrastructure","chapter":"13 交通解析","heading":"13.8 新インフラの優先順位付け","text":"この節では、交通計画分野においてジオコンピュテーションが政策関連の成果を作ることができることを示す。\n持続可能な交通インフラの投資先として有望な場所を、教育目的のシンプルなアプローチで特定する。本章で紹介するデータ駆動型アプローチの利点は、モジュール化されていることである。\nこの段階に至るまでには、(希望線から生成された)短いが車に依存する通勤経路を特定するための手順があり、(ルートネットワーク) セクションで sfnetworks パッケージを使用してルートネットワークの特性を分析することが含まれている。\n本章の最後のコードチャンクは、自転車利用インフラから短い距離のエリアを表す新しいデータセットに、前のセクションの自転車利用潜在力の推定値を重ねることによって、これらの一連の分析を結合する。\nこの新しいデータセットは、以下のコードで作成される。(1) 交通ネットワークを表す bristol_ways オブジェクトから自転車道をフィルタする。(2) 自転車道の個々の線エンティティを単一の複合線オブジェクトに「統合」する (バッファ作成が速くなるため)。(3) 周囲に 100 m バッファのポリゴンを生成する。次の段階は、ネットワークの中で、自転車利用の可能性が高いが、自転車利用のための設備がほとんどない点を表すデータセットを作成することである。Figure 13.8 は、自動車への依存度が高く、自転車の利用可能性が高いが、自転車専用道路が整備されていないルートを示している。\nFIGURE 13.8: Bristol における自動車依存度を下げるために、自転車インフラを優先的に整備するルートの候補。静的マップは、既存のインフラと自動車と自転車の乗り換えの可能性が高いルートとの間のオーバーレイの概要を提供する (左)。qtm() 関数から生成されたインタラクティブ地図のスクリーンショットは、新しい自転車道から利益を得ることができる場所として Whiteladies Road を強調している (右)。\nこの方法には限界がある。現実には、人々はゾーン重心に移動したり、特定のモードの最短ルートアルゴリズムを常に使用するわけではない。\nしかし、この結果は、自動車依存と公共交通の観点から、自転車専用道路を優先的に整備できるルートを示している。\nこの分析は、現実の交通計画の立案に使うためには、より大きなデータを使うなど、大幅に拡大して行う必要がある。","code":"\nexisting_cycleways_buffer = bristol_ways |> \n  filter(highway == \"cycleway\") |>    # 1) filter out cycleways\n  st_union() |>                       # 2) unite geometries\n  st_buffer(dist = 100)               # 3) create buffer\nroute_network_no_infra = st_difference(\n  route_network_scenario,\n  route_network_scenario |> st_set_crs(st_crs(existing_cycleways_buffer)),\n  existing_cycleways_buffer\n)\ntmap_mode(\"view\")\nqtm(route_network_no_infra, basemaps = leaflet::providers$Esri.WorldTopoMap,\n    lines.lwd = 5)"},{"path":"transport.html","id":"future-directions-of-travel","chapter":"13 交通解析","heading":"13.9 今後の方向性","text":"この章では、交通研究にジオコンピュテーションを利用する可能性を紹介し、オープンデータと再現可能なコードを用いて、都市の交通システムを構成するいくつかの重要な地理的要素を調査した。\nこの結果は、どこに投資が必要かを計画するのに役立つだろう。交通システムは複数の相互作用レベルで機能しているため、ジオコンピュテーションの手法は交通システムの仕組みや異なる介入の効果を理解する上で大きな可能性を秘めている。\nこの分野でできることはまだまだたくさんある。この章で紹介した基礎の上に、さまざまな方向性を構築することが可能であろう。\n交通は、多くの国で温室効果ガスの排出源として最も急速に増加しており、「特に先進国では最大の温室効果ガス排出部門」になると言われている (EURACTIV.com を参照)。\n交通機関の排出量は社会全体で非常に不平等に配分されており、交通機関は (食料や暖房とは異なり) 幸福に不可欠なものではない。\n需要の削減、車両の電化、徒歩や自転車などのアクティブな移動手段の導入により、このセクターが急速に脱炭素化する大きな可能性を秘めている。\n新しいテクノロジーは、カーシェアリングを可能にすることで、車への依存度を下げることができる。\nドックレス型自転車や e スクーターのような「マイクロモビリティ」システムも出現し、General Bikeshare Feed Specification (GBFS) フォーマットの貴重なデータセットを作成し、gbfsパッケージで取り込んで処理することができるようになった。\nこうした新しい交通などの変化は、人々が必要とする雇用やサービスの場所に到達する能力であるアクセシビリティに大きな影響を与える。そして、accessibility パッケージなどのパッケージを使用して、現在および変化のシナリオに基づいて定量化することが可能である。\nこうした「交通の未来」を地域や国レベルでさらに探求することで、新たな知見を得られるだろう。方法論的には、本章で示した基礎は、より多くの変数を分析に含めることで拡張することが可能である。\n速度制限、交通量の多さ、自転車や歩行者用保護道の設置などのルートの特徴は、「モーダルスプリット」 (modal split、異なる交通手段によるトラベルの割合) に関連付けることができる。\n例えば、OpenStreetMap のデータをバッファや、Chapter 3 と Chapter 4 で紹介した地理データ手法で集約すれば、交通ルートの近くに緑地があるかどうかを検出することも可能である。\nR の統計モデリング機能を使えば、例えば、現在と将来の自転車利用のレベルを予測することができるだろう。この種の分析のベースには、Propensity Cycle Tool (PCT)がある。 これは、R で開発された一般にアクセス可能な ( www.pct.bike 参照) 地図作成ツールで、イングランド全域の自転車利用への投資を優先させるために使用されている (Lovelace et al. 2017)。\n同様のツールは、世界中の大気汚染や公共交通機関へのアクセスなど、他のテーマに関連したエビデンスに基づく交通政策を奨励するためにも使用できる。","code":""},{"path":"transport.html","id":"ex-transport","chapter":"13 交通解析","heading":"13.10 演習","text":"E1. 本章で紹介した分析の多くでは、アクティブ (自転車のこと) という交通モードに焦点を当てたが、車でのトリップ についてはどうだろうか?desire_lines オブジェクトに含まれるトリップのうち、車でのトリップの割合は?直線距離が 5 km 以上のdesire_linesの割合は?長さが 5 km 以上の希望線に含まれるトリップのうち、車で移動するトリップの割合は?長さが 5 km 未満で、移動の 50% 以上が車である希望線をプロットする。これらの自動車に依存しながらも短い希望線の位置について、何か気づくことはあるか?E2. Figure 13.8 に示されたすべてのルート (既存の自転車道から 100 m 以上離れた区間) が建設された場合、自転車道の長さはどの程度増加するか?E3. desire_lines に含まれるトリップのうち、routes_short_scenario に含まれるトリップの割合はいくらか?ボーナス: 全トリップのうち、routes_short_scenario を横切る希望線の割合は？E4. 本章で紹介する分析は、ジオコンピュテーションの手法をどのように交通研究に応用できるかを教えるためのものである。\n実際に政府機関や交通コンサルタント会社でこのようなことをする場合、どの点が変わるだろうか? 大きいものから 3 点述べなさい。E5. Figure 13.8 で特定されたルートは、明らかに、全体像の一部を示しているに過ぎない。\nどのように分析を拡張するか?E6. カーフリーゾーン、駐輪ポイント、減車戦略など、場所ベースのサイクリング政策に投資するための主要なエリア (ルートではない) を作成することによって、シナリオを拡張したいと想像する。\nラスタデータセットは、この作業をどのように支援できるか?ボーナス: Bristol 地域を 100 のセル (10 * 10) に分割し、それぞれの道路の平均制限速度を bristol_ways データセットから推定するラスタレイヤを開発しなさい (Chapter 14 参照)。","code":""},{"path":"location.html","id":"location","chapter":"14 商圏分析","heading":"14 商圏分析","text":"","code":""},{"path":"location.html","id":"prerequisites-14","chapter":"14 商圏分析","heading":"必須パッケージ","text":"この章では、以下のパッケージが必要である (revgeo もインストールしておく必要がある)。必要なデータは順次ダウンロードする読者の利便性と再現性を確保するため、ダウンロードしたデータを spDataLarge パッケージで公開している。","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(terra)\nlibrary(osmdata)\nlibrary(spDataLarge)"},{"path":"location.html","id":"イントロダクション","chapter":"14 商圏分析","heading":"14.1 イントロダクション","text":"この章では、パート とパート II で学んだスキルを特定のドメインに適用する方法を示す。商圏分析 (立地分析やロケーションインテリジェンスとも呼ばれることがある) である。\n研究・実用化されている分野は幅広い。\nその典型的な例として、どこに新しい店舗を置くかを考えよう。\nここでの目的は、最も多くの訪問者を集め、最終的に最も多くの利益を上げることである。\nまた、例えば新しい医療サービスをどこに配置するかなど、この技術を公共の利益のために利用できる非商業的なアプリケーションも多い (Tomintz, Clarke, Rigby 2008)。立地分析の基本は人である。 特に時間やその他のリソースを費やす可能性が高い場所である。\n興味深いことに、エコロジーの概念やモデルは、店舗立地分析に使われるものと非常によく似ている。\n動物や植物は、空間的に変化する変数に基づいて、特定の「最適な」場所でそのニーズを最もよく満たすことができる (Muenchow et al. (2018); Chapter 15 も参照)。\nこれはジオコンピュテーションや GIS サイエンス全般の大きな強みである。コンセプトや手法は他の分野にも転用可能である。\n例えば、ホッキョクグマは気温が低く餌 (アザラシやアシカ) が豊富な北方を好む。\n同様に、人間は特定の場所に集まる傾向があり、北極の生態学的ニッチに類似した経済的ニッチ (そして高い地価) を作り出す。\n立地分析の主な作業は、利用可能なデータに基づいて、特定のサービスにとってそのような「最適な場所」がどこであるかを見つけ出すことである。\n典型的なリサーチクエスチョンとしては、以下のようなものがある。ターゲット層はどこに住んでいて、どのエリアによく行くのか?競合する店舗やサービスはどこにあるのか?特定の店舗にどれくらいの人が行きやすいか?既存のサービスは、市場の潜在力を過大に、あるいは過小に開拓していないか?ある企業の特定地域における市場シェアはどのくらいか?本章では、ジオコンピュテーションが実データと仮想的なケーススタディに基づく疑問に答えることができることを示す。","code":""},{"path":"location.html","id":"case-study","chapter":"14 商圏分析","heading":"14.2 ケーススタディ: ドイツの自転車店","text":"ドイツで自転車店のチェーンを始めるとする。\n店舗は、できるだけ多くの潜在顧客がいる都市部に配置したい。\nさらに、仮定の調査 (この章のために考案されたもので、商業利用はできない!) によると、独身の若い男性 (20 歳から40 歳) が貴社の製品を購入する可能性が最も高いのでターゲット層である。\nあなたは、何店舗も出店できる十分な資金を持っている幸運な立場にある。\nしかし、どこに配置すればいいのだろうか?\nコンサルティング会社 (商圏分析アナリストを雇っている) は、このような質問に答えるために喜んで高い料金を取るだろう。\n幸い、オープンデータやオープンソースソフトウェアの力を借りれば、私たち自身でそれを行うことができる。\n以下の章では、本書の最初の章で学んだテクニックを、サービスロケーション解析の一般的なステップにどのように応用できるかを紹介する。ドイツの国勢調査の入力データを整理 (Section 14.3)集計された国勢調査データをラスタオブジェクトに変換 (Section 14.4 )人口密度の高い都市圏の特定 (Section 14.5 )これらの地域の詳細な地理データ (osmdata で OpenStreetMap) をダウンロード (Section 14.6)マップ代数を用いて異なる場所の相対的な望ましさをスコアリングするためのラスタを作成 (Section 14.4)これらのステップは、特定のケーススタディに適用されたが、店舗立地や公共サービス提供の多くのシナリオに一般化することができる。","code":""},{"path":"location.html","id":"tidy-the-input-data","chapter":"14 商圏分析","heading":"14.3 入力データを整頓","text":"ドイツ政府は、1 km または 100 m の解像度でグリッド化された国勢調査データを提供している。\n次のコードは、1 km のデータをダウンロード、解凍、読み込むものである。なお、census_de は spDataLarge パッケージ (data(\"census_de\", package = \"spDataLarge\") からも入手可能である。census_de オブジェクトは、ドイツ全土の 30 万以上のグリッドセルのデータフレームで、変数が 13 ある。\nこれからの作業では、東経 (x) と北緯 (y)、住民数 (人口 pop)、平均年齢 (mean_age)、女性の割合 (women)、平均世帯人員 (hh_size) だけが必要である。\n以下のコードチャンクではこれらの必要な変数のみを選択する。なお、この際に変数名をドイツ語から英語に変更する。その結果は Table 14.1 に要約した。\nさらに mutate_all() で、値 -1 と -9 (不明を意味する) を NA に変換する。TABLE 14.1: ダウンロードした census.zip の Datensatzbeschreibung…xlsx から 国勢調査データの各変数のカテゴリ","code":"\ndownload.file(\"https://tinyurl.com/ybtpkwxz\", \n              destfile = \"census.zip\", mode = \"wb\")\nunzip(\"census.zip\") # ファイルを解凍\ncensus_de = readr::read_csv2(list.files(pattern = \"Gitter.csv\"))\ndata(\"census_de\", package = \"spDataLarge\")\n# pop = population, hh_size = household size\ninput = select(census_de, x = x_mp_1km, y = y_mp_1km, pop = Einwohner,\n                      women = Frauen_A, mean_age = Alter_D, hh_size = HHGroesse_D)\n# 値 -1 と -9 を NA に設定\ninput_tidy = mutate(input, across(.cols = c(pop, women, mean_age, hh_size), \n                                  .fns =  ~ifelse(.x %in% c(-1, -9), NA, .x)))"},{"path":"location.html","id":"create-census-rasters","chapter":"14 商圏分析","heading":"14.4 国勢調査ラスタを作成","text":"前処理を行った後、rast() 関数で SpatRaster に変換することができる (Section 2.3.4 と 3.3.1 を参照)。\ntype 引数を xyzとすると、入力データの x y 列は通常グリッドの座標に対応する。\n残りの列 (ここでは pop、women、mean_age、hh_size) は、ラスタレイヤの値として使うことができる (Figure 14.1; GitHub リポジトリの code/14-location-figures.R も参照)。\nFIGURE 14.1: グリッド化した2011年ドイツ国勢調査 (クラスの内容は Table 14.1)。\n次に、input_ras に格納されているラスタの値を、Section 14.2 で述べた調査に従って、Section 4.3.3 で紹介した raster 関数 reclassify() を用いて再分類している。\n母集団データの場合、クラスの平均値を用いて数値データ型に変換する。\nラスタセルは、値 1 (「クラス 1」のセルが 3～250 人の住民を含む) の場合は 127 人、値 2 (250～500人の住民を含む) の場合は 375 人と仮定される (Table 14.1 を参照)。\nこれらのセルには 8,000 人以上の人が含まれているため、「クラス 6」のセル値には 8,000 人の住民が選ばれた。\nもちろん、これは真の母集団の近似値であり、正確な値ではない。98\nしかし、大都市圏を定義するには十分なレベルである (Section 14.5 参照)。総人口の絶対推計を表す変数 pop とは対照的に、残りの変数は、調査で使用されたウェイトに対応するウェイトに分類し直した。\n例えば、変数 women のクラス 1 は、人口の 0～40% が女性である地域を表す。\nは、ターゲット層が男性であるため、比較的高いウェイトである 3 に分類し直した。\n同様に、若年層や単身世帯の割合が高い層は、高いウェイトを持つように分類し直した。なお、リスト中の再分類行列の順序は、input_ras の要素と同じになるようにした。\n例えば、最初の要素はどちらの場合も母集団に対応する。\nその後、-loop、再分類行列を対応するラスタレイヤに適用する。\n最後に、以下のコードで、reclass のレイヤが input_ras のレイヤと同じ名前であることを確認する。","code":"\ninput_ras = rast(input_tidy, type = \"xyz\", crs = \"EPSG:3035\")\ninput_ras\n#> class       : SpatRaster \n#> size        : 868, 642, 4  (nrow, ncol, nlyr)\n#> resolution  : 1000, 1000  (x, y)\n#> extent      : 4031000, 4673000, 2684000, 3552000  (xmin, xmax, ymin, ymax)\n#> coord. ref. : ETRS89-extended / LAEA Europe (EPSG:3035) \n#> source(s)   : memory\n#> names       : pop, women, mean_age, hh_size \n#> min values  :   1,     1,        1,       1 \n#> max values  :   6,     5,        5,       5\nrcl_pop = matrix(c(1, 1, 127, 2, 2, 375, 3, 3, 1250, \n                   4, 4, 3000, 5, 5, 6000, 6, 6, 8000), \n                 ncol = 3, byrow = TRUE)\nrcl_women = matrix(c(1, 1, 3, 2, 2, 2, 3, 3, 1, 4, 5, 0), \n                   ncol = 3, byrow = TRUE)\nrcl_age = matrix(c(1, 1, 3, 2, 2, 0, 3, 5, 0),\n                 ncol = 3, byrow = TRUE)\nrcl_hh = rcl_women\nrcl = list(rcl_pop, rcl_women, rcl_age, rcl_hh)\nreclass = input_ras\nfor (i in seq_len(nlyr(reclass))) {\n  reclass[[i]] = classify(x = reclass[[i]], rcl = rcl[[i]], right = NA)\n}\nnames(reclass) = names(input_ras)\nreclass # 出力は一部省略\n#> ...\n#> names       :  pop, women, mean_age, hh_size \n#> min values  :  127,     0,        0,       0 \n#> max values  : 8000,     3,        3,       3"},{"path":"location.html","id":"define-metropolitan-areas","chapter":"14 商圏分析","heading":"14.5 大都市圏を定義","text":"大都市圏とは、50 万人以上が住む 20 km2 のピクセルと定義している。\nこの粗い解像度のピクセルは、Section 5.3.3 で紹介したように、aggregate()、速やかに作成することができる。\n以下のコマンドは、引数 fact = 20、結果の解像度を 20 倍にしている (元のラスタの解像度が 1 km2 であったことを思い出そう)。次のステージでは、50 万人以上のセルだけを残す。これをプロットすると、8 つの大都市圏 (Figure 14.2) が見えてくる。\n各領域は、1 つ以上のラスタセルで構成される。\n1 つの地域に属するすべてのセルを結合できればコマンドは、\nterra の patches() である。\nその後、.polygons() でラスタオブジェクトを空間ポリゴンに変換し、st_as_sf() で sf. オブジェクトに変換する。\nFIGURE 14.2: 人口ラスタ (分解能20km)、大都市圏 (金色のポリゴン) とその名称。\n自転車店に適した 8 つの都市圏 (Figure 14.2; 図の作成については code/14-location-jm.R も参照) が得られたが、まだ名前が分からない。\n逆ジオコーディングというアプローチでこの問題を解決することができ、対応する住所が得られる。\n各都市圏の重心座標を抽出することで、逆ジオコーディング API の入力とすることができる。\nこれは、tmaptools パッケージの rev_geocode_OSM() 関数が期待するものとまったく同じである。\nさらに .data.frame を TRUE に設定すると、通りの名前、家の番号、都市名など、場所を示すいくつかの列を持つ data.frame が返される。\nしかし、ここでは都市の名前にのみ関心がある。読者が全く同じ結果を使用できるようにするため、metro_names オブジェクトとして spDataLarge に入れている。TABLE 14.2: 逆ジオコーディングの結果City 列が大都市名 (Table 14.2) となっているので、おおむね成功と言えるだろう。例外は、Velbert である。より広域の Düsseldorf の方が適切だろう。\nしたがって、Velbert を Düsseldorf (Figure 14.2) に置き換える。\nウムラウト ü は、例えば opq() を使って大都市圏のバウンディングボックスを決定する場合 (後述)、後々トラブルになる可能性があるため、これも変換しておく。","code":"\npop_agg = aggregate(reclass$pop, fact = 20, fun = sum, na.rm = TRUE)\nsummary(pop_agg)\n#>       pop         \n#>  Min.   :    127  \n#>  1st Qu.:  39886  \n#>  Median :  66008  \n#>  Mean   :  99503  \n#>  3rd Qu.: 105696  \n#>  Max.   :1204870  \n#>  NA's   :447\npop_agg = pop_agg[pop_agg > 500000, drop = FALSE] \nmetros = pop_agg |> \n  patches(directions = 8) |>\n  as.polygons() |>\n  st_as_sf()\nmetro_names = sf::st_centroid(metros, of_largest_polygon = TRUE) |>\n  tmaptools::rev_geocode_OSM(as.data.frame = TRUE) |>\n  select(city, town, state)\n# 小さい都市は town の列で返される。すべての名前を1つの列にするため、\n# NA である場合に備えて、町の名前を市の列に移動させる。\nmetro_names = dplyr::mutate(metro_names, city = ifelse(is.na(city), town, city))\nmetro_names = metro_names$city |> \n  as.character() |>\n  (\\(x) ifelse(x == \"Velbert\", \"Düsseldorf\", x))() |>\n  gsub(\"ü\", \"ue\", x = _)"},{"path":"location.html","id":"points-of-interest","chapter":"14 商圏分析","heading":"14.6 地理的目標物","text":"\nosmdata パッケージは、OSM データへの使いやすいアクセスを提供する (Section 8.5 も参照)。\nドイツ全土の店舗をダウンロードするのではなく、定義された大都市圏にクエリを限定しよう。こうすることで計算負荷を軽減し、目標地域に限定して店舗位置を提供する。\n下のコードチャンクでは、以下のようないくつかの関数を用いてこれを行う。map(): (lapply() の tidyverse 相当)。これは、OSM クエリ関数 opq() (Section 8.5 参照) のバウンディングボックスを定義する、8 つの大都市名すべてを繰り返し処理add_osm_feature(): キー値が shop の OSM 要素を指定する (共通のキー:値のペアの一覧は wiki.openstreetmap.org を参照)osmdata_sf(): これは OSM データを空間オブジェクト (クラス sf) に変換while(): ダウンロードに失敗すると、さらに 2 回ダウンロードを試みる99このコードを実行する前に、 のデータをダウンロードするデータの大きさが約 2 GB となることに注意しよう。\n時間とリソースを節約するために、shops という名前の出力を spDataLarge に格納してある。\n自分の環境で利用できるようにするには、spDataLarge パッケージがロードされていることを確認するか、data(\"shops\", package = \"spDataLarge\") を実行してみよう。定義された大都市圏に店舗がないことはまずありえない。\n次の の条件は、各地域に少なくとも 1 つの店舗があるかどうかをチェックするだけである。\nその場合は、該当する地域の店舗を再度ダウンロードすることを勧める。各リストの要素 (sf データフレーム) が同じ列を持っていることを確認するために100 osm_id と shop 列だけを残し、さらに map_dfr ループを使って全ての店舗を一つの大きな sf オブジェクトに統合する。注: shops は、以下のように spDataLarge パッケージから取得する。最後に、空間点オブジェクトをラスタに変換する (Section 6.4 参照)。\nsf オブジェクト shops は、reclass オブジェクトと同じパラメータ (寸法、解像度、CRS) を持つラスタに変換される。\n重要なのは、ここで length() 関数を用いて、各セルのショップ数を算出していることである。そのため、後続のコードチャンクの結果は、店舗密度 (店舗/km2) の推定値となる。\nst_transform() は、両入力の CRS が一致するように、rasterize() の前に使用される。他のラスタレイヤ (人口、女性、平均年齢、世帯人員) と同様、poi ラスタは 4 つのクラスに再分類される (Section 14.4 参照)。\nクラス間隔の定義は、ある程度恣意的に行われる。\n均等割、分位割、固定値などを使用することができる。\nここでは、クラス内分散を最小化する Fisher-Jenks 自然分類法を選択し、その結果を再分類行列の入力とする。","code":"\nshops = purrr::map(metro_names, function(x) {\n  message(\"Downloading shops of: \", x, \"\\n\")\n  # サーバに時間を与える\n  Sys.sleep(sample(seq(5, 10, 0.1), 1))\n  query = osmdata::opq(x) |>\n    osmdata::add_osm_feature(key = \"shop\")\n  points = osmdata::osmdata_sf(query)\n  # ダウンロードしなかった場合、同じデータをリクエスト\n  iter = 2\n  while (nrow(points$osm_points) == 0 && iter > 0) {\n    points = osmdata_sf(query)\n    iter = iter - 1\n  }\n  # 点フィーチャのみ返す\n  points$osm_points\n})\n# 各都道府県のダウンロードショップがあるかどうかの確認\nind = map(shops, nrow) == 0\nif (any(ind)) {\n  message(\"There are/is still (a) metropolitan area/s without any features:\\n\",\n          paste(metro_names[ind], collapse = \", \"), \"\\nPlease fix it!\")\n}\n# 特定の列のみを選択\nshops = purrr::map_dfr(shops, select, osm_id, shop)\ndata(\"shops\", package = \"spDataLarge\")\nshops = sf::st_transform(shops, st_crs(reclass))\n# POI ラスタを作成\npoi = terra::rasterize(x = shops, y = reclass, field = \"osm_id\", fun = \"length\")\n# 再分類化行列を作成\nint = classInt::classIntervals(values(poi), n = 4, style = \"fisher\")\nint = round(int$brks)\nrcl_poi = matrix(c(int[1], rep(int[-c(1, length(int))], each = 2), \n                   int[length(int)] + 1), ncol = 2, byrow = TRUE)\nrcl_poi = cbind(rcl_poi, 0:3) \n# 再分類\npoi = classify(poi, rcl = rcl_poi, right = NA) \nnames(poi) = \"poi\""},{"path":"location.html","id":"identify-suitable-locations","chapter":"14 商圏分析","heading":"14.7 適当な場所を特定","text":"すべてのレイヤを結合する前に残っている唯一のステップは、poi を reclass のラスタスタックに追加し、そこから人口レイヤを削除することである。\n後者の理由は 2 つある。\nまず、大都市圏、つまりドイツの他の地域に比べて人口密度が平均的に高い地域はすでに定義されている。\n第二に、特定のキャッチメントエリア内に多くの潜在顧客がいることは有利であるが、数が多いだけでは、実際には望ましいターゲットグループを表していない可能性がある。\n例えば、高層マンションは人口密度が高い地域であるが、高価なサイクル部品の購買力が高いとは限らない。他のデータサイエンス・プロジェクトと同様、データの検索と「整頓」が全体の作業負荷の多くを占めている。\nきれいなデータであれば、最後のステップであるすべてのラスタのレイヤを合計して最終的なスコアを計算することも、1 行のコードで実現できる。例えば、9 以上のスコアは、自転車ショップを配置できるラスタセルを示す適切な閾値だろう (Figure 14.3 ; code/14-location-jm.R も参照)。\nFIGURE 14.3: ベルリンにおける自転車店の仮想調査に従った適切なエリア (スコア> 9のラスタセル)。\n","code":"\n# 人口ラスタを削除し、poi ラスタを追加\nreclass = reclass[[names(reclass) != \"pop\"]] |>\n  c(poi)\n# 合計点を計算\nresult = sum(reclass)"},{"path":"location.html","id":"discussion-and-next-steps","chapter":"14 商圏分析","heading":"14.8 考察と次のステップ","text":"今回紹介したアプローチは、GIS の規範的な使い方の典型的な例である (Longley 2015)。\n調査データと専門家による知識・仮定 (大都市圏の定義、クラス間隔の定義、最終的なスコア閾値の定義) を組み合わせている。\nこのアプローチは、科学的な研究よりも、他の情報源と比較すべき、自転車店に適した地域のエビデンスに基づく指標を提供する応用分析に適している。\nアプローチにいくつかの変更を加えることで、分析結果を改善することができる。最終的なスコアの算出には均等なウェイトを用いたが、世帯規模など他の要因も、女性の割合や平均年齢と同様に重要である可能性がある。全ての地理的目標物 を使用したが、DIY、ハードウェア、自転車、釣り、ハンティング、バイク、アウトドア、スポーツショップなど、自転車販売店に関連するもののみ (ショップ値の範囲は OSM Wiki で確認可能)にすると、より洗練された結果を得ることができたかもしれない解像度の高いデータを使うと、出力が向上する場合がある (演習参照)限られた変数のみを使用した。INSPIRE geoportal や OpenStreetMap のサイクリングロードのデータなど、他の情報源からのデータは分析を豊かにするかもしれない (Section 8.5 も参照のこと)。男性比率と単身世帯の関係などの相互作用は考慮されていない。つまり、この分析は多方面に拡張することができる。\n商圏分析の文脈の中で、R で空間データを取得し、扱う方法について、第一印象と理解を深めていただけたと思われる。最後に、今回の分析は、あくまでも適地探しの第一歩に過ぎないということを指摘しておく必要がある。\nこれまでの調査により、1 km 四方で自転車販売店の立地が可能なエリアを特定した。\nその後の分析のステップを踏むことができる。特定のキャッチメントエリア内の住民の数に基づいて最適な場所を見つける。\n例えば、できるだけ多くの人が自転車で15分以内の移動距離で行けるお店であること (キャッチメントエリアルート検索)。\nそのため、店舗から遠ければ遠いほど、実際に店舗を訪れる可能性が低くなることを考慮する必要がある (距離減衰関数)。また、競合他社を考慮するのも良いアイデアだろう。\nつまり、選択した場所の近辺にすでに自転車屋がある場合、可能性のある顧客 (または販売可能性) を競合他社に分散させる必要がある (Huff 1963; Wieland 2017)。例えば、アクセスの良さ、駐車場の有無、通行人の希望頻度、大きな窓があることなど、適切かつ手頃な価格の不動産を探す必要がある。","code":""},{"path":"location.html","id":"演習-10","chapter":"14 商圏分析","heading":"14.9 演習","text":"E1. 100 m セル解像度の住民情報を含む csv ファイルをダウンロードしなさい (https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip?__blob=publicationFile&v=3)。\n解凍したファイルのサイズは 1.23 GB である。\nこのファイルを R に読み込むには、readr::read_csvを使うことができる。\n16 GB の RAM を搭載したパソコンで 30 秒かかる。\ndata.table::fread() はさらに速く、data.table() クラスのオブジェクトを返す。\ndplyr::as_tibble() を使用して、それを tibble に変換しなさい。\n住民ラスタを作成し、セル解像度 1 km に集約し、クラスの平均値を用いて作成した住民ラスタ (inh) との差を比較しなさい。E2. 仮に、自転車店が主に高齢者に電動自転車を販売していたとしよう。\nそれに応じて年齢ラスタを変更し、残りの分析を繰り返し、その変化を元の結果と比較しなさい。","code":""},{"path":"eco.html","id":"eco","chapter":"15 生態学","heading":"15 生態学","text":"","code":""},{"path":"eco.html","id":"prerequisites-15","chapter":"15 生態学","heading":"必須パッケージ","text":"この章では、Chapter 2 ～ Chapter 5 で説明した地理データ解析と処理について十分に理解していることを前提に説明する。\nまた、Chapters 10 と Chapter 12 で解説した GIS へのブリッジと空間交差検証 (cross-validation, CV) も活用していこう。この章では、以下のパッケージを使用する。","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(data.table)        # 高速な data_frame 操作 (mlr3 が使用)\nlibrary(mlr3)              # 機械学習 (Chapter 12 参照)\nlibrary(mlr3spatiotempcv)  # 時空間リサンプリング\nlibrary(mlr3tuning)        # ハイパーパラメータのチューニング\nlibrary(mlr3learners)      # 最重要の機械学習へのインターフェース\nlibrary(paradox)           # ハイパーパラメータ空間を定義\nlibrary(ranger)            # ランダムフォレスト\nlibrary(qgisprocess)       # QGIS へのブリッジ (Chapter 10)\nlibrary(tree)              # 決定木\nlibrary(vegan)             # 生態学"},{"path":"eco.html","id":"introduction-15","chapter":"15 生態学","heading":"15.1 イントロダクション","text":"本章では、霧のオアシスの植生勾配をモデル化し、水の利用可能性に明らかに支配されている特徴的な植生帯を明らかにする。\nケーススタディでは、R におけるジオコンピュテーションスキルを磨くため、これまでの章で紹介した概念をまとめ、さらに拡張していく。霧のオアシスは、地元ではロマ植生と呼ばれ、Peru や Chile の海岸砂漠に沿った山々に発達している。\n同様の植生形態は、Namibia や Yemen、Oman の海岸沿いなど他の地域でも見られる (Galletti, Turner, Myint 2016)。\n年間の降水量平均は 30〜50 mm 程度であり乾燥した条件にもかかわらず、霧の発生により、南半球の冬の間に植物が利用できる水量が増加する。\nその結果、Peru の海岸線に沿った南向きの山の斜面が緑色になる (Figure 15.1)。\n数年に一度、エルニーニョ現象によって、この太陽の降り注ぐ環境に集中豪雨がもたらされ、苗木はその後の乾燥した環境を生き抜くための根を長く張るチャンスを得ることができる (Dillon, Nakazawa, Leiva 2003)。残念ながら、霧のオアシスは、主に人間の活動により、大きく危機に瀕している。\n残されたユニークな植生生態系を効果的に保護するためには、原生植物相の構成と空間分布に関するエビデンスが必要である (Muenchow, Bräuning, et al. 2013; Muenchow, Hauenstein, et al. 2013)。この章では、Peru の中央北岸に位置する Casma 近郊のロマ植生山である Mongón 山の南斜面における維管束植物 (ここでは主に顕花植物) の組成と空間分布を分析する (Figure 15.1)。\nMongón 山での野外調査において、2011年の冬に無作為にサンプリングした 4 x 4 m2 の 100 区画に生息する維管束植物をすべて記録した (Muenchow, Bräuning, et al. 2013)。\nこのサンプリングは、米国海洋大気庁 (NOAA) が発表したデータに示されているように、その年の強いラニーニャ現象が発生した時期と重なった。\nこのため、沿岸部の砂漠では例年よりもさらに高いレベルの乾燥が発生し、Peru のロマ植生山脈の南斜面では霧の活動が活発化した。\nFIGURE 15.1: Mongón 山調査地。Muenchow, Schratz, Brenning (2017) の図。\nまた本章では、前章で扱ったテクニックを、重要な応用分野である生態学に応用する方法を示す。\n具体的には、必要なデータを読み込み、環境予測因子を計算する (Section 15.2)次元削減技術 (座標付け; Section 15.3) を用いて、種組成行列から主な植物学的勾配を抽出する最初の順序軸、すなわち植物相勾配を、標高、傾斜、集水域、NDVI などの環境予測変数の関数としてモデル化する (Section 15.4)\nそのために、ランダムフォレストモデルを利用する。機械学習アルゴリズム (Breiman 2001)の 1 つである。最適な予測を行うために、空間交差検証を用いて、ハイパーパラメータを事前に調整することが望ましい (see Section 12.5.2)調査地内の任意の場所の植物組成の空間分布図を作成する (Section 15.4.2)","code":""},{"path":"eco.html","id":"data-and-data-preparation","chapter":"15 生態学","heading":"15.2 データとデータ準備","text":"以降の解析に必要なデータは、spDataLarge パッケージから入手可能である。study_area は調査地域の外形を表すポリゴン、random_points はランダムに選ばれた 100 地点を含む sf オブジェクトである。\ncomm はワイドデータ形式の群集行列で、行はフィールドで訪問した場所、列は観察された種を表している。101数値はサイトごとの種の被度を表し、サイト面積に対する種の被度の割合 (%; ひとつのサイトでは、個々の植物間の被度の重複により 100% を超える場合があることに注意) で記録された。\ncomm の列名は random_points の id 列に対応する。\ndem は調査地域の数値標高モデル (digital elevation model, DEM) で、ndvi は Landsat シーンの赤色および近赤外チャネルから算出した正規化植生指標 (Normalized Difference Vegetation Index, NDVI) である (Section 4.3.3 および ?spDataLarge::ndvi.tif を参照)。\nFigure 15.2 に示すように、dem に random_points と study_area を重ねて表示すことで、データをより身近なものにすることができる。\nFIGURE 15.2: スタディマスク (ポリゴン)、サンプリング地点 (黒点)、背景の DEM。\n次のステップは、モデリングと予測地図作成に必要な変数 (Section 15.4.2 参照) だけでなく、非計量多次元尺度構成法 (Non-Metric Multidimensional Scaling, NMDS) 軸を調査地域の主要勾配、高度と湿度にそれぞれ整合させるための変数も計算することである (Section 15.3 参照)。具体的には、R-GIS ブリッジを用いて、デジタル標高モデルから集水勾配と集水面積を計算する (Chapter 10 参照)。\n曲率も重要な予測因子である可能性があり、「演習」セクションで、それらがモデリング結果にどのような影響を与えるかを確認することができる。集水域と集水勾配を計算するには、sagang:sagawetnessindex 関数を利用することができる。102\nqgis_show_help() 特定のジオアルゴリズムのすべての関数パラメータとデフォルト値を返す。\nここでは、その一部のみを紹介する。次に、R の名前付き引数を使って、必要なパラメータを指定する (Section 10.2 を参照)。\nディスク上のファイルへのパス、あるいは R のグローバル環境にある SpatRaster を使って、入力ラスタ DEM を指定できることを思い出そう (Section 10.2 参照)。\nSLOPE_TYPE に 1 を指定することで、アルゴリズムが集水勾配を返すようになる。\n生成されたラスタは、SAGA のラスタ形式である .sdat という拡張子で一時ファイルに保存される。これは、計算された出力ラスタへのパスを含む ep という名前のリストを返す。\n集水域と集水勾配を多層構造 SpatRaster オブジェクトに読み込んでみよう (Section 2.3.4 参照)。\nさらに、これに 2 つのラスタオブジェクト dem と ndvi を追加する。さらに、集水域 の値は右側に大きく偏っている (hist(ep$carea))。\n常用対数変換をすると、より正規分布に近くなる。読者の便宜を図るため、spDataLarge に ep を追加した。最後に、現地観測に地形属性を抽出することができる (Section 6.3 も参照)。","code":"\ndata(\"study_area\", \"random_points\", \"comm\", package = \"spDataLarge\")\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nndvi = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))\n# サイト 35 から 40 と、それに対応する群集マトリックスの\n# 最初の 5 種の発生状況\ncomm[35:40, 1:5]\n#>    Alon_meri Alst_line Alte_hali Alte_porr Anth_eccr\n#> 35         0         0         0       0.0     1.000\n#> 36         0         0         1       0.0     0.500\n#> 37         0         0         0       0.0     0.125\n#> 38         0         0         0       0.0     3.000\n#> 39         0         0         0       0.0     2.000\n#> 40         0         0         0       0.2     0.125\n# saga next generation プラグインを有効化していない場合に有効化\nqgisprocess::qgis_enable_plugins(\"processing_saga_nextgen\")\n# ヘルプを表示\nqgisprocess::qgis_show_help(\"sagang:sagawetnessindex\")\n#> Saga wetness index (saga:sagawetnessindex)\n#> ...\n#> ----------------\n#> Arguments\n#> ----------------\n#> \n#> DEM: Elevation\n#>  Argument type:  raster\n#>  Acceptable values:\n#>      - Path to a raster layer\n#> ...\n#> SLOPE_TYPE: Type of Slope\n#>  Argument type:  enum\n#>  Available values:\n#>      - 0: [0] local slope\n#>      - 1: [1] catchment slope\n#> ...\n#> AREA: Catchment area\n#>  Argument type:  rasterDestination\n#>  Acceptable values:\n#>      - Path for new raster layer\n#>... \n#> ----------------\n#> Outputs\n#> ----------------\n#> \n#> AREA: <outputRaster>\n#>  Catchment area\n#> SLOPE: <outputRaster>\n#>  Catchment slope\n#> ...\n# 環境予測因子: 集水勾配と集水面積\nep = qgisprocess::qgis_run_algorithm(\n  alg = \"sagang:sagawetnessindex\",\n  DEM = dem,\n  SLOPE_TYPE = 1, \n  SLOPE = tempfile(fileext = \".sdat\"),\n  AREA = tempfile(fileext = \".sdat\"),\n  .quiet = TRUE)\n# 集水域と集水勾配を読み取る\nep = ep[c(\"AREA\", \"SLOPE\")] |>\n  unlist() |>\n  rast()\nnames(ep) = c(\"carea\", \"cslope\") # 名前をわかりやすく変更\norigin(ep) = origin(dem) # ラスタが同じ原点を持つことを確認\nep = c(dem, ndvi, ep) # 多層 SpatRaster オブジェクトに dem と ndvi を追加する。\nep$carea = log10(ep$carea)\nep = rast(system.file(\"raster/ep.tif\", package = \"spDataLarge\"))\nep_rp = terra::extract(ep, random_points, ID = FALSE)\nrandom_points = cbind(random_points, ep_rp)"},{"path":"eco.html","id":"nmds","chapter":"15 生態学","heading":"15.3 次元性を低減","text":"座標付け (Ordination) は、植生学において、0 で埋め尽くされた大規模な種間プロット行列から主要情報 (生態的勾配に相当することが多い) を抽出するための一般的なツールである。\nしかし、リモートセンシング、土壌学、ジオマーケティング などの分野でも利用されている。\n座標付けテクニックに馴染みがない場合、または復習が必要な場合は、生態学で人気の座標付けテクニックを簡単に紹介した Michael W. Palmer のウェブページ と R でこれらのテクニックを適用する方法について深く調べた Borcard, Gillet, Legendre (2011) に目を通してみよう。\nvegan のパッケージのドキュメントも非常に有用なリソースである (vignette(package = \"vegan\"))。主成分分析 (principal component analysis, PCA) は、おそらく最も有名な座標付け の手法である。\n変数間の線形関係が期待でき、2 つのプロット (観測) における変数の共同不在が類似性とみなせる場合、次元を削減するためのすばらしいツールである。\nこれは植生データではほとんどない。ひとつは、植物の存在は通常、勾配 (湿度、温度、塩分など) に沿って、最も好ましい条件でピークを迎え、好ましくない条件に向かって減少していくという単峰性の関係にある。第二に、ある種が 2 つの区画で同時に存在しないことは、類似性を示す指標とはなりにくい。\nある植物種が、サンプルの中で最も乾燥した場所 (例: 極度の砂漠) と最も湿った場所 (例: 木のサバンナ) から姿を消したとする。\nというのも、この 2 つの全く異なる環境設定に共通するのは、 (稀少なユビキタス種を除いて) 種が存在しないという点だけである可能性が非常に高いからである。NMDS は、生態学でよく使われる次元削減手法の一つである (von Wehrden et al. 2009)。\nNMDS は、元の行列のオブジェクト間の距離と、座標付けられたオブジェクト間の距離の間のランクベースの差異を低減する。\nその差をストレスとして表現している。\nストレス値が低いほど、座標付け、すなわち元の行列の低次元表現が良好であることを示す。\nストレス値が 10 より小さいと適合性が高く、15 程度でも良好、20 より大きいと適合性が低いことを表している (McCune, Grace, Urban 2002)。\nR では、vegan パッケージの metaMDS() で NMDS を実行することができる。\n入力として、サイトを行、種を列とする群集行列を期待する。\nしばしば、有-無データを用いた座標付け は、 (説明される分散の点で) より良い結果をもたらするが、その代償として、もちろん、入力行列の情報量は少なくなる (演習も参照)。\ndecostand() は、数値の観測結果を、1 が種の発生、0 が種の不在を示す有無に変換する。\nNMDS のようなオーダリング技術では、部位ごとに少なくとも 1 回の観測が必要である。\nしたがって、種が発見されなかったサイトはすべて除外する必要がある。得られた行列は、NMDS の入力として機能する。\nk は出力軸数を指定し、ここでは 4 とする。 103\nNMDS は、各ステップで座標付けされた空間をより入力行列に近づけようとする反復的な手順である。\nアルゴリズムの収束を確認するために、try パラメータを使ってステップ数を 500 に設定した。ストレス値 9 は非常に良い結果を表し、縮小された座標付け空間が入力行列の分散の大部分を表していることを意味する。\n全体として、NMDS は、座標付け空間において、(種の構成という点で) より類似したオブジェクトがより近くに配置される。\nしかし、他の多くの座標付け の手法とは対照的に、軸は任意であり、必ずしも重要度 (Borcard, Gillet, Legendre 2011) によって座標付けされるわけではない。\nしかし、調査地では湿度が主な勾配を表していることが既に分かっている (Muenchow, Bräuning, et al. 2013; Muenchow, Schratz, Brenning 2017)。\n湿度は標高と高い相関があるため、標高に応じて NMDS 軸を回転させる (NMDS 軸の回転の詳細については ?MDSrotate も参照)。\n結果をプロットしてみると、意図したとおり、第 1 軸は明らかに高度 (Figure 15.3) と関連していることがわかる。\nFIGURE 15.3: NMDS の第1軸を高度に対してプロット。\n最初の NMDS 軸のスコアは、Mongón 山の斜面に沿って現れる異なる植生形態、すなわち植物学的勾配を表している。\nそれらを空間的に可視化するために、NMDS のスコアを先に作成した予測因子 (Section 15.2) でモデル化し、得られたモデルを予測地図に利用する (Section 15.4 参照) ことができる。","code":"\n# 存在-不在 行列\npa = vegan::decostand(comm, \"pa\") # 100 行 (箇所) , 69 列 (種) \n# 1種以上発見されたサイトのみを残す\npa = pa[rowSums(pa) != 0, ]  # 84 行, 69 列 (種) \nset.seed(25072018)\nnmds = vegan::metaMDS(comm = pa, k = 4, try = 500)\nnmds$stress\n#> ...\n#> Run 498 stress 0.08834745 \n#> ... Procrustes: rmse 0.004100446  max resid 0.03041186 \n#> Run 499 stress 0.08874805 \n#> ... Procrustes: rmse 0.01822361  max resid 0.08054538 \n#> Run 500 stress 0.08863627 \n#> ... Procrustes: rmse 0.01421176  max resid 0.04985418 \n#> *** Solution reached\n#> 0.08831395\nelev = dplyr::filter(random_points, id %in% rownames(pa)) |> \n  dplyr::pull(dem)\n# 高度 (湿度の代理) に応じて NMDS を回転\nrotnmds = vegan::MDSrotate(nmds, elev)\n# 最初の2軸の抽出\nsc = vegan::scores(rotnmds, choices = 1:2, display = \"sites\")\n# 第1軸を高度にプロット\nplot(y = sc[, 1], x = elev, xlab = \"elevation in m\", \n     ylab = \"First NMDS axis\", cex.lab = 0.8, cex.axis = 0.8)"},{"path":"eco.html","id":"modeling-the-floristic-gradient","chapter":"15 生態学","heading":"15.4 植物相の勾配をモデル化","text":"植物相の勾配を空間的に予測するために、ランダムフォレストモデルを使用する。\nランダムフォレストモデルは、環境・生態系のモデリングに頻繁に適用され、予測性能の面で最良の結果をもたらすことが多い (Hengl et al. 2018; Schratz et al. 2019)。\nここで、これらはランダムフォレストの基礎を形成するものであるため、決定木とバギングについて簡単に紹介する。\nランダムフォレストと関連する技術についてのより詳細な説明は James et al. (2013) を参照。決定木を例として紹介すると、まず、回転した NMDS スコアと現場観測値 (random_points) を結合して、応答予測行列を構築する。\nまた、得られたデータフレームは、後で mlr3 のモデリングに使用する予定である。決定木は、予測変数空間をいくつかの領域に分割する。\nこれを説明するために、最初の NMDS の軸のスコアを応答 (sc)、高度 (dem) を唯一の予測因子として、このデータに決定木を適用してみる。\nFIGURE 15.4: 3つの内部ノードと4つの終端ノードを持つ決定木の単純な例。\n結果として得られる木は、3 つの内部ノードと 4 つの終端ノード (Figure 15.4) で構成される。\n木の一番上にある最初の内部ノードは、328.5 m 以下のすべての観測を左に、それ以外のすべての観測を右の枝に割り当てる。\n左の枝に入る観測は、平均 NMDS スコアが -1.198 である。\n全体として、標高が高いほどNMDS のスコアが高くなる、というように解釈できる。\nつまり、単純な決定木によって、すでに 4 つの異なる植物群像が明らかにされているのである。\n詳しく学びたい方は Section 15.4.2 を参照。\n決定木は、過剰に適合する傾向がある。つまり、ノイズを含む入力データを忠実に反映しすぎるため、予測性能が低下するのである (Section 12.4, James et al. 2013)。\nブートストラップ集計 (bootstrap aggregating, bagging, バギング) は、この問題を克服するためのアンサンブル手法である。\nアンサンブル技術は、複数のモデルの予測値を単純に結合するものである。\nこのように、バギングでは同じ入力データから繰り返しサンプルを取り、その予測値を平均化する。\nこれにより、分散と過適合を減らし、決定木と比較してはるかに優れた予測精度を実現する。\n最後に、ランダムフォレストは、相関の高い木の予測を平均化すると、相関の低い木の予測を平均化するよりも分散が大きく、信頼性が低くなるので、バギングを拡張して改良することが望ましい (James et al. 2013)。\nこれを実現するために、ランダムフォレストはバギングを使用するが、従来のバギングでは各木が利用可能なすべての予測子を使用できるのとは対照的に、ランダムフォレストは利用可能なすべての予測子のランダムサンプルだけを使用する。","code":"\n# response-predictor 行列を作成\n# id- と response 変数\nrp = data.frame(id = as.numeric(rownames(sc)), sc = sc[, 1])\n# 予測因子 (dem、ndvi、terrain 属性) を結合 \nrp = inner_join(random_points, rp, by = \"id\")\ntree_mo = tree::tree(sc ~ dem, data = rp)\nplot(tree_mo)\ntext(tree_mo, pretty = 0)"},{"path":"eco.html","id":"mlr3-building-blocks","chapter":"15 生態学","heading":"15.4.1 mlr3 のビルドブロック","text":"このセクションのコードは、Section 12.5.2 で紹介したステップをほぼ踏襲している。\n違いは以下の通りである。応答変数は数値なので、回帰タスクは、Section 12.5.2 の分類タスクに取って代わる。応答変数がカテゴリであるときにしか使えない AUROC の代わりに、性能指標として平均二乗誤差 (root mean squared error, RMSE) を使うことにする。サポートベクタマシン の代わりに、ランダムフォレストモデルを使用している。これは当然ながら、異なるハイパーパラメータを伴う。バイアスを低減した性能指標の評価は、読者の皆様への課題として残している (演習問題参照)。\nその代わりに、 (空間) 予測のためのハイパーパラメータを調整する方法を示す。Section 12.5.2 で、100 回繰り返しの 5 回空間交差検証と 50 回のランダムサーチを使用した場合、バイアス低減された性能推定値を得るために 125,500 個のモデルが必要だったことを思い出してみよう。\nハイパーパラメータのチューニングレベルでは、最適なハイパーパラメータの組み合わせを見つけ、それを特定の空間分割のテストデータを予測するための外部パフォーマンスレベルで使用した (Figure 12.6 も参照)。\nこれを 5 つの空間分割に対して行い、100 回繰り返した結果、合計 500 の最適なハイパーパラメータの組み合わせが得られた。\n空間分布図作成に使うべきはどちらか？\n答えは簡単で、全くない。\nこのチューニングは、バイアスを低減した性能推定値を得るために行われたものであり、最良の空間予測を行うために行われたものではないことに留意してみよう。\n後者については、完全なデータセットから最適なハイパーパラメータの組み合わせを推定する。\nこれは、内部のハイパーパラメータのチューニング・レベルがもはや必要ないことを意味する。なぜなら、真のアウトカムが不明の新しいデータ (未訪問のフィールド観測) にこのモデルを適用するので、テストはいかなる場合でも不可能なのである。\nそこで、5 回繰り返しの空間 CV によって、完全なデータセットで良好な空間予測を行うためにハイパーパラメータを調整することにした。\n既に入力変数 (rp) を構築しているので、mlr3 の構成要素 (タスク、学習器、リサンプリング) を指定するための準備は全て整っている。\n空間タスクの指定には、再び mlr3spatiotempcv パッケージを使用する (Schratz et al. 2021 Section 12.5)。そして、応答 (sc) は数値なので、回帰タスクを使用する。バックエンドとして sf オブジェクトを使用すると、後の空間分割に必要なジオメトリ情報が自動的に提供される。\nさらに、id と spri の列は、これらの変数をモデリングにおける予測因子として使用すべきではないため、削除した。\n次に、ranger パッケージのランダムフォレスト学習器を構築する (Wright Ziegler 2017)。例えばサポートベクタマシン (Section 12.5.2 参照) とは対照的に、ランダムフォレストはハイパーパラメータのデフォルト値で使用した場合、既に良い性能を示すことが多い (これが人気の理由の一つだろう)。\nそれでも、チューニングによってモデル結果が適度に改善されることが多いので、努力する価値はある (Probst, Wright, Boulesteix 2018)。\nランダムフォレストでは、ハイパーパラメータ mtry、min.node.size、sample.fraction がランダム性の度合いを決定するので、これらを調整する必要がある (Probst, Wright, Boulesteix 2018)。\nmtry は、各ツリーでいくつの予測変数を使用すべきかを示す。\nすべての予測変数が使用される場合、これは事実上バギングに相当する (Section 15.4 の冒頭を参照)。\nsample.fraction パラメータは、各ツリーで使用される観測の割合を指定する。\n分画が小さいと多様性が増すので、相関のある樹木が少なくなり、望ましいことが多い (上記参照)。\nmin.node.size パラメータは、端末ノードが少なくとも持つべき観測値の数を示す (Figure 15.4 も参照)。\n当然ながら、木や演算時間が大きくなればなるほど、min.node.size は小さくなる。ハイパーパラメータの組み合わせはランダムに選択されるが、特定のチューニング限界 (paradox::ps() で作成) の範囲内に収まる必要がある。\nmtry は 1 から予測変数の数 4) までの範囲でなければならない。sample.fraction は 0.2 から 0.9 の間、min.node.size は 1 から 10 の間でなければならない (Probst, Wright, Boulesteix 2018)。探索空間を定義したことで、AutoTuner() 関数でチューニングを指定する準備が整った。\n地理的なデータを扱うので、今回も空間交差検証を用いてハイパーパラメータを調整する (Section 12.4 と Section 12.5 を参照)。\n具体的には、1 回だけ繰り返す 5 分割の空間分割 (rsmp()) を使用することにする。\nこれらの空間分割のそれぞれにおいて、あらかじめ定義された限界値 (seach_space) の範囲内でランダムに選択されたハイパーパラメータ構成 (tnr()) を用いながら 50 個のモデル(trm())を実行し、最適なハイパーパラメータの組合せを見出す (Section 12.5.2 と https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner, Bischl et al. 2024 を参照)。\n性能指標は二乗平均平方根誤差 (root mean squared error, RMSE) である。AutoTuner -オブジェクトの train() メソッドを呼び出すと、最終的にハイパーパラメータのチューニングが実行され、指定したパラメータに対して最適なハイパーパラメータの組み合わせが見つかる。","code":"\n# task を作成\ntask = mlr3spatiotempcv::as_task_regr_st(\n  select(rp, -id, -spri),\n  target = \"sc\",\n  id = \"mongon\"\n)\nlrn_rf = lrn(\"regr.ranger\", predict_type = \"response\")\n# 探索空間を指定\nsearch_space = paradox::ps(\n  mtry = paradox::p_int(lower = 1, upper = ncol(task$data()) - 1),\n  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),\n  min.node.size = paradox::p_int(lower = 1, upper = 10)\n)\nautotuner_rf = mlr3tuning::auto_tuner(\n  learner = lrn_rf,\n  resampling = mlr3::rsmp(\"spcv_coords\", folds = 5), # 空間分割\n  measure = mlr3::msr(\"regr.rmse\"), # パフォーマンス測定\n  terminator = mlr3tuning::trm(\"evals\", n_evals = 50), # 繰り返しを 50 回に指定\n  search_space = search_space, # 指定済みハイパーパラメータの探索\n  tuner = mlr3tuning::tnr(\"random_search\") # ランダム探索の指定\n)\n# ハイパーパラメータのチューニング\nset.seed(24092024)\nautotuner_rf$train(task)\nautotuner_rf$tuning_result\n#>     mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse\n#>    <int>           <num>         <int>             <list>    <list>     <num>\n#> 1:     4           0.784            10          <list[4]> <list[3]>     0.382"},{"path":"eco.html","id":"predictive-mapping","chapter":"15 生態学","heading":"15.4.2 予測地図","text":"調整されたハイパーパラメータは、これで予測に使用することができる。\nそのためには、適合した AutoTuner オブジェクトの predict メソッドを実行するだけでよい。predict メソッドは、モデリングに使用されるすべての観測にモデルを適用する。\nモデリングで使用された予測因子として名付けられたラスタを含む多層 SpatRaster、terra::predict() は空間分布図作成、すなわち新しいデータに対する予測も行う。\nFIGURE 15.5: 植物相の勾配を予測するマッピングにより、はっきりとした植生帯が明らかになった。\nterra::predict() がモデル・アルゴリズムに対応していない場合でも、手動で予測を行うことができる。予測地図には、はっきりとした植生帯 (Figure 15.5) が描かれている。\nロマ植生山の植生帯の詳細については、Muenchow, Hauenstein, et al. (2013) を参照。\n青い色調は、いわゆる ハナアナナス帯を表している。\nハナアナナスは、特にロマ植生山脈の砂地やかなり砂漠的な麓で大量に見られる、高度に適応した属植物である。\n黄色は草本植生帯で、ハナアナナス植生帯に比べ植物被度が高いことを表している。\nオレンジ色はブロメリア帯を表し、種の豊富さと植物被覆率が最も高いことを特徴としている。\n霧による湿度が最も高い気温逆転地帯 (標高約 750 - 850 m) の真下で見られる。\n逆転温度以上になると当然水分は減少し、再び砂漠化し、数種の多肉植物が見られるようになる (多肉植物帯; 赤色)。\n興味深いのは、空間予測によってブロメリア帯が途切れていることが明らかになったことである。これは、予測地図なしでは発見できなかった非常に興味深い発見であった。","code":"\n# 最適なハイパーパラメータの組み合わせで予測\nautotuner_rf$predict(task)\n#> \n#> ── <PredictionRegr> for 84 observations: ───────────────────────────────────────\n#>  row_ids  truth response\n#>        1 -1.084   -1.176\n#>        2 -0.975   -1.176\n#>        3 -0.912   -1.168\n#>      ---    ---      ---\n#>       82  0.814    0.594\n#>       83  0.814    0.746\n#>       84  0.808    0.807\npred = terra::predict(ep, model = autotuner_rf, fun = predict)\nnewdata = as.data.frame(as.matrix(ep))\ncolSums(is.na(newdata))  # 0 NAs\n# 0 があると仮定すると、より一般的なアプローチになる\nind = rowSums(is.na(newdata)) == 0\ntmp = autotuner_rf$predict_newdata(newdata = newdata[ind, ], task = task)\nnewdata[ind, \"pred\"] = data.table::as.data.table(tmp)[[\"response\"]]\npred_2 = ep$dem\n# ここで、ラスタを予測値で埋める\npred_2[] = newdata$pred\n# terra と我々の手動予測が同じかどうかをチェックする。\nall(values(pred - pred_2) == 0)"},{"path":"eco.html","id":"結論","chapter":"15 生態学","heading":"15.5 結論","text":"本章では、NMDS (Section 15.3) を用いて、ロマ植生の Mongón 山の群集行列を座標付けした。\n最初の軸は、調査地域の主な植物相の勾配を表し、部分的に R-GIS ブリッジ (Section 15.2) を使って導き出した環境予測因子の関数としてモデル化された。\nmlr3 パッケージは、ハイパーパラメータ mtry、sample.fraction および min.node.size (Section 15.4.1 ) を空間的に調整するためのビルディングブロックを提供している。\n調整されたハイパーパラメータは最終モデルの入力となり、このモデルを環境予測変数に適用して植物相の勾配を空間的に表現した (Section 15.4.2)。\nその結果、砂漠の真ん中にある驚異的な生物多様性を空間的に示すことができたのである。\nロマ植生山は絶滅の危機に瀕しているため、予測地図は保護区域を定める際の判断材料となり、地域住民に身近にあるユニークな存在であることを認識させることができるのである。方法論の面では、いくつかの追加的な指摘ができる。2 つ目の軸もモデル化し、2 つの軸のモデル化されたスコアを 1 つの予測地図に統合して可視化する革新的な方法を見つけるのは興味深いことであるもし、生態学的に意味のある方法でモデルを解釈することに興味があれば、おそらく (セミ) パラメトリックモデルを使うべきだろう (Muenchow, Bräuning, et al. 2013; . Zuur et al. 2009; . F. Zuur et al. 2017)。\nしかし、少なくともランダムフォレストのような機械学習モデルの解釈を助けるアプローチは存在する (例えば、https://mlr-org.com/posts/2018-04-30-interpretable-machine-learning-iml--mlr/ を参照)本章で使用したハイパーパラメータのランダム化最適化よりも、逐次モデルベース最適化 (sequential model-based optimization, SMBO) の方が望ましいかもしれない (Probst, Wright, Boulesteix 2018)最後に、ランダムフォレストと他の機械学習モデルは、多くの観測と多くの予測因子、この章で使われるよりもはるかに多く、どの変数と変数の相互作用が応答を説明するのに寄与するかが不明である設定で頻繁に使用されることに注意しておこう。\nさらに、その関係は高度に非線形である可能性もある。\nこのユースケースでは、レスポンスと予測変数の関係はかなり明確で、非線型はわずかであり、観測と予測変数の数は少ない。\nしたがって、線形モデルを試してみる価値はあるかもしれない。\n線形モデルは、ランダムフォレストモデルよりも説明や理解がしやすいので好まれ (思考節約の原理)、さらに計算負荷が少ない (演習を参照)。\n線形モデルがデータに存在する非線形性の程度に対処できない場合、一般化加法モデル (generalized additive model, GAM) を試してみることもできる。\nここで重要なのは、データサイエンティストのツールボックスは複数のツールで構成されており、目の前のタスクや目的に最適なツールを選択するのはあなたの責任であるということである。\nここでは、ランダムフォレストのモデリングと、それに対応した空間予測図への利用方法を紹介したいと思われる。\nこの目的のためには、反応と予測因子の関係が既知の、よく研究されたデータセットが適切である。\nしかし、これはランダムフォレストモデルが予測性能の面で最良の結果を返したことを意味するものではない。","code":""},{"path":"eco.html","id":"演習-11","chapter":"15 生態学","heading":"15.6 演習","text":"回答するには、以下のパッケージをアタッチすることとする (他のパッケージも必要に応じてアタッチする)。E1. コミュニティ行列のパーセンテージデータを使用して、NMDS を実行する。\nストレス値を報告し、存在-不在データを使用して NMDS から取得したストレス値と比較します。\nこの違いを説明するものは何か?E2. この章で使用したすべての予測ラスタ (集水勾配、集水面積) を計算し、SpatRasterオブジェクトに格納しなさい。\nそこに dem と ndvi を追加しなさい。\n次に、プロファイルと接線曲率を計算し、追加の予測ラスタとして追加しなさい (ヒント: grass7:r.slope.aspect)。\n最後に、応答予測行列を構築しなさい。\n最初の NMDS 軸のスコア (存在-不在コミュニティ行列を使用したときの結果) を標高に従って回転させたものが応答変数を表し、random_pointsに結合とする (内側結合を使用する)。\n応答予測行列を完成させるために、環境予測ラスタ・オブジェクトの値を random_points に抽出しなさい。E3. 空間交差検証を使用して、ランダムフォレストと線形モデルのバイアス削減 RMSE を取得しなさい。\nランダムフォレストのモデリングには、最適なハイパーパラメータの組み合わせの推定 (50 回の反復によるランダム探索) を内部チューニングループに含めなさい。\nチューニングレベルを並列化しなさい。\n平均 RMSE を報告し、箱ひげ図を使用して、検索されたすべての RMSE を可視化しなさい。\nこの課題は、mlr3 の関数 benchmark_grid() と benchmark() (詳細は https://mlr3book.mlr-org.com/perf-eval-cmp.html#benchmarking を参照) を用いて解くのが最適。","code":""},{"path":"conclusion.html","id":"conclusion","chapter":"16 結論","heading":"16 結論","text":"","code":""},{"path":"conclusion.html","id":"introduction-16","chapter":"16 結論","heading":"イントロダクション","text":"第 1 章と同様、結論にはコードチャンクはほとんどない。\nここでの目的は、繰り返されるテーマ/概念に言及しながら、本書の内容を総括し、今後の応用や開発の方向性を鼓舞することにある。\nこの章には前提条件はない。\nしかし、第I部 (基礎編) の練習問題を読んで挑戦し、第II部 (拡張機能) のより高度な問題に挑戦し、第III部 (応用編) の章を参考に、ジオコンピュテーションが仕事、研究、その他の問題の解決にどのように役立つかを考えていたならば、より多くのものを得ることができるであろう。本章は、以下のように構成されている。\nSection 16.1 は、R で地理データを扱うための幅広いオプションについて説明する。\n選択は、オープンソースソフトウェアの重要な特徴である。このセクションでは、様々なオプションの中から選択するためのガイダンスを提供する。\nSection 16.2 では、本書の内容とのギャップを説明し、意図的に省略された研究分野、強調された分野の理由を説明している。\nSection 16.3 では、問題に直面した時にどのように質問し、オンラインで解決を探すためのアドバイスを行う。\nSection 16.4は、この本を読んだあとで、次はどこへ行くのかという問いに答える。\nSection 16.5 は、Chapter 1 で提起されたより広範な問題に戻る。\nその中で、ジオコンピュテーションを、手法が一般にアクセス可能で、再現性があり、協力的なコミュニティによってサポートされていることを保証する、より広い「オープンソースアプローチ」の一部として考えている。\nこの最終章では、参加するためのポイントも紹介している。","code":""},{"path":"conclusion.html","id":"package-choice","chapter":"16 結論","heading":"16.1 パッケージの選択","text":"オープンソース全般に言えることだが、R の特徴として、同じ結果を得るために複数の方法が存在することが多い。\n下記のコードでは、Chapter 3 と Chapter 5 にある 3 つの関数を使って、New Zealand の 16 の地域を 1 つの幾何学的形状にまとめている。作成されたクラス、属性、列の名称は nz_u1 から nz_u3 まで異なるが、その幾何学的な形状は同一であることを Base R 関数 identical() を使って検証している。104\nどの方法を使うべきかは、ケースバイケースである。\n１番目の方法は nz に含まれるジオメトリデータのみを処理するので高速であるが、２・３番目の方法は属性操作を行うので、この後の処理に役立つ可能性がある。\nBase R の aggregate() 関数を使うか、dplyr の summarise() を使うかは好みの問題であるが、後者の方が読みやすいだろう。つまり、R で地理データを扱う場合、たとえ 1 つのパッケージであっても、複数の選択肢から選ぶことができる場合が多いということである。\nR のパッケージが増えれば、さらに選択肢は広がる。例えば、古いパッケージの sp を使っても同じ結果を得ることができる。\nしかしながら、良いアドバイスをしたいという本書のゴールに従うと、パフォーマンスもよく将来性のある sf パッケージを推奨する。\nこのことは、本書のすべてのパッケージに当てはまるが、他の手段の存在を知って自分の選択について正当化できることは、(邪魔にならない程度に) 役に立つことがある。ジオコンピュテーションをする際の選択で困ることの代表として、 tidyverse と Base R のどちらを使うかという簡単に答えられない問題がある。\n例えば、次のコードチャンクは、Chapter 3 で説明したした nz オブジェクトから Name 列を抽出する操作を、tidyverse と Base R の 2 つの方法を示している。ここで、「どちらを使うべきか？」という問題が出てくる。\n答えは、「人それぞれ」である。\nそれぞれのアプローチには利点がある。Base R は、安定でよく知られており、依存関係が最小である傾向があるため、ソフトウェア (パッケージ) 開発に好まれることが多い。\n一方、Tidyverse アプローチは、対話型プログラミングに好まれている。\nしたがって、2つのアプローチのどちらを選ぶかは、好みと用途の問題である。本書では、Rの基本演算子である [ サブセット演算子や、上のコードで示した dplyr 関数 select() など、一般的に必要とされる関数を取り上げているが、地理データを扱うための、他のパッケージの関数には触れていないものが多くある。\nChapter 1 では、地理データを扱うための 20 以上の有力なパッケージが紹介したが、この本ではそのうちのほんの一握りしか紹介されていない。\nR で地理データを扱うためのパッケージは他にも何百とあり、毎年多くのパッケージが開発されている。\n2024年現在、Spatial Task Viewで紹介されているパッケージは 160 以上あり、地理データ解析のための無数の関数が毎年開発されている。R の空間生態系の進化のスピードは速いが、幅広い選択肢に対応するための戦略がある。\nアドバイスは、まず 1 つのアプローチを深く学び、利用可能なオプションの広さを知っておく。\nこのアドバイスは、R で地理的な問題を解決する際にも他の分野の知識や応用と同様に適用される。\n他の言語での開発については、Section 16.4 で説明する。もちろん、同じタスクでもパッケージによっては他のパッケージより性能が良いものもあり、その場合はどのパッケージを使うべきかを知ることが重要です。\nこの本では、将来性があり (将来も使える)、(他の R パッケージと比較して) 高性能で、(ユーザーや開発者のコミュニティがあり) よくメンテナンスされており、補完的なパッケージに焦点を当てることを目的としている。\nまた、本書で取り上げたパッケージの中には、重複しているものもあり、例えば、「地図作成用パッケージの多様性」については、本章の「地図作成用パッケージの多様性」で紹介している。機能が重複することは良いことである。\n既存のパッケージと同様の (しかし同一ではない) 機能を持つ新しいパッケージは、オープンソースソフトウェアでジオコンピュテーションを行う重要な利点である回復力、パフォーマンス (開発者間の切磋琢磨と相互学習による部分もある)、選択肢を増やすことができる。\nこの文脈では、sf、tidyverse、terra や他のパッケージのどの組み合わせを使用するかを決めることは、代替案を知った上で行うべきである。\n例えば、sf が取って代わるように設計されている sp エコシステムは、本書で取り上げたことの多くを行うことができ、その古さゆえに他の多くのパッケージが構築されている。\n2024年の執筆時点で、463 のパッケージが sp を Depend または Importしており、2018年10月の 452 からわずかに増加しており、そのデータ構造が広く使われ、多くの方向に拡張されていることを示している。\nsf の方はというと、2018年に 69、2023年に 431 であり、このパッケージが将来性を持ち、ユーザーベースと開発者コミュニティが拡大していることを強調している (Bivand 2021)。\n点パターン解析でよく知られている spatstat パッケージは、ラスタやその他のベクトルジオメトリもサポートし、空間統計などのための強力な機能を提供する (Baddeley Turner 2005)。\nまた、既存のパッケージでは満たされないニーズがある場合は、開発中の新しい選択肢を研究する価値があるかもしれない。","code":"\nlibrary(spData)\nnz_u1 = sf::st_union(nz)\nnz_u2 = aggregate(nz[\"Population\"], list(rep(1, nrow(nz))), sum)\nnz_u3 = dplyr::summarise(nz, t = sum(Population))\nidentical(nz_u1, nz_u2$geometry)\n#> [1] TRUE\nidentical(nz_u1, nz_u3$geom)\n#> [1] TRUE\nlibrary(dplyr)                          # TidyVerse パッケージをアタッチ\nnz_name1 = nz[\"Name\"]                   # Base R による方法\nnz_name2 = nz |>                        # TidyVerse による方法\n  select(Name)\nidentical(nz_name1$Name, nz_name2$Name) # 結果の確認\n#> [1] TRUE"},{"path":"conclusion.html","id":"gaps","chapter":"16 結論","heading":"16.2 ギャップとオーバーラップ","text":"ジオコンピュテーションは巨大な分野であり、多くのギャップがあることは避けられない。\n特定にトピックやテクニック、パッケージを意図的に強調し、あるいは省略するなど、選択的に行っている。\n地理データの操作、座標参照系の基本、データの読み書き、可視化技術など、実際のアプリケーションで最もよく必要とされるトピックを重視するよう努めた。\nまた、本書は、ジオコンピュテーションに必要なスキルを身につけ、さらに高度なトピックや特定のアプリケーションに進む方法を紹介することを目的としており、いくつかのトピックやテーマは何度も登場する。また、他で深く取り上げられているトピックをあえて省略した。\n例えば、点パターン解析、空間補間 (例えばクリギング)、空間回帰といった空間データの統計的モデリングは、機械学習の文脈で Chapter 12 で触れているが、詳細には触れていない。\nこれらの手法については、Pebesma Bivand (2023c) の統計学的指向の章や、ポイントパターン分析に関する書籍 (Baddeley, Rubak, Turner 2015)、空間データに適用するベイズ手法 (Gómez-Rubio 2020; Moraga 2023)、健康 (Moraga 2019) や山火事深刻度分析 など特定のアプリケーションに焦点を当てた書籍といった優れた資料がすでに存在している (Wimberly 2023)。\nその他の話題としては、リモートセンシングや、GIS 専用ソフトと並行しての R の利用 (ブリッジとしてではなく) などが挙げられるが、これらは限定的である。\nこれらの話題については、R におけるリモートセンシングについての議論、Wegmann, Leutner, Dech (2016) や Marburg University から入手できる GIS 関連教材など、多くの資料がある。Chapter 12 と Chapter 15 で空間統計推論よりも機械学習に焦点を当てたのは、このトピックに関する質の高いリソースが豊富にあるためである。\nこれらのリソースには、生態系のユースケースに焦点を当てた . Zuur et al. (2009)、. F. Zuur et al. (2017)、そして css.cornell.edu/faculty/dgr2 でホストされている Geostatistics & Open-source Statistical Computing の自由に利用できる教材とコードがある。\nR Geographic Data Science では、地理データサイエンスとモデリングのための R の紹介をしている。また、「ビッグデータ」(ハイスペックなラップトップに収まらないデータセットという意味) に対するジオコンピュテーションはほとんど省略している。\nこの決定は、一般的な研究や政策アプリケーションに必要な地理的データセットの大部分は、個人用ハードウェアに収まるという事実によって正当化される (Section 10.8 参照)。\nコンピュータの RAM を増やしたり、GitHub Codespaces: 本書のコードを実行可能 のようなプラットフォームで利用できる計算能力を一時的に「借りる」ことは可能である。\nさらに、小さなデータセットで問題を解くことを学ぶことは、巨大なデータセットで問題を解くための前提条件であり、本書で強調しているのは「始めること」であり、ここで学んだスキルは大きなデータセットに移行したときに役立つものである。\n「ビッグデータ」の解析には、特定の統計解析のためデータベースからデータを抽出することもある。\nChapter 10 で紹介した空間データベースは、メモリで処理しきれないデータセットの解析に有用である。\n‘Earth observation cloud back-ends’ は、openeo パッケージを使うことで R でアクセスすることができる。\n大きな地理データを扱う必要がある場合は、Apache Sedona などのプロジェクトや、GeoParquet などの新しいファイル形式について調べることも勧める。","code":""},{"path":"conclusion.html","id":"questions","chapter":"16 結論","heading":"16.3 ヘルプを求める","text":"ジオコンピュテーションは大規模で困難な分野であるため、問題や一時的な作業中断は避けられない。\n多くの場合、データ解析のワークフローの特定の時点で、デバッグが困難な不可解なエラーメッセージに直面し、「立ち往生」することがある。\nまた、何が起こっているのかほとんどわからないまま、予期せぬ結果が出ることもある。\nこのセクションでは、そのような問題を克服するために、問題を明確に定義し、解決策に関する既存の知識を検索し、それらのアプローチで問題が解決されない場合は、良い質問をする技術によって、ポイントを提供する。\nある地点で行き詰まったとき、まず一歩下がって、どのようなアプローチが最も解決につながるかを考えてみるのもよいだろう。\n以下のステップを順番に試すことで、問題解決のための構造的なアプローチが得られる (すでに試している場合はステップをスキップすることもできる)。第一原理から始めて、何を達成しようとしているのかを正確に定義する (多くの場合、以下のようなスケッチも必要)。コードの個々の行や個々のコンポーネントの出力を実行し、調べることによって、コードのどこで予期せぬ結果が発生したかを正確に診断する (例えば、RStudio でカーソルで選択し、Ctrl + Enter を押すことによって、複雑なコマンドの個々の部分を実行することができる)。前のステップで「問題のポイント」と診断された関数のドキュメントを読んでみよう。関数に必要な入力を理解し、ヘルプページの下部によく掲載されている例を実行するだけで、驚くほど大きな割合の問題を解決できる (コマンド ?terra::rast を実行し、その関数を使い始めるときに再現する価値のある例までスクロールダウンする。例)前のステップで説明したように R の付属ドキュメントを読んでも問題が解決しない場合は、あなたが見ている問題について他の人が書いていないかどうか、オンラインで広く検索してみるのもよいだろう。検索する場所については、以下のヘルプリストを参照。上記のすべてのステップが失敗し、オンライン検索で解決策が見つからない場合、再現性のある例で質問を作成し、適切な場所に投稿することができる。上記のステップ 1 から 3 は自明なことであるが、インターネットは広大であり、検索オプションも多数あるため、質問を作成する前に効果的な検索方法を検討する価値がある。","code":""},{"path":"conclusion.html","id":"searching-for-solutions-online","chapter":"16 結論","heading":"16.3.1 オンラインによる解決方法の検索","text":"多くの問題に対して論理的なスタートを切るのは、検索エンジンである。\n「ググる」ことで、あなたが抱えている問題についてのブログ記事、フォーラムメッセージ、その他のオンラインコンテンツを発見することができる場合があるのである。\n問題や質問について明確に記述することは有効な方法であるが、具体的に記述することが重要である (例えば、データセット固有の問題であれば、関数やパッケージ名、入力データセットのソースなどを参照すること)。\nまた、詳細な情報を記載することで、オンライン検索をより効果的にすることができる。\n引用符を使用すると、返される結果の数を減るため、検索結果が問題に関連する確率を上げる。たとえば、GeoJSON ファイルをすでに存在する場所に保存しようとして失敗した場合、“GDAL Error 6: DeleteLayer() supported dataset” というメッセージを含むエラーが表示される。引用符を使わずに GDAL Error 6 を検索するよりも、\"GDAL Error 6\" sf のような特定の検索クエリを使用したほうが、解決策が見つかる可能性が高くなる期間の制限を設定する。例えば、過去1年以内に作成されたコンテンツのみを返すようにすれば、進化するパッケージのヘルプを検索する際に便利である追加の検索エンジン機能を利用する。例えば、site:r-project.org で CRAN にホストされているコンテンツに検索を限定する","code":""},{"path":"conclusion.html","id":"help","chapter":"16 結論","heading":"16.3.2 助けを求めるための検索 (依頼) 場所","text":"ネットで検索しても解決しない場合は、助けを求めてもよい。\nこれを行うには、以下のような多くのフォーラムがある.R の Special Interest Group Geographic データメーリングリスト ( R-SIG-GEO)GIS Stackexchange のウェブサイト (gis.stackexchange.com)大型・汎用プログラミングQ&Aサイト stackoverflow.comPosit Community、rOpenSci Discuss ウェブフォーラム、 Stan フォーラムなど、特定のソフトウェアツールに関連するフォーラムなど、特定のエンティティに関連するオンラインフォーラム。GitHub などのソフトウェア開発プラットフォームは、R-spatial パッケージの大半の課題トラッカーや、最近では sfnetworks パッケージに関する議論 (バグ報告だけでなく) を促すために作られた議論ページなどをホストしている (luukvdmeer/sfnetworks/discussionsを参照)。チャットルームやフォーラム rOpenSci や geocompx コミュニティ (これには質問をすることができる Discord server もある)。本書もここに関係している。OSGeoJapan のメーリングリスト (日本語)r-wakalang Slack (日本語)","code":""},{"path":"conclusion.html","id":"reprex","chapter":"16 結論","heading":"16.3.3 reprex による再現性の例","text":"良い質問とは、明確に述べられた質問で、さらにアクセスしやすい完全に再現可能な例があるとよい (https://r4ds.hadley.nz/workflow-help.html も参照)。\nまた、ユーザーの視点から「うまくいかなかった」コードを示した後、何を見たいかを説明することも有効である。\n再現可能な例を作成するための非常に便利なツールが、reprex パッケージである。\n予期せぬ動作を強調するために、問題を示す完全に再現可能なコードを書き、reprex()関数を使って、フォーラムや他のオンラインスペースに貼り付けられるようなコードのコピーを作成することができる。青い海と緑の陸地がある世界地図を作ろうとしているとしよう。\n前項で説明したような場所で、その方法を尋ねることもできできる。\nしかし、あなたがこれまでに試したことの再現可能な例を示すことで、より良い回答が得られる可能性がある。\n次のコードは、青い海と緑の陸地がある世界地図を作成するが、陸地は塗りつぶされない。このコードをフォーラムに投稿すれば、より具体的で有用な回答が得られる可能性がある。\n例えば、Figure 16.1 のように、問題を解決する以下のようなコードを投稿してくれる人がいるかもしれない。\nFIGURE 16.1: 世界地図の土地を緑色で示した再現可能な例題 (左) と解 (右) の地図。\n読者のための練習: 上記のコードをコピーし、コマンド reprex::reprex() を実行し (またはコマンドを reprex() の関数呼び出しにペーストし)、その出力をフォーラムや他のオンラインスペースにペーストしなさい。ジオコンピュテーションが、オープンソースとコラボレーションすることで、膨大で進化し続ける知識体系を生み出すことは強みとなり、本書もその一部である。\n問題を解決するための自分自身の努力を示し、問題の再現可能な例を提供することは、この知識体系に貢献する方法である。","code":"\nlibrary(sf)\nlibrary(spData)\nplot(st_geometry(world), col = \"green\")\nlibrary(sf)\nlibrary(spData)\n# 塗りつぶすために引数を使用\nplot(st_geometry(world), col = \"green\", bg = \"lightblue\")"},{"path":"conclusion.html","id":"defining-and-sketching-the-problem","chapter":"16 結論","heading":"16.3.4 問題の定義とスケッチ","text":"場合によっては、ネット上で問題解決策を見つけることができなかったり、検索エンジンで答えられるような質問を立てることができないこともある。\n新しいジオコンピュテーションの方法論やアプローチを開発するときの最良の出発点は、ペンと紙 (または共同スケッチやアイデアの迅速な共有を可能にする Excalidraw や tldraw などの同等のデジタルスケッチツール) である。方法論開発の作業の最も創造的な初期段階においては、どのようなソフトウェアも思考の速度を落とし、重要な抽象概念から思考を遠ざけることになる。\n方法論開発の最も創造的な初期段階において、ソフトウェア (あらゆる種類のもの) は、思考を鈍らせ、重要な抽象的思考から思考を遠ざけることができる。\nまた、数値的に「前と後」をスケッチできる最小限の例を参照しながら、数学で質問を組み立てることも強く推奨される。\nもし、あなたにスキルがあり、問題がそれを必要とするならば、代数的にアプローチを記述することは、場合によっては効果的な実装を開発するのに役立つ。","code":""},{"path":"conclusion.html","id":"next","chapter":"16 結論","heading":"16.4 次はどこへ行く？","text":"Section 16.2 にあるように、この本は R の地理的なエコシステムのほんの一部しかカバーしておらず、まだまだ発見があるはずである。\nChapter 2 の地理データモデルから、Chapter 15 の高度なアプリケーションまで、急速に進展している。\n学習した技術の統合、地理データを扱うための新しいパッケージやアプローチの発見、新しいデータセットやドメインへの手法の適用が今後の方向性として提案されている。\nこのセクションでは、この一般的なアドバイスに加え、具体的な「次のステップ」を提案し、以下の太字で強調している。R を使って、例えば前節で引用した研究を参考に、さらなる地理的手法や応用について学ぶことに加え、R そのものの理解を深めることが、論理的な次のステップとなる。\nR の基本クラスである data.frame や matrix は、sf や terra クラスの基礎となるものなので、これらを勉強することで地理データの理解が深まるだろう。\nこれは、R の一部であり、help.start() コマンドで見つけることができるドキュメントや、Wickham (2019) や Chambers (2016) などによるこのテーマに関する追加リソースを参照することで行うことができる。また、ソフトウェア関連の今後の学習方向としては、他の言語によるジオコンピュテーションの発見が挙げられる。\nChapter 1 で紹介されているように、ジオコンピュテーションのための言語として R を学習することには理由があるが、R が唯一の選択肢というわけではない。105\nPython、C++、JavaScript、Scala、Rust を使っても、同じ深さでジオコンピュテーションを勉強することができる。\nそれぞれが進化した地理空間能力を有している。\n例えば、Pythonのパッケージ rasterio は、この本で使われている terra パッケージを補足/置換できるものである。\nPython のジオコンピュテーションについては、Geocomputation Python を参照。C++ では、GDAL や GEOS などのよく知られたライブラリから、リモートセンシング (ラスタ) データを処理する Orfeo Toolbox などのあまり知られていないライブラリなど、数十の地理空間ライブラリが開発されている。\nTurf.js は、JavaScript でジオコンピュテーションを行う可能性を示す一例である。\nGeoTrellis は、Java ベースの言語である Scala でラスタおよびベクタデータを扱うための関数を提供する。\nまた、WhiteBoxTools は、Rust で実装された急速に進化するコマンドライン GIS の例を示している。\nこれらのパッケージ/ライブラリ/言語はそれぞれジオコンピュテーションに有利であり、オープンソースの地理空間リソースのキュレーションリスト Awesome-Geospatial に記されているように、さらに多くの発見がある。しかし、ジオコンピュテーションには、ソフトウェア以上のものがある。\n学術的・理論的な観点から、新しい研究テーマや手法の探求・習得をお勧めできる。\nこれまで書かれてきた手法の中には、まだ実装されていないものも多くある。\nそのため、コードを書く前に、地理的な手法や潜在的なアプリケーションについて学ぶことは有意義なことである。\nR で実装されることが多くなった地理的手法の例として、科学的なアプリケーションのためのサンプル戦略がある。\nこの場合の次のステップは、 github.com/DickBrus/TutorialSampling4DSM でホストされている再現可能なコードとチュートリアルコンテンツを伴う Brus (2018) などの領域の関連記事を読み解くことである。","code":""},{"path":"conclusion.html","id":"benefit","chapter":"16 結論","heading":"16.5 オープンソースのアプローチ","text":"この本は技術書であるから、前節で説明した次のステップも技術的なものであることに意味がある。\nしかし、この最後のセクションでは、ジオコンピュテーションの定義に戻り、より広範な問題を検討する価値がある。\nChapter 1 で紹介した用語の要素のひとつに、「ジオグラフィック・メソッドはポジティブな影響を与えるものでなければならない」というものがある。\nもちろん、「ポジティブ」をどう定義し、どう測定するかは、本書の範囲を超えた、主観的で哲学的な問題である。\nどのような世界観を持っていても、ジオコンピュテーションがもたらす影響について考えることは有益なことである。\nまた逆に、新しい手法は多くの応用分野を開拓する可能性がある。\nこれらのことから、ジオコンピュテーションはより広範な「オープンソースアプローチ」の一部であるという結論が導き出される。Section 1.1 は、地理データ科学 (geographic data science, GDS) や GIScience など、ジオコンピュテーションとほぼ同じ意味を持つ他の用語を提示した。\nどちらも地理データを扱うことの本質を捉えているが、ジオコンピュテーションには利点がある。本書で提唱する地理データの「計算」的な扱い方 (コードで実装されているので再現性がある) を簡潔に捉え、初期の定義にあった望ましい要素に基づいて構築されている (Openshaw Abrahart 2000)。地理データのクリエイティブな活用法実社会の問題への応用「科学的」な道具を作る再現性再現性は、ジオコンピュテーションの初期の研究ではほとんど言及されていなかったが、最初の 2 つの要素に不可欠な要素であることを強く主張することができる。\n再現性は、(共有コードで容易に利用できる) 基本から応用へと焦点を移すことで、創造性を促進する。「車輪の再発明」を防ぐ: 他の人がやったことを、他の人が使えるのであれば、やり直す必要はない。あらゆる分野の誰もが新しい分野であなたの方法を適用できるようにすることで、研究をより実世界での応用に近づけられるようにする。もし再現性がジオコンピュテーション (あるいはコマンドライン GIS) の決定的な資産であるならば、何が再現性をもたらすかを考える価値がある。\nそこで、「オープンソースアプローチ」に行き着くのだが、これには 3 つの主要な要素がある。コマンドラインインターフェース (CLI): 地理的な作業を記録したスクリプトの共有と再現を促進するオープンソースソフトウェア: 世界中の誰もが検査し、改良できる可能性がある。活発なユーザーと開発者コミュニティは、補完的でモジュール化されたツールを構築するために協力し、自己組織化している。ジオコンピュテーションという言葉があるように、オープンソース的アプローチは単なる技術的な存在にとどまらない。\n商業的、法的な制約を受けず、誰でも使える高性能なツールを作るという共通の目的を持って日々活動している人たちで構成されるコミュニティである。\n地理データを扱うオープンソース的アプローチには、ソフトウェアの動作に関する技術的な問題を超えて、学習、コラボレーション、効率的な分業を促進する利点がある。特に GitHub のような、コミュニケーションとコラボレーションを促進するコードホスティングサイトの出現により、このコミュニティに参加する方法はたくさんある。\n手始めに、興味のある地理的なパッケージのソースコード、issues、commits のいくつかに目を通すとよいだろう。\nr-spatial/sf GitHub リポジトリをざっと見ると、sf パッケージの基礎となるコードをホストしており、100 人以上がコードベースとドキュメントに貢献していることがわかる。\nさらに何十人もの人が、質問をしたり、sf が使っている「上流」のパッケージに貢献している。\nその issue tracker には 1,500 以上の問題がクローズされ、より速く、より安定した、ユーザーフレンドリーな sf を実現するための膨大な作業が行われている。\nこの例は、数十のパッケージのうちのたった一つの例であるが、R を非常に効果的で継続的に進化するジオコンピュテーションとするために行われている知的作業の規模を表している。GitHub のような公的な場で絶え間なく起こる開発活動を見るのも勉強になるが、自分が積極的に参加することでより大きな収穫を得ることができる。\nこれは、オープンソースの最大の特徴で、人々の参加を促すものである。\nこの本自体もオープンソース化した結果である。\nこれは、過去 20 年間の R の地理的機能の驚くべき発展によって動機づけられたが、共同作業のためのプラットフォーム上での対話とコード共有によって、現実的に可能になった。\n本書が、地理データを扱うための有用な手法を広めるだけでなく、よりオープンソースに近いアプローチをとるきっかけになればと願っている。","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
